# hidiffusion
关注comfyui插件原生实现

sd.next已经支持    
4.29   
估计是直接使用diffusers    




优先级应该是hidiffusion 然后cfg++     
或者cfg设法hook过去 已有一些代码     
但我感觉效果没那么好       

文生图功能 sdxl优先支持

Update

2024.6.19 - 💥 Integrated into OpenBayes, see the demo. Thank OpenBayes team!

2024.6.16 - 💥 Support PyTorch 2.X.

2024.6.16 - 💥 Fix non-square generation issue. Now HiDiffusion supports more image sizes and aspect ratios.

2024.5.7 - 💥 Support image-to-image task, see here.

2024.4.16 - 💥 Release source code.


如何在 Automatic1111 Stable Diffusion Web UI 中使用它（适用于 SD 1.5、XL 等） #8    
这个讨论太早 5.11最晚      

ComfyUI 支持吗？ #1     
这个也很早 5.22最晚

https://github.com/blepping/comfyui_jankhidiffusion#use-with-controlnet     
这个原生插件更新于 5.21最晚 难以适用     


https://github.com/florestefano1975/ComfyUI-HiDiffusion    
这个diffusers包装插件zuiwangeng信誉5.4 也太早




Supported Tasks
✅ Text-to-image
✅ ControlNet, including text-to-image, image-to-image
✅ Inpainting

Supported Models
✅ Stable Diffusion XL
✅ Stable Diffusion XL Turbo
✅ Stable Diffusion v2
✅ Stable Diffusion v1
Note: HiDiffusion also supports the downstream diffusion models based on these repositories, such as Ghibli-Diffusion, Playground, etc.

高分辨类似      
Kohya Deep Shrink     
ScaleCrafter    

https://arxiv.org/abs/2311.17528

[Submitted on 29 Nov 2023 (v1), last revised 29 Apr 2024 (this version, v2)]

HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models

扩散模型已成为高分辨率图像合成的主流方法。然而，直接从预训练的扩散模型生成更高分辨率的图像会遇到不合理的对象重复并成倍增加生成时间。在本文中，我们发现对象重复源于 U-Net 深层块中的特征重复。同时，我们将生成时间的延长归因于 U-Net 顶部块中的自注意力冗余。为了解决这些问题，我们提出了一个无需调整的高分辨率框架 HiDiffusion。具体来说，HiDiffusion 包含分辨率感知 U-Net (RAU-Net)，它可以动态调整特征图大小来解决对象重复问题，并使用改进的移位窗口多头自注意力 (MSW-MSA)，利用优化的窗口注意力来减少计算。我们可以将 HiDiffusion 集成到各种预训练的扩散模型中，以将图像生成分辨率扩展到 4096x4096，推理速度是以前方法的 1.5-6 倍。大量实验表明，我们的方法可以解决对象重复和计算量大的问题，在高分辨率图像合成任务上实现最先进的性能。


![alt text](assets/624628/image.png)
















# cfg++

https://github.com/invoke-ai/InvokeAI/issues/6516    
两周前到四天前

https://github.com/invoke-ai/InvokeAI/pull/4335     
Nov 30, 2023    
cfg rescale 已经merge


https://github.com/dunkeroni/InvokeAI_ModularDenoiseNodes


https://gitlab.com/keturn/invert_denoise_invoke/-/tree/invoke-v3.5






CFG++ 与 CFG Rescale 一样，试图解决线性无分类器引导函数容易产生分布外值的方式。

CFG++, like CFG Rescale, is an attempt to address the way the linear Classifier-Free Guidance function is prone to producing out-of-distribution values.


据我了解，数学很简单。但它以一种可怕的方式与调度器在扩散器中的抽象方式（以及 Invoke）发生冲突。我已经创建了这个问题，所以有一个地方可以记录它。

But it clashes in an awful way with how Schedulers are abstracted in diffusers (and thus Invoke). I've created this issue so there's a place to keep notes about that.


Invoke 基于 diffusers构建？?


无分类器引导 (CFG)是现代扩散模型中用于文本引导生成的基本工具。尽管 CFG 很有效，但它需要较高的引导规模，这有明显的缺点：

模式崩溃和饱和
可逆性较差
不自然、弯曲的 PF-ODE 轨迹



我们针对这个看似固有的限制提出了一个简单的解决方案，并提出了 CFG++，它纠正了 CFG 的流形外问题。观察到以下优点

![alt text](assets/624628/image-1.png)

样本质量更好，更符合原文要求
更平滑、更直的 PF-ODE 轨迹
增强可逆性

实验结果证实，我们的方法显著提高了文本到图像生成、DDIM 反转、编辑和解决逆问题的性能，表明在利用文本指导的各个领域具有广泛的影响和潜在的应用。


> [!note]
> This work is currently in the preprint stage, and there may be some changes to the code.


这应该是一个失败项目

CFG++: Manifold-constrained Classifier Free Guidance For Diffusion Models
Hyungjin Chung*, Jeongsol Kim*, Geon Yeong Park*, Hyelin Nam*, Jong Chul Ye
KAIST


![alt text](assets/624628/image-2.png)



官方仅有ddim支持

image edit 论文上看起来效果比较好

文生图比较不稳定

edit如何用？









# 结尾
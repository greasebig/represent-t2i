# hidiffusion
å…³æ³¨comfyuiæ’ä»¶åŸç”Ÿå®ç°

sd.nextå·²ç»æ”¯æŒ    
4.29   
ä¼°è®¡æ˜¯ç›´æ¥ä½¿ç”¨diffusers    




ä¼˜å…ˆçº§åº”è¯¥æ˜¯hidiffusion ç„¶åcfg++     
æˆ–è€…cfgè®¾æ³•hookè¿‡å» å·²æœ‰ä¸€äº›ä»£ç      
ä½†æˆ‘æ„Ÿè§‰æ•ˆæœæ²¡é‚£ä¹ˆå¥½       

æ–‡ç”Ÿå›¾åŠŸèƒ½ sdxlä¼˜å…ˆæ”¯æŒ

Update

2024.6.19 - ğŸ’¥ Integrated into OpenBayes, see the demo. Thank OpenBayes team!

2024.6.16 - ğŸ’¥ Support PyTorch 2.X.

2024.6.16 - ğŸ’¥ Fix non-square generation issue. Now HiDiffusion supports more image sizes and aspect ratios.

2024.5.7 - ğŸ’¥ Support image-to-image task, see here.

2024.4.16 - ğŸ’¥ Release source code.


å¦‚ä½•åœ¨ Automatic1111 Stable Diffusion Web UI ä¸­ä½¿ç”¨å®ƒï¼ˆé€‚ç”¨äº SD 1.5ã€XL ç­‰ï¼‰ #8    
è¿™ä¸ªè®¨è®ºå¤ªæ—© 5.11æœ€æ™š      

ComfyUI æ”¯æŒå—ï¼Ÿ #1     
è¿™ä¸ªä¹Ÿå¾ˆæ—© 5.22æœ€æ™š

https://github.com/blepping/comfyui_jankhidiffusion#use-with-controlnet     
è¿™ä¸ªåŸç”Ÿæ’ä»¶æ›´æ–°äº 5.21æœ€æ™š éš¾ä»¥é€‚ç”¨     


https://github.com/florestefano1975/ComfyUI-HiDiffusion    
è¿™ä¸ªdiffusersåŒ…è£…æ’ä»¶zuiwangengä¿¡èª‰5.4 ä¹Ÿå¤ªæ—©




Supported Tasks
âœ… Text-to-image
âœ… ControlNet, including text-to-image, image-to-image
âœ… Inpainting

Supported Models
âœ… Stable Diffusion XL
âœ… Stable Diffusion XL Turbo
âœ… Stable Diffusion v2
âœ… Stable Diffusion v1
Note: HiDiffusion also supports the downstream diffusion models based on these repositories, such as Ghibli-Diffusion, Playground, etc.

é«˜åˆ†è¾¨ç±»ä¼¼      
Kohya Deep Shrink     
ScaleCrafter    

https://arxiv.org/abs/2311.17528

[Submitted on 29 Nov 2023 (v1), last revised 29 Apr 2024 (this version, v2)]

HiDiffusion: Unlocking Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models

æ‰©æ•£æ¨¡å‹å·²æˆä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼Œç›´æ¥ä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒä¼šé‡åˆ°ä¸åˆç†çš„å¯¹è±¡é‡å¤å¹¶æˆå€å¢åŠ ç”Ÿæˆæ—¶é—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¯¹è±¡é‡å¤æºäº U-Net æ·±å±‚å—ä¸­çš„ç‰¹å¾é‡å¤ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆæ—¶é—´çš„å»¶é•¿å½’å› äº U-Net é¡¶éƒ¨å—ä¸­çš„è‡ªæ³¨æ„åŠ›å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— éœ€è°ƒæ•´çš„é«˜åˆ†è¾¨ç‡æ¡†æ¶ HiDiffusionã€‚å…·ä½“æ¥è¯´ï¼ŒHiDiffusion åŒ…å«åˆ†è¾¨ç‡æ„ŸçŸ¥ U-Net (RAU-Net)ï¼Œå®ƒå¯ä»¥åŠ¨æ€è°ƒæ•´ç‰¹å¾å›¾å¤§å°æ¥è§£å†³å¯¹è±¡é‡å¤é—®é¢˜ï¼Œå¹¶ä½¿ç”¨æ”¹è¿›çš„ç§»ä½çª—å£å¤šå¤´è‡ªæ³¨æ„åŠ› (MSW-MSA)ï¼Œåˆ©ç”¨ä¼˜åŒ–çš„çª—å£æ³¨æ„åŠ›æ¥å‡å°‘è®¡ç®—ã€‚æˆ‘ä»¬å¯ä»¥å°† HiDiffusion é›†æˆåˆ°å„ç§é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥å°†å›¾åƒç”Ÿæˆåˆ†è¾¨ç‡æ‰©å±•åˆ° 4096x4096ï¼Œæ¨ç†é€Ÿåº¦æ˜¯ä»¥å‰æ–¹æ³•çš„ 1.5-6 å€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è§£å†³å¯¹è±¡é‡å¤å’Œè®¡ç®—é‡å¤§çš„é—®é¢˜ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚


![alt text](assets/624628/image.png)

## åŸç†

![alt text](assets_picture/624628/image.png)



    def apply_hidiffusion(
            model: torch.nn.Module,
            apply_raunet: bool = True,
            apply_window_attn: bool = True):
        """
        model: diffusers model. We support SD 1.5, 2.1, XL, XL Turbo.
        
        apply_raunet: whether to apply RAU-Net
        
        apply_window_attn: whether to apply MSW-MSA.
        '''
        # Make sure the module is not currently patched
        remove_hidiffusion(model)







Diffusion models have become a mainstream approach for high-resolution image synthesis. However, directly generating higherresolution images from pretrained diffusion models will encounter unreasonable object duplication and exponentially increase the generation time. In this paper, we discover that object duplication arises from feature duplication in the deep blocks of the U-Net. Concurrently, We pinpoint the extended generation times to self-attention redundancy in U-Netâ€™s top blocks. To address these issues, we propose a tuning-free higher-resolution framework named HiDiffusion. Specifically, HiDiffusion contains Resolution-Aware U-Net (RAU-Net) that dynamically adjusts the feature map size to resolve object duplication and engages Modified Shifted Window Multi-head Self-Attention (MSW-MSA) that utilizes optimized window attention to reduce computations. we can integrate HiDiffusion into various pretrained diffusion models to scale image generation resolutions even to 4096Ã—4096 at 1.5-6Ã— the inference speed of previous methods. Extensive experiments demonstrate that our approach can address object duplication and heavy computation issues, achieving state-of-the-art performance on higher-resolution image synthesis tasks.


![alt text](assets_picture/624628/image-1.png)


ä»–è¿™ä¸ªç«Ÿç„¶ä¸ç”¨è®­ç»ƒ ä¸ç”¨æƒé‡ å°±èƒ½è¿™æ ·è¯¥ç»“æ„


å¦‚æœè¯´cutå±äºpatchæ‰¾è§„å¾‹ é‚£è¿™ä¸ªå°±æœ‰ç‚¹ç¡¬æ ¸

![alt text](assets_picture/624628/image-2.png)


![alt text](assets_picture/624628/image-3.png)



![alt text](assets_picture/624628/image-4.png)


![alt text](assets_picture/624628/image-5.png)


![alt text](assets_picture/624628/image-6.png)



## ä»£ç 
çœ‹èµ·æ¥ä¸»è¦æ˜¯æ¢äº†æ¨¡å‹ç»“æ„å»æ¨ç†      
æœ‰ä¸€äº›é˜ˆå€¼


    # T1_ratio: see T1 introduced in the main paper. T1 = number_inference_step * T1_ratio. A higher T1_ratio can better mitigate object duplication. We set T1_ratio=0.4 by default. You'd better adjust it to fit your prompt. Only active when apply_raunet=True.
    # T2_ratio: see T2 introduced in the appendix, used in extreme resolution image generation. T2 = number_inference_step * T2_ratio. A higher T2_ratio can better mitigate object duplication. Only active when apply_raunet=True
    switching_threshold_ratio_dict = {
        'sd15_1024': {'T1_ratio': 0.4, 'T2_ratio': 0.0},
        'sd15_2048': {'T1_ratio': 0.7, 'T2_ratio': 0.3},
        'sdxl_2048': {'T1_ratio': 0.4, 'T2_ratio': 0.0},
        'sdxl_4096': {'T1_ratio': 0.7, 'T2_ratio': 0.3},
        'sdxl_turbo_1024': {'T1_ratio': 0.5, 'T2_ratio': 0.0},
    }

mitigate object duplication.

    is_aggressive_raunet = True
    aggressive_step = 8


    def make_diffusers_cross_attn_down_block(block_class: Type[torch.nn.Module]) -> Type[torch.nn.Module]:
        # replace conventional downsampler with resolution-aware downsampler


    def make_diffusers_downsampler_block(block_class: Type[torch.nn.Module]) -> Type[torch.nn.Module]:
        # replace conventional downsampler with resolution-aware downsampler


ä¹Ÿä½¿ç”¨unpack



    diffusion_model.info = {
        'size': None, 
        'upsample_size': None,
        'hooks': [], 
        'text_to_img_controlnet': hasattr(model, 'controlnet'),
        'is_inpainting_task': 'inpainting' in name_or_path, 
        'is_playground': 'playground' in name_or_path,
        'pipeline': model}
    model.info = diffusion_model.info
    hook_diffusion_model(diffusion_model)



def hook_diffusion_model(model: torch.nn.Module):

    """ Adds a forward pre hook to get the image size. This hook can be removed with remove_hidiffusion. """
    def hook(module, args):
        module.info["size"] = (args[0].shape[2], args[0].shape[3])
        return None

    model.info["hooks"].append(model.register_forward_pre_hook(hook))

argsæ€ä¹ˆæ¥çš„


ä¸»è¦éœ€è¦æ”¹æ¨¡å‹ç»“æ„     
å…¶å®å°±æ˜¯è°ƒç”¨çš„è¾“å…¥è¾“å‡ºåšå˜åŒ–     

ç›´æ¥æ”¹æ¨¡å‹ç»“æ„ä¹Ÿæ–¹ä¾¿äº› å°±æ˜¯å“ªäº›æ­¥æœ‰å“ªç§è¾“å…¥

å¦‚æœæ”¹æ¨¡å‹ç»“æ„å°±æ˜¯è¦æ”¹kdiffusion

éæ–¹å½¢åˆ†è¾¨ç‡ é—®é¢˜è§£å†³åœ¨    

num_upsamplers

cross_attn_up_block

cross_attn_down_block

make_diffusers_transformer_block

å¦‚æœæ”¹èƒ½æ€ä¹ˆæ”¹

script æ€ä¹ˆä¿®æ”¹æ¨¡å‹ç»“æ„      
å…¶å®ä¹Ÿä¸æ˜¯ä¿®æ”¹æ¨¡å‹ç»“æ„     
æ¯ä¸€å°å—çš„è¾“å…¥è¾“å‡ºåšè°ƒæ•´      

èƒ½æ’å…¥å—       
ç»“æŸåéœ€è¦å¤åŸ

ä»¥patchå½¢å¼å—

ä»–è¿™ä¸ªä¸æ˜¯æ”¹é‡‡æ ·å™¨     



    def sdxl_hidiffusion_key():
        modified_key = dict()
        modified_key['down_module_key'] = ['down_blocks.1']
        modified_key['down_module_key_extra'] = ['down_blocks.1.downsamplers.0.conv']
        modified_key['up_module_key'] = ['up_blocks.1']
        modified_key['up_module_key_extra'] = ['up_blocks.0.upsamplers.0.conv']
        modified_key['windown_attn_module_key'] = ['down_blocks.1.attentions.0.transformer_blocks.0', 
        'down_blocks.1.attentions.0.transformer_blocks.1', 
        'down_blocks.1.attentions.1.transformer_blocks.0',
        'down_blocks.1.attentions.1.transformer_blocks.1',
        'up_blocks.1.attentions.0.transformer_blocks.0', 
        'up_blocks.1.attentions.0.transformer_blocks.1',
        'up_blocks.1.attentions.1.transformer_blocks.0', 
        'up_blocks.1.attentions.1.transformer_blocks.1', 
        'up_blocks.1.attentions.2.transformer_blocks.0', 
        'up_blocks.1.attentions.2.transformer_blocks.1']
        
        return modified_key


æ”¹æ¨¡å‹åªèƒ½æ˜¯pacth hookè¿‡å»

å•çº¯scriptæ”¹ä¸äº† æ”¹æ¨¡å‹ä¿å­˜æ¨¡å‹æ˜¾å­˜ä»£ä»·è¿‡å¤§


https://github.com/kijai/ComfyUI-ELLA-wrapper/blob/main/nodes.py

è¿™ä¸ªæ’ä»¶åªæ˜¯

    from diffusers.loaders.single_file_utils import (
            convert_ldm_vae_checkpoint, 
            convert_ldm_unet_checkpoint, 
            create_vae_diffusers_config, 
            create_unet_diffusers_config,
            create_text_encoder_from_ldm_clip_checkpoint
        )         

https://github.com/invoke-ai/InvokeAI/issues/6309

å®ƒçš„å¼€æºä»£ç æ˜¯åŸºäºæ‰©æ•£å™¨çš„ï¼Œå› æ­¤æ·»åŠ æ­¤åŠŸèƒ½ä¸€å®šç›¸å½“å®¹æ˜“ã€‚

æˆ‘ä»¬åœ¨æ‰©æ•£å™¨å‘¨å›´æœ‰å¾ˆå¤šè‡ªå®šä¹‰é€»è¾‘ï¼Œè€Œâ€œåªéœ€æ·»åŠ ä¸€è¡Œï¼â€å¹¶ä¸ä¸€å®šé€‚ç”¨äºæˆ‘ä»¬çš„å®ç°ã€‚

@RyanJDick @lsteinæ‚¨èƒ½å°±å®æ–½è¿™é¡¹åŠŸèƒ½çš„åŠªåŠ›æå‡ºå»ºè®®å—ï¼Ÿå®ƒå°†å–ä»£ HRO åŠŸèƒ½ï¼ˆè‡ªåŠ¨ç¬¬äºŒé img2imgï¼‰ã€‚

TLDRï¼šæˆ‘è®¤ä¸ºHiDiffusion å¯ä»¥ä»¥ä¸æˆ‘ä»¬æ‰€æœ‰å…¶ä»–åŠŸèƒ½å…¼å®¹çš„æ–¹å¼å¾—åˆ°æ”¯æŒã€‚ä½†æ˜¯ï¼Œè¿™è‚¯å®šä¼šæ¯”ä»–ä»¬å®£ä¼ çš„å•è¡Œä»£ç ä»˜å‡ºæ›´å¤šåŠªåŠ›ã€‚æˆ‘ä»¬åº”è¯¥è¿›è¡Œæ›´å¤šæµ‹è¯•ï¼Œä»¥ç¡®ä¿æ­¤åŠŸèƒ½å€¼å¾—å®æ–½/ç»´æŠ¤ï¼ˆè®ºæ–‡ä¸­çš„ç¤ºä¾‹çœ‹èµ·æ¥å¾ˆæ£’ï¼‰ã€‚


HiDiffusion ä»¥ä¸¤ç§æ–¹å¼ä¿®æ”¹äº† UNetï¼šRAU-Netï¼ˆåˆ†è¾¨ç‡æ„ŸçŸ¥ U-Netï¼‰å’Œ MSW-MSAï¼ˆæ”¹è¿›çš„ç§»ä½çª—å£å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼‰ã€‚è¿™äº›éƒ½æ˜¯å¯¹ UNet çš„æ— éœ€è°ƒæ•´çš„ä¿®æ”¹ï¼Œå³ä¸éœ€è¦æ–°çš„æƒé‡ã€‚

RAU-Net æ—¨åœ¨é¿å…é«˜åˆ†è¾¨ç‡ä¸‹çš„ä¸»é¢˜é‡å¤ã€‚å®ƒé€šè¿‡æ”¹å˜ UNet å±‚çš„ä¸‹é‡‡æ ·/ä¸Šé‡‡æ ·æ¨¡å¼æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™æ ·æ·±å±‚å°±å¯ä»¥ä»¥æ›´æ¥è¿‘å…¶è®­ç»ƒæ—¶çš„åˆ†è¾¨ç‡è¿è¡Œã€‚     
æ ¸å¿ƒè¿˜æ˜¯è¿™ä¸ªä¸œè¥¿    


MSW-MSA ä¿®æ”¹é€šè¿‡å¯¹é¡¶éƒ¨ UNet å—çš„è‡ªæ³¨æ„å±‚åº”ç”¨çª—å£æ¥æ”¹å–„é«˜åˆ†è¾¨ç‡çš„ç”Ÿæˆæ—¶é—´ã€‚


A lot of code in RAU-Net are directly copied from diffusers impl (That is why I do not like diffuers). The main logic seem to be further increase compression / decompression by 2.


å¯¹çš„ã€‚ä»æˆ‘å¼„æ˜ç™½çš„æƒ…å†µæ¥çœ‹ï¼Œé™¤äº† MSW-MSA æ³¨æ„éƒ¨åˆ†ä¹‹å¤–ï¼Œå®ƒå‡ ä¹ä¸ Kohya Deep Shrink å®Œå…¨ä¸€æ ·ã€‚

äº¤å‰æ³¨æ„éƒ¨åˆ†ä½¿ç”¨ Torchavg_pool2dè€Œä¸æ˜¯åŒä¸‰æ¬¡ã€‚
RAU é™é‡‡æ ·å™¨éƒ¨åˆ†ä½¿ç”¨è½¬æ¢æ­¥å¹…/æ‰©å¼ æ¥ç¼©å°è§„æ¨¡ï¼Œè€Œä¸æ˜¯åŒä¸‰æ¬¡ã€‚

å¯¹äºç¬¬äºŒä¸ªï¼Œç¼©å°å‘ç”Ÿçš„ä½ç½®å¯èƒ½å¾ˆé‡è¦ã€‚è½¬æ¢æ–¹æ³•ç¡®å®ä¼¼ä¹æ¯”æ·±åº¦ç¼©å°äº§ç”Ÿæ›´å¥½çš„ç»“æœï¼ˆåŒ…æ‹¬æˆ‘å°è¯•è¿‡çš„å…¶ä»–ç¼©å°æ–¹æ³•ï¼‰ã€‚

æ®æˆ‘æ‰€çŸ¥ï¼ŒRAU-Net éƒ¨åˆ†æœ¬è´¨ä¸Šæ˜¯ Kohya Deep Shrinkï¼ˆåˆåPatchModelAddDownscaleï¼‰ï¼šå…¶æ¦‚å¿µæ˜¯åœ¨ç”Ÿæˆå¼€å§‹æ—¶ç¼©å°å›¾åƒï¼Œè®©æ¨¡å‹è®¾ç½®ä¸»è¦ç»†èŠ‚ï¼Œä¾‹å¦‚è§’è‰²æœ‰å¤šå°‘æ¡è…¿ï¼Œç„¶åå…è®¸æ¨¡å‹åœ¨ç¼©æ”¾æ•ˆæœç»“æŸåç»†åŒ–å’Œæ·»åŠ ç»†èŠ‚ã€‚è¯¥éƒ¨åˆ†çš„ä¸»è¦åŒºåˆ«åœ¨äºç¼©å°æ–¹æ³• - å®ƒä½¿ç”¨å·ç§¯ä¸æ­¥å¹…/æ‰©å¼ å’Œæ± å¹³å‡æ¥ç¼©å°å°ºå¯¸ï¼Œè€Œ Deep Shrink é€šå¸¸ä½¿ç”¨åŒä¸‰æ¬¡ç¼©å°å°ºå¯¸ã€‚ç¼©æ”¾å‘ç”Ÿçš„ä½ç½®ä¹Ÿå¯èƒ½å¾ˆé‡è¦ - å®ƒä¼¼ä¹ç¡®å®æ¯” Deep Shrink æ•ˆæœå¥½å¾—å¤šï¼Œè‡³å°‘å¯¹äº SD 1.5 æ¥è¯´æ˜¯è¿™æ ·ã€‚



    def process(self, p, enable, only_one_pass, d1, d2, s1, s2, scaler, downscale, upscale, smooth_scaling, early_out):
        self.config = DictConfig({name: var for name, var in locals().items() if name not in ['self', 'p']})
        if not enable or self.disable:
            script_callbacks.remove_current_script_callbacks()
            return
        model = p.sd_model.model.diffusion_model
        if s1 > s2: self.config.s2 = s1
        self.p1 = (s1, d1 - 1)
        self.p2 = (s2, d2 - 1)
        self.step_limit = 0
        
        def denoiser_callback(params: script_callbacks.CFGDenoiserParams):
            if params.sampling_step < self.step_limit: return
            for s, d in [self.p1, self.p2]:
                out_d = d if self.config.early_out else -(d + 1)
                if params.sampling_step < params.total_sampling_steps * s:
                    if not isinstance(model.input_blocks[d], Scaler):
                        model.input_blocks[d] = Scaler(self.config.downscale, model.input_blocks[d], self.config.scaler)
                        model.output_blocks[out_d] = Scaler(self.config.upscale, model.output_blocks[out_d], self.config.scaler)
                    elif self.config.smooth_scaling:
                        scale_ratio = params.sampling_step / (params.total_sampling_steps * s)
                        downscale = min((1 - self.config.downscale) * scale_ratio + self.config.downscale, 1.0)
                        model.input_blocks[d].scale = downscale
                        model.output_blocks[out_d].scale = self.config.upscale * (self.config.downscale / downscale)
                    return
                elif isinstance(model.input_blocks[d], Scaler) and (self.p1[1] != self.p2[1] or s == self.p2[0]):
                    model.input_blocks[d] = model.input_blocks[d].block
                    model.output_blocks[out_d] = model.output_blocks[out_d].block
            self.step_limit = params.sampling_step if self.config.only_one_pass else 0

        script_callbacks.on_cfg_denoiser(denoiser_callback)



è¿™ä¸ªç›´æ¥æ”¹cfg denoiseræ„Ÿè§‰å—ç‰ˆæœ¬å½±å“å¤§


    æ£€æŸ¥å½“å‰é‡‡æ ·æ­¥éª¤æ˜¯å¦è¾¾åˆ°å¤„ç†æ¡ä»¶ã€‚
    æ ¹æ®æ¡ä»¶å¯¹æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºå—è¿›è¡Œç¼©æ”¾å¤„ç†ã€‚
    å¦‚æœå¯ç”¨å¹³æ»‘ç¼©æ”¾ï¼Œä¼šæ ¹æ®å½“å‰æ­¥éª¤åŠ¨æ€è°ƒæ•´ç¼©æ”¾æ¯”ä¾‹ã€‚
    åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œæ¢å¤åŸå§‹çš„è¾“å…¥å’Œè¾“å‡ºå—ã€‚

æ³¨å†Œå›è°ƒ:   
pythonCopyscript_callbacks.on_cfg_denoiser(denoiser_callback)     
å°†å®šä¹‰çš„å›è°ƒå‡½æ•°æ³¨å†Œåˆ°å»å™ªå™¨ä¸­ã€‚

è¿™æ®µä»£ç çš„ä¸»è¦ç›®çš„ä¼¼ä¹æ˜¯åœ¨å›¾åƒç”Ÿæˆæˆ–å¤„ç†è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®ä¸åŒçš„é‡‡æ ·é˜¶æ®µåŠ¨æ€è°ƒæ•´æ¨¡å‹çš„æŸäº›å±‚çš„ç¼©æ”¾è¡Œä¸ºã€‚è¿™å¯èƒ½ç”¨äºæé«˜ç”Ÿæˆè´¨é‡æˆ–ä¼˜åŒ–å¤„ç†æ•ˆç‡ã€‚


    æ­¥éª¤é™åˆ¶æ£€æŸ¥ï¼š
    pythonCopyif params.sampling_step < self.step_limit: return
    å¦‚æœå½“å‰é‡‡æ ·æ­¥éª¤å°äº step_limitï¼Œå‡½æ•°ç›´æ¥è¿”å›ã€‚è¿™å¯èƒ½æ˜¯ä¸ºäº†é¿å…é‡å¤å¤„ç†æˆ–é™åˆ¶å¤„ç†é¢‘ç‡ã€‚
    ä¸»å¾ªç¯ï¼š
    pythonCopyfor s, d in [self.p1, self.p2]:
    è¿™ä¸ªå¾ªç¯éå†ä¸¤ç»„å‚æ•° p1 å’Œ p2ï¼Œå®ƒä»¬åˆ†åˆ«ä»£è¡¨ä¸åŒçš„å¤„ç†é˜¶æ®µæˆ–æ¡ä»¶ã€‚

    è¾“å‡ºå±‚ç´¢å¼•è®¡ç®—ï¼š
    pythonCopyout_d = d if self.config.early_out else -(d + 1)
    æ ¹æ®æ˜¯å¦å¯ç”¨ early_outï¼Œè®¡ç®—è¾“å‡ºå±‚çš„ç´¢å¼•ã€‚
    ä¸»è¦å¤„ç†é€»è¾‘ï¼š
    pythonCopyif params.sampling_step < params.total_sampling_steps * s:
    è¿™ä¸ªæ¡ä»¶æ£€æŸ¥å½“å‰æ˜¯å¦å¤„äºéœ€è¦è¿›è¡Œç¼©æ”¾å¤„ç†çš„é‡‡æ ·é˜¶æ®µã€‚
    ç¼©æ”¾å™¨æ›¿æ¢ï¼š
    pythonCopyif not isinstance(model.input_blocks[d], Scaler):
        model.input_blocks[d] = Scaler(self.config.downscale, model.input_blocks[d], self.config.scaler)
        model.output_blocks[out_d] = Scaler(self.config.upscale, model.output_blocks[out_d], self.config.scaler)
    å¦‚æœå½“å‰è¾“å…¥å—ä¸æ˜¯ Scaler ç±»å‹ï¼Œåˆ™ç”¨ Scaler å¯¹è±¡æ›¿æ¢è¾“å…¥å’Œè¾“å‡ºå—ã€‚è¿™å®ç°äº†åŠ¨æ€ç¼©æ”¾åŠŸèƒ½ã€‚


    å¹³æ»‘ç¼©æ”¾ï¼š
    pythonCopyelif self.config.smooth_scaling:
        scale_ratio = params.sampling_step / (params.total_sampling_steps * s)
        downscale = min((1 - self.config.downscale) * scale_ratio + self.config.downscale, 1.0)
        model.input_blocks[d].scale = downscale
        model.output_blocks[out_d].scale = self.config.upscale * (self.config.downscale / downscale)
    å¦‚æœå¯ç”¨äº†å¹³æ»‘ç¼©æ”¾ï¼Œæ ¹æ®å½“å‰æ­¥éª¤åŠ¨æ€è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œå¹¶åº”ç”¨åˆ°è¾“å…¥å’Œè¾“å‡ºå—ã€‚
    æ¢å¤åŸå§‹å—ï¼š
    pythonCopyelif isinstance(model.input_blocks[d], Scaler) and (self.p1[1] != self.p2[1] or s == self.p2[0]):
        model.input_blocks[d] = model.input_blocks[d].block
        model.output_blocks[out_d] = model.output_blocks[out_d].block
    åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆæ¯”å¦‚å¤„ç†é˜¶æ®µç»“æŸï¼‰ï¼Œå°†è¢« Scaler æ›¿æ¢çš„å—æ¢å¤ä¸ºåŸå§‹å—ã€‚

    æ›´æ–°æ­¥éª¤é™åˆ¶ï¼š
    pythonCopyself.step_limit = params.sampling_step if self.config.only_one_pass else 0
    å¦‚æœé…ç½®ä¸ºåªå¤„ç†ä¸€æ¬¡ï¼Œæ›´æ–° step_limit ä»¥é˜²æ­¢é‡å¤å¤„ç†ã€‚



éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªå‡½æ•°å‡è®¾æ¨¡å‹ç»“æ„ä¸­æœ‰ input_blocks å’Œ output_blocksï¼Œå¹¶ä¸”å¯ä»¥åŠ¨æ€æ›¿æ¢è¿™äº›å—çš„å®ç°ã€‚è¿™è¡¨æ˜å®ƒå¯èƒ½æ˜¯é’ˆå¯¹ç‰¹å®šç±»å‹çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æŸäº›æ‰©æ•£æ¨¡å‹ï¼‰è®¾è®¡çš„ã€‚



    class Scaler(torch.nn.Module):
        def __init__(self, scale, block, scaler):
            super().__init__()
            self.scale = scale
            self.block = block
            self.scaler = scaler
            
        def forward(self, x, *args):
            x = torch.nn.functional.interpolate(x, scale_factor=self.scale, mode=self.scaler)
            return self.block(x, *args)

å…ˆç¼©æ”¾ å†è¿›å…¥æ¨¡å—ï¼Ÿï¼Ÿ


è¿™ä¸ª Scaler ç±»çš„ä¸»è¦ç›®çš„æ˜¯åœ¨ç¥ç»ç½‘ç»œçš„æŸä¸ªå—ä¹‹å‰æ·»åŠ ä¸€ä¸ªç¼©æ”¾æ“ä½œã€‚å®ƒå¯ä»¥ç”¨æ¥è°ƒæ•´ç‰¹å¾å›¾çš„å¤§å°ï¼Œå¯èƒ½ç”¨äºä¸Šé‡‡æ ·æˆ–ä¸‹é‡‡æ ·ã€‚
ä½¿ç”¨è¿™ä¸ªç±»çš„å¥½å¤„åŒ…æ‹¬ï¼š

çµæ´»æ€§ï¼šå¯ä»¥è½»æ¾åœ°åœ¨ç½‘ç»œçš„ä»»ä½•éƒ¨åˆ†æ·»åŠ ç¼©æ”¾æ“ä½œã€‚
å¯é…ç½®ï¼šç¼©æ”¾å› å­å’Œæ–¹æ³•å¯ä»¥åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šã€‚
é€æ˜æ€§ï¼šåŸå§‹çš„ç½‘ç»œå—ä»ç„¶è¢«ä¿ç•™å’Œä½¿ç”¨ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸ª Scaler ç±»å¯èƒ½è¢«ç”¨äºåŠ¨æ€è°ƒæ•´ç½‘ç»œæŸäº›å±‚çš„è¾“å…¥å¤§å°ï¼Œè¿™åœ¨å›¾åƒç”Ÿæˆæˆ–å¤„ç†ä»»åŠ¡ä¸­å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹ä¿æŒæˆ–è°ƒæ•´ç‰¹å¾çš„è¡¨ç¤ºã€‚


    åˆå§‹åŒ–æ–¹æ³•ï¼š
    pythonCopydef __init__(self, scale, block, scaler):
        super().__init__()
        self.scale = scale
        self.block = block
        self.scaler = scaler

scale: ç¼©æ”¾å› å­
block: è¦åŒ…è£…çš„åŸå§‹ç¥ç»ç½‘ç»œå—
scaler: ç¼©æ”¾æ–¹æ³•ï¼ˆå¦‚ 'bilinear', 'nearest' ç­‰ï¼‰



ç›´æ¥æ›¿æ¢BLOCK

class Scaler(torch.nn.Module):

è¿™é‡Œé¢ä¿å­˜äº†å±æ€§ä¸€ä¸ªblock


æˆ–è€…åƒ cnä¸€æ ·hijack??      
cnæ˜¯å¤§å±‚æ”¹å˜è¾“å…¥

hidiffusionå…¶å®å°±æ˜¯å‹ç¼©æ–¹å¼æ”¹å˜äº† æ•°æ®çš„å‹ç¼©æ–¹å¼      


å…ˆå°è¯•åªå®ç°raunet


## æ’å…¥æ–¹å¼

scripts.script_callbacks.on_cfg_denoiser(self.guidance_schedule_handler)


onediffçš„cn
















# onediff
è½¯è¿å¤±è´¥

ln: failed to create symbolic link '/m': Function not implemented

æœªçŸ¥åŸå› é”™è¯¯

mntå†…

teamå†…

teamåˆ°mntéƒ½ä¸è¡Œ

mntåˆ°teamsä¹Ÿä¸è¡Œ



åœ¨Linuxå‘½ä»¤è¡Œä¸­ï¼Œå¯ä»¥ä½¿ç”¨ find å‘½ä»¤æ¥æœç´¢ä¸€ä¸ªæ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰è½¯è¿æ¥ã€‚ä¸‹é¢æ˜¯å…·ä½“çš„å‘½ä»¤ï¼š

    bash
    å¤åˆ¶ä»£ç 
    find /path/to/directory -type l



ç›´æ¥ä¸è½¯è¿æ¥    
å¤åˆ¶è¿‡å»    
ä½†æ˜¯å¥½åƒæ²¡æœ‰è¿è¡Œinstall.py

æ‰‹è£… pip install git+https://github.com/siliconflow/onediff.git

Successfully installed onediff-1.2.0.dev1

å¯ä»¥äº†

ä½†æ˜¯æˆ‘åœ¨1åŒº 5åŒºéƒ½æ²¡æœ‰çœ‹è§è½¯è¿æ¥

æ¨ç†æŠ¥é”™

æ‰‹è£…

pip install --pre oneflow -f https://oneflow-pro.oss-cn-beijing.aliyuncs.com/branch/community/cu118


oneflowå·²ç»è¢«é˜¿é‡Œæ”¶è´­

å†å»ºonediff

Collecting oneflow    
Downloading https://oneflow-pro.oss-cn-beijing.aliyuncs.com/branch/community/cu118/ec7b682e30831cc0eb30d7cc07d4dcb366c588cd/oneflow-0.9.1.dev20240703%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1422.5 MB)

Collecting nvidia-cudnn-cu11 (from oneflow)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/40/8e/111f88f108cbad7b8fd293fdeb2a7a251205feb48adb504c6caecd0e20e3/nvidia_cudnn_cu11-9.2.0.82-py3-none-manylinux2014_x86_64.whl (572.1 MB)

Collecting nvidia-cublas-cu11 (from oneflow)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/46/be/c222e33e60d28ecd496a46fc4d78ccae0ee28e1fd7dc705b6288b4cad27e/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)



Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cudnn-cu11-9.2.0.82 nvidia-cufft-cu11-10.9.0.58 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 oneflow-0.9.1.dev20240703+cu118


è½¯è¿åªå­˜åœ¨äºæœåŠ¡å¯åŠ¨


åŠ å¤šä½™å‘½ä»¤ å¯åŠ¨å¾ˆæ…¢ï¼Ÿï¼Ÿ

è¿˜æ˜¯è¯´å› ä¸ºæˆ‘æŠŠæ‰€æœ‰cnè¾“å…¥è¿›å»ä»–è¦ä¸€ä¸ªä¸ªç¼–è¯‘ï¼Ÿè¿™ä¸ªä¸æ˜¯


å»æ‰ä¸€äº›å‚æ•°ç¡®å®å¯ä»¥






















# cfg++


## åŸºæœ¬ä¿¡æ¯
https://github.com/invoke-ai/InvokeAI/issues/6516    
ä¸¤å‘¨å‰åˆ°å››å¤©å‰

https://github.com/invoke-ai/InvokeAI/pull/4335     
Nov 30, 2023    
cfg rescale å·²ç»merge


https://github.com/dunkeroni/InvokeAI_ModularDenoiseNodes


https://gitlab.com/keturn/invert_denoise_invoke/-/tree/invoke-v3.5






CFG++ ä¸ CFG Rescale ä¸€æ ·ï¼Œè¯•å›¾è§£å†³çº¿æ€§æ— åˆ†ç±»å™¨å¼•å¯¼å‡½æ•°å®¹æ˜“äº§ç”Ÿåˆ†å¸ƒå¤–å€¼çš„æ–¹å¼ã€‚

CFG++, like CFG Rescale, is an attempt to address the way the linear Classifier-Free Guidance function is prone to producing out-of-distribution values.


æ®æˆ‘äº†è§£ï¼Œæ•°å­¦å¾ˆç®€å•ã€‚ä½†å®ƒä»¥ä¸€ç§å¯æ€•çš„æ–¹å¼ä¸è°ƒåº¦å™¨åœ¨æ‰©æ•£å™¨ä¸­çš„æŠ½è±¡æ–¹å¼ï¼ˆä»¥åŠ Invokeï¼‰å‘ç”Ÿå†²çªã€‚æˆ‘å·²ç»åˆ›å»ºäº†è¿™ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªåœ°æ–¹å¯ä»¥è®°å½•å®ƒã€‚

But it clashes in an awful way with how Schedulers are abstracted in diffusers (and thus Invoke). I've created this issue so there's a place to keep notes about that.


Invoke åŸºäº diffusersæ„å»ºï¼Ÿ?


æ— åˆ†ç±»å™¨å¼•å¯¼ (CFG)æ˜¯ç°ä»£æ‰©æ•£æ¨¡å‹ä¸­ç”¨äºæ–‡æœ¬å¼•å¯¼ç”Ÿæˆçš„åŸºæœ¬å·¥å…·ã€‚å°½ç®¡ CFG å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒéœ€è¦è¾ƒé«˜çš„å¼•å¯¼è§„æ¨¡ï¼Œè¿™æœ‰æ˜æ˜¾çš„ç¼ºç‚¹ï¼š

æ¨¡å¼å´©æºƒå’Œé¥±å’Œ
å¯é€†æ€§è¾ƒå·®
ä¸è‡ªç„¶ã€å¼¯æ›²çš„ PF-ODE è½¨è¿¹



æˆ‘ä»¬é’ˆå¯¹è¿™ä¸ªçœ‹ä¼¼å›ºæœ‰çš„é™åˆ¶æå‡ºäº†ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº† CFG++ï¼Œå®ƒçº æ­£äº† CFG çš„æµå½¢å¤–é—®é¢˜ã€‚è§‚å¯Ÿåˆ°ä»¥ä¸‹ä¼˜ç‚¹

![alt text](assets/624628/image-1.png)

æ ·æœ¬è´¨é‡æ›´å¥½ï¼Œæ›´ç¬¦åˆåŸæ–‡è¦æ±‚
æ›´å¹³æ»‘ã€æ›´ç›´çš„ PF-ODE è½¨è¿¹
å¢å¼ºå¯é€†æ€§

å®éªŒç»“æœè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€DDIM åè½¬ã€ç¼–è¾‘å’Œè§£å†³é€†é—®é¢˜çš„æ€§èƒ½ï¼Œè¡¨æ˜åœ¨åˆ©ç”¨æ–‡æœ¬æŒ‡å¯¼çš„å„ä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„å½±å“å’Œæ½œåœ¨çš„åº”ç”¨ã€‚


> [!note]
> This work is currently in the preprint stage, and there may be some changes to the code.


è¿™åº”è¯¥æ˜¯ä¸€ä¸ªå¤±è´¥é¡¹ç›®

CFG++: Manifold-constrained Classifier Free Guidance For Diffusion Models
Hyungjin Chung*, Jeongsol Kim*, Geon Yeong Park*, Hyelin Nam*, Jong Chul Ye
KAIST


![alt text](assets/624628/image-2.png)



å®˜æ–¹ä»…æœ‰ddimæ”¯æŒ

image edit è®ºæ–‡ä¸Šçœ‹èµ·æ¥æ•ˆæœæ¯”è¾ƒå¥½

æ–‡ç”Ÿå›¾æ¯”è¾ƒä¸ç¨³å®š

editå¦‚ä½•ç”¨ï¼Ÿ


## å·²ç»mergeåˆ°dev
æ–‡ç”Ÿå›¾ä¹‹å‰commitè‡ªæµ‹æ•ˆæœä¸å¥½


    git pull
    Updating 663a4d80..1da49079
    Fast-forward
    .github/ISSUE_TEMPLATE/bug_report.yml          |   2 +-
    README.md                                      |   2 +-
    configs/sd3-inference.yaml                     |   5 +
    extensions-builtin/LDSR/sd_hijack_ddpm_v1.py   |   2 +-
    extensions-builtin/Lora/extra_networks_lora.py |  23 +-
    extensions-builtin/Lora/network.py             |   4 +-
    extensions-builtin/Lora/networks.py            |   4 +-
    javascript/imageviewer.js                      |  17 ++
    javascript/ui.js                               |   8 +
    modules/api/api.py                             |   4 +-
    modules/cmd_args.py                            |   2 +-
    modules/deepbooru.py                           |   2 +-
    modules/devices.py                             |   2 +-
    modules/gfpgan_model.py                        |   4 +-
    modules/lowvram.py                             |  28 ++-
    modules/modelloader.py                         |  32 ++-
    modules/models/diffusion/uni_pc/uni_pc.py      |   2 +-
    modules/models/sd3/mmdit.py                    | 619 ++++++++++++++++++++++++++++++++++++++++++++++
    modules/models/sd3/other_impls.py              | 508 +++++++++++++++++++++++++++++++++++++
    modules/models/sd3/sd3_cond.py                 | 218 ++++++++++++++++
    modules/models/sd3/sd3_impls.py                | 373 ++++++++++++++++++++++++++++
    modules/models/sd3/sd3_model.py                |  84 +++++++
    modules/postprocessing.py                      |   2 +-
    modules/processing.py                          |  12 +-
    modules/prompt_parser.py                       |   2 +-
    modules/sd_hijack.py                           |   5 +-
    modules/sd_hijack_clip.py                      |  59 +++--
    modules/sd_models.py                           | 101 ++++++--
    modules/sd_models_config.py                    |  14 +-
    modules/sd_models_types.py                     |   6 +
    modules/sd_samplers.py                         |   6 +-
    modules/sd_samplers_cfg_denoiser.py            |  16 +-
    modules/sd_samplers_common.py                  |   6 +-
    modules/sd_samplers_kdiffusion.py              |  16 +-
    modules/sd_samplers_timesteps_impl.py          |   3 +
    modules/sd_schedulers.py                       |  41 +++
    modules/sd_vae_approx.py                       |  27 +-
    modules/sd_vae_taesd.py                        |  40 ++-
    modules/shared.py                              |   2 +-
    modules/shared_gradio_themes.py                |  41 +++
    modules/shared_options.py                      |   5 +-
    modules/torch_utils.py                         |   5 +-
    modules/ui_extra_networks_user_metadata.py     |   2 +-
    modules/ui_gradio_extensions.py                |   5 +
    modules/util.py                                |   2 +-
    requirements.txt                               |   1 +
    requirements_versions.txt                      |   4 +-
    scripts/xyz_grid.py                            |   1 -
    webui-macos-env.sh                             |   7 +-
    webui.sh                                       |   8 +-
    50 files changed, 2252 insertions(+), 132 deletions(-)
    create mode 100644 configs/sd3-inference.yaml
    create mode 100644 modules/models/sd3/mmdit.py
    create mode 100644 modules/models/sd3/other_impls.py
    create mode 100644 modules/models/sd3/sd3_cond.py
    create mode 100644 modules/models/sd3/sd3_impls.py
    create mode 100644 modules/models/sd3/sd3_model.py

## åŸºäº1.10.0 rc å¼€å‘æ’ä»¶

ä¸ddimå·®å¼‚

    e_t = model(x, timesteps[index].item() * s_in, **extra_args)
    last_noise_uncond = model.last_noise_uncond æ–°å¢è¡Œ

    dir_xt = (1. - a_prev - sigma_t ** 2).sqrt() * e_t åŸddim

    dir_xt = (1. - a_prev - sigma_t ** 2).sqrt() * last_noise_uncond æ–°ddim




ç–‘æƒ‘æ˜¯ ä¸ºä»€ä¹ˆlast_noiseå®šä¹‰åœ¨sampleä¹‹å é‚£æ€ä¹ˆè¾“å…¥ä¿¡æ¯ï¼Ÿ


    if is_cfg_pp:
        self.last_noise_uncond = x_out[-uncond.shape[0]:]
        self.last_noise_uncond = torch.clone(self.last_noise_uncond)

    if is_edit_model:
        denoised = self.combine_denoised_for_edit_model(x_out, cond_scale)
    elif skip_uncond:
        denoised = self.combine_denoised(x_out, conds_list, uncond, 1.0)
    elif is_cfg_pp:
        denoised = self.combine_denoised(x_out, conds_list, uncond, cond_scale/12.5) # CFG++ scale of (0, 1) maps to (1.0, 12.5)


## ä»£ç é¡ºåº

modules/processing.py

### def process_images(p: StableDiffusionProcessing) -> Processed:

res = process_images_inner(p)


### def process_images_inner(p: StableDiffusionProcessing) -> Processed:

samples_ddim = p.sample(conditioning=p.c, unconditional_conditioning=p.uc, seeds=p.seeds, subseeds=p.subseeds, subseed_strength=p.subseed_strength, prompts=p.prompts)


### class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):


def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):

samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))


### class CompVisSampler(sd_samplers_common.Sampler):

modules/sd_samplers_timesteps.py

def sample(self, p, x, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):

samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))


### class Sampler:
modules/sd_samplers_common.py

def launch_sampling(self, steps, func):
        self.model_wrap_cfg.steps = steps
        self.model_wrap_cfg.total_steps = self.config.total_steps(steps)
        state.sampling_steps = steps
        state.sampling_step = 0

        try:
            return func()


### samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))

modules/sd_samplers_timesteps.py

### context

    @functools.wraps(func)
    def decorate_context(*args, **kwargs):
        with ctx_factory():
            return func(*args, **kwargs)

    return decorate_context

### modules/sd_samplers_timesteps_impl.py


def ddim_cfgpp(model, x, timesteps, extra_args=None, callback=None, disable=None, eta=0.0):


e_t = model(x, timesteps[index].item() * s_in, **extra_args)



### class CFGDenoiser(torch.nn.Module):
modules/sd_samplers_cfg_denoiser.py

def forward(self, x, sigma, uncond, cond, cond_scale, s_min_uncond, image_cond):

    if tensor.shape[1] == uncond.shape[1] or skip_uncond:
        if shared.opts.batch_cond_uncond:
            x_out = self.inner_model(x_in, sigma_in, cond=make_condition_dict(cond_in, image_cond_in))

    if self.need_last_noise_uncond:
        self.last_noise_uncond = torch.clone(x_out[-uncond.shape[0]:])

    if is_edit_model:
        denoised = self.combine_denoised_for_edit_model(x_out, cond_scale * self.cond_scale_miltiplier)
    elif skip_uncond:
        denoised = self.combine_denoised(x_out, conds_list, uncond, 1.0)
    else:
        denoised = self.combine_denoised(x_out, conds_list, uncond, cond_scale * self.cond_scale_miltiplier)


å†è¿›å…¥è¿™ä¸ªinner å‡ºæ¥ååšcfg è¿™æ—¶éœ€è¦æ’å…¥ä¸€äº›åˆ¤æ–­


![alt text](assets/624628/image-13.png)


é‡Œé¢è¿˜æœ‰å¥½å‡ å±‚


![alt text](assets/624628/image-14.png)


![alt text](assets/624628/image-15.png)


æ¨¡å‹åè¿˜æ˜¯ä¸€è‡´  å°±æ˜¯è¿›å»çš„æ—¶å€™hookåˆåŒ…äº†

æ„Ÿè§‰é‡å†™cfg_denoisersä¸å¤ªç°å®ï¼Ÿ     
æˆ–è€…åªæ˜¯åœ¨ddimcfgppä½¿ç”¨æ—¶å€™é‡å†™ ä¹‹åè¿˜åŸ

    e_t = model(x, timesteps[index].item() * s_in, **extra_args) åº”è¯¥ç†è§£ä¸ºæ˜¯æ¨¡å‹é¢„æµ‹çš„å™ªå£°
    last_noise_uncond = model.last_noise_uncond

è¿™ä¸¤ä¸ªéƒ½æ˜¯[1,4,64,64]

dir_xt = (1. - a_prev - sigma_t ** 2).sqrt() * last_noise_uncond

ä»…è¿™é‡Œè®¡ç®—æ—¶ä¸ä¸€æ ·

æ¥æºï¼š

    x_out = self.inner_model(x_in, sigma_in, cond=make_condition_dict(cond_in, image_cond_in))

    if self.need_last_noise_uncond:
        self.last_noise_uncond = torch.clone(x_out[-uncond.shape[0]:])

    if is_edit_model:
        denoised = self.combine_denoised_for_edit_model(x_out, cond_scale * self.cond_scale_miltiplier)
    elif skip_uncond:
        denoised = self.combine_denoised(x_out, conds_list, uncond, 1.0)
    else:
        denoised = self.combine_denoised(x_out, conds_list, uncond, cond_scale * self.cond_scale_miltiplier)

    denoised = after_cfg_callback_params.x

    self.step += 1
    return denoised


å¥½åƒå°±æ˜¯dir_xtçš„è®¡ç®—å› å­æœ‰ä¸€ä¸ªä¸ä½¿ç”¨cfg

    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()
    dir_xt = (1. - a_prev - sigma_t ** 2).sqrt() * last_noise_uncond
    noise = sigma_t * k_diffusion.sampling.torch.randn_like(x)
    x = a_prev.sqrt() * pred_x0 + dir_xt + noise

ä¹Ÿå°±æ˜¯çº¯é«˜æ–¯å™ªå£°é€æ­¥å‡å»é¢„æµ‹å™ªå£°ï¼Œç®—å‡ºx

    if opts.live_preview_content == "Prompt":
        preview = self.sampler.last_latent
    elif opts.live_preview_content == "Negative prompt":
        preview = self.get_pred_x0(x_in[-uncond.shape[0]:], x_out[-uncond.shape[0]:], sigma)
    else:
        preview = self.get_pred_x0(torch.cat([x_in[i:i+1] for i in denoised_image_indexes]), torch.cat([denoised[i:i+1] for i in denoised_image_indexes]), sigma)


å°±æ˜¯éœ€è¦ x_out ä¸­ neg çš„ç»“æœï¼Œ     
å¯æƒœcfgdenoiserä¸èƒ½è¿”å› åªè¿”å›ä¸€ä¸ªcfgç»“æœ       
è¿™ä¸ªæ€ä¹ˆhook?         
å› ä¸ºè¿™æ˜¯å‡½æ•°ä¸­é—´çš„ç»“æœ       
åœ¨å‡½æ•°å‰åhookéƒ½æ²¡ç”¨        
ä¸­é—´ä¸çŸ¥é“æœ‰ä»€ä¹ˆå¯ä»¥ hook       

x_out = self.inner_model(x_in, sigma_in, cond=make_condition_dict(cond_in, image_cond_in))

hookåˆ°inneræˆªå–ç»“æœå¯èƒ½å¯è¡Œ

self.last_noise_uncond = torch.clone(x_out[-uncond.shape[0]:])

ç›®æ ‡æ˜¯è¿™ä¸ª

cfgå¸Œæœ›ä¹Ÿèƒ½æˆªæ–­ä¿®æ”¹ ç¡®å®å¯ä»¥æˆªæ–­ å°±åœ¨extra_args

model.cond_scale_miltiplier = 1 / 12.5

denoised = self.combine_denoised(x_out, conds_list, uncond, cond_scale * self.cond_scale_miltiplier)

def forward(self, x, sigma, uncond, cond, cond_scale, s_min_uncond, image_cond):


### x_out

s_in = x.new_ones((x.shape[0]))

e_t = model(x, timesteps[index].item() * s_in, **extra_args)



    if not is_edit_model:
        x_in = torch.cat([torch.stack([x[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [x])
        sigma_in = torch.cat([torch.stack([sigma[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [sigma])
        image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_uncond])

    if skip_uncond:
        x_in = x_in[:-batch_size]
        sigma_in = sigma_in[:-batch_size]

    def catenate_conds(conds):
        if not isinstance(conds[0], dict):
            return torch.cat(conds) æ‰§è¡Œ

        return {key: torch.cat([x[key] for x in conds]) for key in conds[0].keys()}


    if shared.opts.pad_cond_uncond_v0 and tensor.shape[1] != uncond.shape[1]:
        tensor, uncond = self.pad_cond_uncond_v0(tensor, uncond)
    elif shared.opts.pad_cond_uncond and tensor.shape[1] != uncond.shape[1]:
        tensor, uncond = self.pad_cond_uncond(tensor, uncond)

    if tensor.shape[1] == uncond.shape[1] or skip_uncond:

    tensor torch.Size([1, 77, 768])
    uncond torch.Size([1, 77, 768])

        if is_edit_model:
                cond_in = catenate_conds([tensor, uncond, uncond])
            elif skip_uncond:
                cond_in = tensor
            else:
                cond_in = catenate_conds([tensor, uncond])

                cond_in torch.Size([2, 77, 768])

            if shared.opts.batch_cond_uncond:
                x_out = self.inner_model(x_in, sigma_in, cond=make_condition_dict(cond_in, image_cond_in))

### modules/sd_samplers_timesteps.py

class CFGDenoiserTimesteps(CFGDenoiser):

    def __init__(self, sampler):
        super().__init__(sampler)

        self.alphas = shared.sd_model.alphas_cumprod
        self.mask_before_denoising = True

    def get_pred_x0(self, x_in, x_out, sigma):
        ts = sigma.to(dtype=int)

        a_t = self.alphas[ts][:, None, None, None]
        sqrt_one_minus_at = (1 - a_t).sqrt()

        pred_x0 = (x_in - sqrt_one_minus_at * x_out) / a_t.sqrt()

        return pred_x0

    @property
    def inner_model(self):
        if self.model_wrap is None:
            denoiser = CompVisTimestepsVDenoiser if shared.sd_model.parameterization == "v" else CompVisTimestepsDenoiser
            self.model_wrap = denoiser(shared.sd_model)

        return self.model_wrap


class CompVisTimestepsDenoiser(torch.nn.Module):

    def __init__(self, model, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.inner_model = model

    def forward(self, input, timesteps, **kwargs):
        return self.inner_model.apply_model(input, timesteps, **kwargs)




è¿è¡Œ

cond=make_condition_dict(cond_in, image_cond_in)

    if shared.sd_model.model.conditioning_key == "crossattn-adm":
        image_uncond = torch.zeros_like(image_cond)
        make_condition_dict = lambda c_crossattn, c_adm: {"c_crossattn": [c_crossattn], "c_adm": c_adm}
    else:
        image_uncond = image_cond
        if isinstance(uncond, dict):
            make_condition_dict = lambda c_crossattn, c_concat: {**c_crossattn, "c_concat": [c_concat]} 
        else:
            make_condition_dict = lambda c_crossattn, c_concat: {"c_crossattn": [c_crossattn], "c_concat": [c_concat]} è¿è¡Œ

å°±æ˜¯ç®€å•çš„åˆ—è¡¨æ„å»ºå­—å…¸å‡½æ•°



### modules/sd_hijack_unet.py
è¿™ä¸ªçœ‹èµ·æ¥æœ‰åŠ©äºhidiffusion


    # Below are monkey patches to enable upcasting a float16 UNet for float32 sampling
    def apply_model(orig_func, self, x_noisy, t, cond, **kwargs):
        """Always make sure inputs to unet are in correct dtype."""
        if isinstance(cond, dict):
            for y in cond.keys():
                if isinstance(cond[y], list):
                    cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]
                else:
                    cond[y] = cond[y].to(devices.dtype_unet) if isinstance(cond[y], torch.Tensor) else cond[y]

        with devices.autocast():
            result = orig_func(self, x_noisy.to(devices.dtype_unet), t.to(devices.dtype_unet), cond, **kwargs)
            if devices.unet_needs_upcast:
                return result.float()
            else:
                return result


### x_out
x_in torch.Size([2, 4, 64, 64])     
x_out ä»inner modelå‡ºæ¥     
torch.Size([2, 4, 64, 64])     

    denoised_params = CFGDenoisedParams(x_out, state.sampling_step, state.sampling_steps, self.inner_model)
    cfg_denoised_callback(denoised_params)


def cfg_denoised_callback(params: CFGDenoisedParams):

    for c in ordered_callbacks('cfg_denoised'): ç›®å‰æ²¡æœ‰è°ƒç”¨ ç›´æ¥è¿”å›
        try:
            c.callback(params)
        except Exception:
            report_exception(c, 'cfg_denoised_callback')


category 'cfg_denoised'


def ordered_callbacks(category, unordered_callbacks=None, *, enable_user_sort=True):

    if unordered_callbacks is None:
        unordered_callbacks = callback_map.get('callbacks_' + category, []) è¿›å…¥

    if not enable_user_sort:
        return sort_callbacks(category, unordered_callbacks, enable_user_sort=False)

    callbacks = ordered_callbacks_map.get(category)
    if callbacks is not None and len(callbacks) == len(unordered_callbacks):
        return callbacks

    callbacks = sort_callbacks(category, unordered_callbacks)

    ordered_callbacks_map[category] = callbacks
    return callbacks

self.last_noise_uncond = torch.clone(x_out[-uncond.shape[0]:])

å¦‚æœæ‚¨æƒ³è¦è·å–æœ€åä¸€ä¸ªå…ƒç´ åˆ°ç»“å°¾,æ­£ç¡®çš„åˆ‡ç‰‡åº”è¯¥æ˜¯ x[-1:]ã€‚











### è€ƒè™‘ç‚¹

modules/script_callbacks.py


    def on_cfg_denoiser(callback, *, name=None):
        """register a function to be called in the kdiffussion cfg_denoiser method after building the inner model inputs.
        The callback is called with one argument:
            - params: CFGDenoiserParams - parameters to be passed to the inner model and sampling state details.
        """
        add_callback(callback_map['callbacks_cfg_denoiser'], callback, name=name, category='cfg_denoiser')


    def on_cfg_denoised(callback, *, name=None):
        """register a function to be called in the kdiffussion cfg_denoiser method after building the inner model inputs.
        The callback is called with one argument:
            - params: CFGDenoisedParams - parameters to be passed to the inner model and sampling state details.
        """
        add_callback(callback_map['callbacks_cfg_denoised'], callback, name=name, category='cfg_denoised')


    def on_cfg_after_cfg(callback, *, name=None):
        """register a function to be called in the kdiffussion cfg_denoiser method after cfg calculations are completed.
        The callback is called with one argument:
            - params: AfterCFGCallbackParams - parameters to be passed to the script for post-processing after cfg calculation.
        """
        add_callback(callback_map['callbacks_cfg_after_cfg'], callback, name=name, category='cfg_after_cfg')


### å®ç°
æˆ‘åœ¨æ–‡ä»¶aä¸­æœ‰è¿™è¡Œä»£ç ï¼Œcfg_denoised_callback(denoised_params)ã€‚æˆ‘æƒ³åœ¨æ–‡ä»¶bä¸­è®¾ç½®ä¸ªå›è°ƒï¼Œè¿›è¡Œä¸€äº›æ•°æ®è·å–æ“ä½œï¼Œè¯¥æ€ä¹ˆå®ç°ã€‚

class MyCallback:
    def callback(self, params: CFGDenoisedParams):
        # åœ¨è¿™é‡Œè¿›è¡Œæ•°æ®è·å–æ“ä½œ
        print(f"x_out: {params.x}")
        # å¯ä»¥è®¿é—®paramsä¸­çš„å…¶ä»–å±æ€§è¿›è¡Œæ›´å¤šæ“ä½œ


from modules import script_callbacks

def on_cfg_denoised(params: CFGDenoisedParams):
    MyCallback().callback(params)

script_callbacks.on_cfg_denoised(on_cfg_denoised)


è¯¥ç”¨åŸæœ‰å·²ç»å®šä¹‰çš„æ–¹æ³•

    def on_cfg_denoised(callback, *, name=None):
        """register a function to be called in the kdiffussion cfg_denoiser method after building the inner model inputs.
        The callback is called with one argument:
            - params: CFGDenoisedParams - parameters to be passed to the inner model and sampling state details.
        """
        add_callback(callback_map['callbacks_cfg_denoised'], callback, name=name, category='cfg_denoised')



è¿™ç§æ–¹æ³•æ¯”ä¹‹å‰å»ºè®®çš„æ–¹æ³•æ›´åŠ ç›´æ¥å’Œç®€æ´ã€‚å®ƒç›´æ¥ä½¿ç”¨æä¾›çš„on_cfg_denoisedå‡½æ•°æ¥æ³¨å†Œå›è°ƒï¼Œæ— éœ€åˆ›å»ºå•ç‹¬çš„å›è°ƒç±»ã€‚
nameå‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†æä¾›ä¸€ä¸ªåç§°å¯ä»¥å¸®åŠ©åœ¨è°ƒè¯•æˆ–ç®¡ç†å¤šä¸ªå›è°ƒæ—¶æ›´å®¹æ˜“è¯†åˆ«æ‚¨çš„å›è°ƒã€‚
è®°ä½ï¼Œæ‚¨ä»ç„¶éœ€è¦ç¡®ä¿æ–‡ä»¶båœ¨é€‚å½“çš„æ—¶å€™è¢«å¯¼å…¥æˆ–æ‰§è¡Œï¼Œä»¥ä¾¿æ³¨å†Œèƒ½å¤Ÿç”Ÿæ•ˆã€‚

    def my_cfg_denoised_callback(params: CFGDenoisedParams):
        # åœ¨è¿™é‡Œè¿›è¡Œæ•°æ®è·å–æ“ä½œ   
        print(f"x_out: {params.x}")



    script_callbacks.on_cfg_denoised(my_cfg_denoised_callback, name="My CFG Denoised Callback")



åŸå‹


    def add_callback(callbacks, fun, *, name=None, category='unknown', filename=None):
        if filename is None:
            stack = [x for x in inspect.stack() if x.filename != __file__]
            filename = stack[0].filename if stack else 'unknown file'

        extension = extensions.find_extension(filename)
        extension_name = extension.canonical_name if extension else 'base'

        callback_name = f"{extension_name}/{os.path.basename(filename)}/{category}"
        if name is not None:
            callback_name += f'/{name}'

        unique_callback_name = callback_name
        for index in range(1000):
            existing = any(x.name == unique_callback_name for x in callbacks)
            if not existing:
                break

            unique_callback_name = f'{callback_name}-{index+1}'

        callbacks.append(ScriptCallback(filename, fun, unique_callback_name))



### ç‰ˆæœ¬ä¸€ å¾ªç¯ä¸­ä¸æ–­æ³¨å†Œå›è°ƒ

    last_noise_uncond = x.new_ones((x.shape[0], 1, 1, 1))
    def my_cfg_denoised_callback(params: CFGDenoisedParams):
        # åœ¨è¿™é‡Œè¿›è¡Œæ•°æ®è·å–æ“ä½œ   
        print(f"x_out: {params.x}")
        last_noise_uncond = params.x
    script_callbacks.on_cfg_denoised(my_cfg_denoised_callback, name="My CFG Denoised Callback")


### ç‰ˆæœ¬äºŒ
åªæ³¨å†Œä¸€æ¬¡

remove_current_script_callbacks():
è¿™ä¸ªå‡½æ•°ç§»é™¤å½“å‰è„šæœ¬æ–‡ä»¶ä¸­æ³¨å†Œçš„æ‰€æœ‰å›è°ƒã€‚å®ƒé€šè¿‡æ£€æŸ¥è°ƒç”¨æ ˆæ¥ç¡®å®šå½“å‰è„šæœ¬çš„æ–‡ä»¶åï¼Œç„¶åç§»é™¤æ‰€æœ‰ä¸è¯¥æ–‡ä»¶åç›¸å…³è”çš„å›è°ƒã€‚
remove_callbacks_for_function(callback_func):
è¿™ä¸ªå‡½æ•°ç§»é™¤ä¸ç‰¹å®šå›è°ƒå‡½æ•°ç›¸å…³è”çš„æ‰€æœ‰å›è°ƒã€‚å®ƒéœ€è¦æ‚¨ä¼ å…¥å…·ä½“çš„å›è°ƒå‡½æ•°ä½œä¸ºå‚æ•°ã€‚

è€ƒè™‘åˆ°æ‚¨çš„æƒ…å†µï¼Œæˆ‘å»ºè®®ä½¿ç”¨ remove_callbacks_for_function(callback_func)ï¼ŒåŸå› å¦‚ä¸‹ï¼š

æ›´ç²¾ç¡®ï¼šå®ƒåªç§»é™¤æ‚¨ç‰¹å®šæ³¨å†Œçš„å›è°ƒå‡½æ•°ï¼Œä¸ä¼šå½±å“åŒä¸€æ–‡ä»¶ä¸­å¯èƒ½å­˜åœ¨çš„å…¶ä»–å›è°ƒã€‚
æ›´å®‰å…¨ï¼šå®ƒä¸ä¾èµ–äºè°ƒç”¨æ ˆæ£€æŸ¥ï¼Œè¿™åœ¨æŸäº›å¤æ‚çš„æ‰§è¡Œç¯å¢ƒä¸­å¯èƒ½æ›´å¯é ã€‚
æ›´çµæ´»ï¼šå¦‚æœæ‚¨çš„ä»£ç è¢«å…¶ä»–è„šæœ¬å¯¼å…¥å’Œä½¿ç”¨ï¼Œremove_current_script_callbacks() å¯èƒ½ä¼šç§»é™¤é”™è¯¯çš„å›è°ƒã€‚

*** Error running process: /home/stable_diffusion_webui_master/extensions/ddimcfgpp-wujie/scripts/cfgpp.py
    Traceback (most recent call last):
      File "/home/stable_diffusion_webui_master/modules/scripts.py", line 784, in process
        script.process(p, *script_args)
    TypeError: Script.process() missing 3 required positional arguments: 'enabled', 'width', and 'height'



*** Error creating UI for ddim_cfgpp: 
    Traceback (most recent call last):
      File "/home/stable_diffusion_webui_master/modules/scripts.py", line 631, in create_script_ui
        self.create_script_ui_inner(script)
      File "/home/stable_diffusion_webui_master/modules/scripts.py", line 648, in create_script_ui_inner
        for control in controls:
    TypeError: 'InputAccordion' object is not iterable


    def ui(self, is_img2img):
        if not is_img2img:
            with InputAccordion(False, label=self.title()) as enabled:
                cutdiff_proportion = gr.Slider(minimum=0., maximum=1., value=0.5, label='cutdiff_proportion')
                with gr.Row():
                    cfg_scale_1 = gr.Slider(minimum=0, maximum=100., value=7., label='cfg_scale_1')
                    
            return enabled


è¿˜æ˜¯ä¸è¡Œ

    def ui(self, is_img2img):
        if not is_img2img:
            with InputAccordion(False, label=self.title()) as enabled:
                cutdiff_proportion = gr.Slider(minimum=0., maximum=1., value=0.5, label='cutdiff_proportion')
                with gr.Row():
                    cfg_scale_1 = gr.Slider(minimum=0, maximum=100., value=7., label='cfg_scale_1')
            return enabled, cutdiff_proportion, cfg_scale_1

å¿…é¡»è¿”å›è‡³å°‘ä¸¤ä¸ªå‚æ•° æ‰èƒ½åˆ›å»º

*** Error running process: /home/stable_diffusion_webui_master/extensions/ddimcfgpp-wujie/scripts/cfgpp.py
    Traceback (most recent call last):
      File "/home/stable_diffusion_webui_master/modules/scripts.py", line 784, in process
        script.process(p, *script_args)
    TypeError: Script.process() takes 3 positional arguments but 5 were given


### é‡‡æ ·å™¨å·®å¼‚
timestepç±»ï¼ˆè¾“å…¥ä¸ºtimestepï¼‰ lcmç±» a1111è‡ªå·±å®ç°     
euler dpmç­‰è¾“å…¥ä¸ºsigmas ä½¿ç”¨kdiffusionå®ç°    

timestepç±»ï¼ˆè¾“å…¥ä¸ºtimestepï¼‰

    samplers_timesteps = [
        ('DDIM', sd_samplers_timesteps_impl.ddim, ['ddim'], {}),
        ('PLMS', sd_samplers_timesteps_impl.plms, ['plms'], {}),
        ('UniPC', sd_samplers_timesteps_impl.unipc, ['unipc'], {}),
    ]


    samplers_data_timesteps = [
        sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: CompVisSampler(funcname, model), aliases, options)
        for label, funcname, aliases, options in samplers_timesteps
    ]

lcm

    samplers_lcm = [('LCM', sample_lcm, ['k_lcm'], {})]
    samplers_data_lcm = [
        sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: LCMSampler(funcname, model), aliases, options)
        for label, funcname, aliases, options in samplers_lcm
    ]


æ‰€æœ‰

    all_samplers = [
        *sd_samplers_kdiffusion.samplers_data_k_diffusion,
        *sd_samplers_timesteps.samplers_data_timesteps,
        *sd_samplers_lcm.samplers_data_lcm,
    ]
    all_samplers_map = {x.name: x for x in all_samplers}

kdiffusion

    
    samplers_k_diffusion = [
        ('DPM++ 2M Karras', 'sample_dpmpp_2m', ['k_dpmpp_2m_ka'], {'scheduler': 'karras'}),
        ('DPM++ SDE Karras', 'sample_dpmpp_sde', ['k_dpmpp_sde_ka'], {'scheduler': 'karras', "second_order": True, "brownian_noise": True}),
        ('DPM++ 2M SDE Exponential', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_exp'], {'scheduler': 'exponential', "brownian_noise": True}),
        ('DPM++ 2M SDE Karras', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_ka'], {'scheduler': 'karras', "brownian_noise": True}),
        ('Euler a', 'sample_euler_ancestral', ['k_euler_a', 'k_euler_ancestral'], {"uses_ensd": True}),
        ('Euler', 'sample_euler', ['k_euler'], {}),
        ('LMS', 'sample_lms', ['k_lms'], {}),
        ('Heun', 'sample_heun', ['k_heun'], {"second_order": True}),
        ('DPM2', 'sample_dpm_2', ['k_dpm_2'], {'discard_next_to_last_sigma': True, "second_order": True}),
        ('DPM2 a', 'sample_dpm_2_ancestral', ['k_dpm_2_a'], {'discard_next_to_last_sigma': True, "uses_ensd": True, "second_order": True}),
        ('DPM++ 2S a', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a'], {"uses_ensd": True, "second_order": True}),
        ('DPM++ 2M', 'sample_dpmpp_2m', ['k_dpmpp_2m'], {}),
        ('DPM++ SDE', 'sample_dpmpp_sde', ['k_dpmpp_sde'], {"second_order": True, "brownian_noise": True}),
        ('DPM++ 2M SDE', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_ka'], {"brownian_noise": True}),
        ('DPM++ 2M SDE Heun', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_heun'], {"brownian_noise": True, "solver_type": "heun"}),
        ('DPM++ 2M SDE Heun Karras', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_heun_ka'], {'scheduler': 'karras', "brownian_noise": True, "solver_type": "heun"}),
        ('DPM++ 2M SDE Heun Exponential', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_heun_exp'], {'scheduler': 'exponential', "brownian_noise": True, "solver_type": "heun"}),
        ('DPM++ 3M SDE', 'sample_dpmpp_3m_sde', ['k_dpmpp_3m_sde'], {'discard_next_to_last_sigma': True, "brownian_noise": True}),
        ('DPM++ 3M SDE Karras', 'sample_dpmpp_3m_sde', ['k_dpmpp_3m_sde_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, "brownian_noise": True}),
        ('DPM++ 3M SDE Exponential', 'sample_dpmpp_3m_sde', ['k_dpmpp_3m_sde_exp'], {'scheduler': 'exponential', 'discard_next_to_last_sigma': True, "brownian_noise": True}),
        ('DPM fast', 'sample_dpm_fast', ['k_dpm_fast'], {"uses_ensd": True}),
        ('DPM adaptive', 'sample_dpm_adaptive', ['k_dpm_ad'], {"uses_ensd": True}),
        ('LMS Karras', 'sample_lms', ['k_lms_ka'], {'scheduler': 'karras'}),
        ('DPM2 Karras', 'sample_dpm_2', ['k_dpm_2_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, "uses_ensd": True, "second_order": True}),
        ('DPM2 a Karras', 'sample_dpm_2_ancestral', ['k_dpm_2_a_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, "uses_ensd": True, "second_order": True}),
        ('DPM++ 2S a Karras', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a_ka'], {'scheduler': 'karras', "uses_ensd": True, "second_order": True}),
        ('Restart', sd_samplers_extra.restart_sampler, ['restart'], {'scheduler': 'karras', "second_order": True}),
    ]
    

    samplers_data_k_diffusion = [
        sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: KDiffusionSampler(funcname, model), aliases, options)
        for label, funcname, aliases, options in samplers_k_diffusion
        if callable(funcname) or hasattr(k_diffusion.sampling, funcname)
    ]

    sampler_extra_params = {
        'sample_euler': ['s_churn', 's_tmin', 's_tmax', 's_noise'],
        'sample_heun': ['s_churn', 's_tmin', 's_tmax', 's_noise'],
        'sample_dpm_2': ['s_churn', 's_tmin', 's_tmax', 's_noise'],
        'sample_dpm_fast': ['s_noise'],
        'sample_dpm_2_ancestral': ['s_noise'],
        'sample_dpmpp_2s_ancestral': ['s_noise'],
        'sample_dpmpp_sde': ['s_noise'],
        'sample_dpmpp_2m_sde': ['s_noise'],
        'sample_dpmpp_3m_sde': ['s_noise'],
    }

    k_diffusion_samplers_map = {x.name: x for x in samplers_data_k_diffusion}
    k_diffusion_scheduler = {
        'Automatic': None,
        'karras': k_diffusion.sampling.get_sigmas_karras,
        'exponential': k_diffusion.sampling.get_sigmas_exponential,
        'polyexponential': k_diffusion.sampling.get_sigmas_polyexponential
    }


### kdiffusion


class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):

    def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):

        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))



è¿›å…¥

class KDiffusionSampler(sd_samplers_common.Sampler):

    sigmas = self.get_sigmas(p, steps)
    
    if 'sigmas' in parameters:
        extra_params_kwargs['sigmas'] = sigmas

    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))


modules/sd_samplers_common.py

class Sampler:

def launch_sampling(self, steps, func):


repositories/k-diffusion/k_diffusion/sampling.py

def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):


### ddim
çœŸç‹—å± é»˜è®¤è¿›å…¥kdiffusion?



class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):

    def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):

    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))







modules/sd_samplers_timesteps.py

class CompVisSampler(sd_samplers_common.Sampler):

    def sample(self, p, x, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):

        if 'timesteps' in parameters:
            extra_params_kwargs['timesteps'] = timesteps


        samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))


modules/sd_samplers_timesteps_impl.py

def ddim(model, x, timesteps, extra_args=None, callback=None, disable=None, eta=0.0):


### æ„Ÿè§‰æ˜¯ä»å®šä¹‰å“ªé‡Œå¼€å§‹åŒ… åˆå§‹å®šä¹‰çš„æ—¶å€™

    class ddim_cfgppSampler(KDiffusionSampler):
        def initialize(self, p):
            extra_params_kwargs = super().initialize(p)
            return extra_params_kwargs

see the monster

æœç„¶æ˜¯é«˜æ‰‹å•Š

å¤ªç¦»è°±äº†































# Be-Your-Outpainter 
è€—æ—¶å››ä¸ªæœˆå¼€æº

https://github.com/G-U-N/Be-Your-Outpainter 


![alt text](assets/624628/image-4.png)


è®­ç»ƒ17386MB
æ¯ä¸ªè§†é¢‘åŸºäºé¢„è®­ç»ƒloraå†è®­ç»ƒ800æ­¥ lora   
æ¨ç†9496mb

800æ­¥ è€—æ—¶ 20åˆ†é’Ÿ

éš¾é“å¸‚é¢ä¸Šéƒ½è¿™ä¸ªå¥—è·¯å—        
å¦‚æœåªæœ‰16å¸§åº”è¯¥å¾ˆå¿«

768    
144å¸§ 11min 50æ­¥    
è¿™ä¹ˆå¤šå¸§å…¶å®ç®—å¿«çš„äº†      
ä½†æ˜¯æ‰6s 24fps   
æ­£å¸¸ç”Ÿå›¾é€Ÿåº¦    


MOTIA åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè¾“å…¥ç‰¹å®šé€‚åº”å’Œæ¨¡å¼æ„ŸçŸ¥ä¿®å¤ã€‚è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µæ¶‰åŠå¯¹å•æ¬¡æºè§†é¢‘è¿›è¡Œé«˜æ•ˆä¸”æœ‰æ•ˆçš„ä¼ªä¿®å¤å­¦ä¹ ã€‚æ­¤è¿‡ç¨‹é¼“åŠ±æ¨¡å‹è¯†åˆ«å’Œå­¦ä¹ æºè§†é¢‘ä¸­çš„æ¨¡å¼ï¼Œå¹¶å¼¥åˆæ ‡å‡†ç”Ÿæˆè¿‡ç¨‹å’Œä¿®å¤ä¹‹é—´çš„å·®è·ã€‚åç»­é˜¶æ®µï¼Œæ¨¡å¼æ„ŸçŸ¥ä¿®å¤ï¼Œè‡´åŠ›äºæ¦‚æ‹¬è¿™äº›å­¦ä¹ åˆ°çš„æ¨¡å¼ä»¥ç”Ÿæˆä¿®å¤ç»“æœã€‚æå‡ºäº†åŒ…æ‹¬ç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ æ’­åœ¨å†…çš„å…¶ä»–ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œä»æºè§†é¢‘ä¸­è·å–çš„è§†é¢‘æ¨¡å¼ã€‚


ç»“è®ºï¼šè€—æ—¶è¾ƒé•¿ï¼Œæ•ˆæœä¸é”™ï¼Œç¨³å®šä¸”ç»“æ„åˆç†ï¼Œå¶å°”è¿˜æ˜¯èƒ½å‘ç°è¾ƒæ˜æ˜¾åˆ†å‰²çº¿ã€‚

è¿™å°±æ˜¯motion_loraä»¥åŠæ—¶é—´å—çš„å¼ºå¤§ä½œç”¨

ç»“è®ºï¼šè€—æ—¶è¾ƒé•¿ï¼Œæ•ˆæœä¸é”™ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´ å¹³æ»‘è¶…å‚æ•°k è·å¾—è¾ƒå¥½çš„è¾“å‡ºï¼Œå¶å°”è¿˜æ˜¯èƒ½å‘ç°è¾ƒæ˜æ˜¾åˆ†å‰²çº¿ã€‚
ç›®å‰åªæ”¯æŒæ¯”è¾ƒçŸ­çš„è§†é¢‘ã€‚

    æ¶æ„ï¼š
    sd1.5 
    control_v11p_sd15_inpaint 
    temporal_blocks/mm_sd_v15_v2.ckpt
    adjust_motion_lora 

diffusersç»“æ„


è„šæœ¬æœªä¿å­˜lora







## ä»£ç 

    min_distance = min(min_vertical, min_horizontal)

    max_range = max(expand_t, expand_b, expand_l, expand_r)

    # Decrease the mask value based on the distance
    mask[i, j] = np.exp(
        -min_distance / (k * max_range)

è¿™æ®µä»£ç ä¼¼ä¹æ˜¯åœ¨è®¡ç®—æŸç§è¡°å‡çš„æ©ç å€¼

min_distance = min(min_vertical, min_horizontal)
è¿™è¡Œä»£ç è®¡ç®—äº†å‚ç›´å’Œæ°´å¹³æ–¹å‘ä¸Šçš„æœ€å°è·ç¦»ã€‚å®ƒé€‰æ‹©äº†min_verticalå’Œmin_horizontalä¸­çš„è¾ƒå°å€¼ã€‚

max_range = max(expand_t, expand_b, expand_l, expand_r)
è¿™è¡Œä»£ç è®¡ç®—äº†æœ€å¤§èŒƒå›´ã€‚å®ƒä»expand_tï¼ˆé¡¶éƒ¨æ‰©å±•ï¼‰ã€expand_bï¼ˆåº•éƒ¨æ‰©å±•ï¼‰ã€expand_lï¼ˆå·¦ä¾§æ‰©å±•ï¼‰å’Œexpand_rï¼ˆå³ä¾§æ‰©å±•ï¼‰ä¸­é€‰æ‹©æœ€å¤§å€¼ã€‚

mask[i, j] = np.exp(-min_distance / (k * max_range))
è¿™è¡Œä»£ç è®¡ç®—æ©ç æ•°ç»„ä¸­ [i, j] ä½ç½®çš„å€¼ã€‚å®ƒä½¿ç”¨äº†æŒ‡æ•°è¡°å‡å‡½æ•°:

np.exp() æ˜¯è‡ªç„¶æŒ‡æ•°å‡½æ•°ï¼ˆeçš„å¹‚ï¼‰ã€‚
-min_distance / (k * max_range) æ˜¯æŒ‡æ•°çš„å‚æ•°ã€‚

k æ˜¯ä¸€ä¸ªå¯èƒ½ç”¨äºæ§åˆ¶è¡°å‡é€Ÿç‡çš„å¸¸æ•°ã€‚
è·ç¦»è¶Šå¤§ï¼ŒæŒ‡æ•°çš„è´Ÿå€¼è¶Šå¤§ï¼Œç»“æœè¶Šæ¥è¿‘0ã€‚
max_range ç”¨äºå½’ä¸€åŒ–è·ç¦»ã€‚

è¿™ä¸ªå…¬å¼çš„æ•ˆæœæ˜¯:

    å½“ min_distance ä¸º0æ—¶ï¼Œæ©ç å€¼ä¸º1ï¼ˆe^0 = 1ï¼‰ã€‚
    éšç€è·ç¦»å¢åŠ ï¼Œæ©ç å€¼å‘ˆæŒ‡æ•°è¡°å‡ï¼Œæ¥è¿‘ä½†æ°¸ä¸è¾¾åˆ°0ã€‚
    k å’Œ max_range å…±åŒæ§åˆ¶äº†è¡°å‡çš„é€Ÿç‡ã€‚

è¿™ç§ç±»å‹çš„æ©ç é€šå¸¸ç”¨äºå›¾åƒå¤„ç†ã€è®¡ç®—æœºè§†è§‰æˆ–æœºå™¨å­¦ä¹ ä¸­ï¼Œå¯èƒ½ç”¨äºåˆ›å»ºå¹³æ»‘çš„è¾¹ç¼˜ã€æƒé‡æˆ–æ³¨æ„åŠ›æœºåˆ¶ã€‚

ä¸»è¦æ§åˆ¶è¡°å‡é€Ÿç‡


æ—¶é—´æ¨¡å—[9]ã€‚ ä¸ºäº†ç»™æ¨¡å‹é…å¤‡é¢å¤–çš„æ—¶é—´å…ˆéªŒï¼Œæˆ‘ä»¬ä½¿ç”¨åœ¨å¤§è§„æ¨¡æ–‡æœ¬è§†é¢‘æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ™®é€šè½¬æ¢å™¨æ¶æ„æ¥åˆå§‹åŒ–é¢å¤–çš„æ—¶é—´æ³¨æ„å±‚ã€‚ è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²ç»è¡¨æ˜ï¼Œå¦‚æœæ²¡æœ‰æˆ‘ä»¬æå‡ºçš„ç‰¹å®šäºè¾“å…¥çš„é€‚åº”è¿‡ç¨‹ï¼Œç›´æ¥åº”ç”¨æ­¤æ—¶é—´å…ˆéªŒè¿›è¡Œè§†é¢‘ç»˜åˆ¶ä¼šå¯¼è‡´è¾ƒå·®çš„ç»“æœã€‚

Note that, we have shown that directly applying this temporal prior for video outpainting leads to poor results without our proposed input-specific adaptation process.

æ´›æ‹‰[13]ã€‚  LoRA æ˜¯ä¸ºäº†å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒè€Œæå‡ºçš„ã€‚ å®ƒå·²å¹¿æ³›åº”ç”¨äºå„ç§åŸºäºæ‰©æ•£çš„åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬è§†é¢‘ç¼–è¾‘å’Œæ“ä½œã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿé€‰æ‹©LoRAä½œä¸ºåŸºç¡€å­¦ä¹ ç»„ä»¶ã€‚ æ­¤å¤–ï¼Œä¸ä¹‹å‰ç›´æ¥æ’å…¥è®­ç»ƒå¥½çš„LoRAçš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œæ ¹æ®ç»™å®šç‰¹å¾çš„ç©ºé—´ä½ç½®è°ƒæ•´LoRAçš„æ’å…¥æƒé‡ï¼Œä»è€Œåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„å­¦ä¹ æ¨¡å¼å’Œç”Ÿæˆå…ˆéªŒä¹‹é—´å®ç°æ›´å¥½çš„å¹³è¡¡ã€‚

æ§åˆ¶ç½‘[34]ã€‚  ControlNet ä½œä¸ºç¨³å®šæ‰©æ•£çš„å³æ’å³ç”¨æ¨¡å—ï¼Œå…è®¸å…¶æ¥å—é¢å¤–çš„è¾“å…¥ä»¥æ›´å¥½åœ°æ§åˆ¶å»å™ªç»“æœã€‚ æˆ‘ä»¬åœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šåº”ç”¨é¢„è®­ç»ƒçš„ ControlNetï¼Œæ¥å—æ©æ¨¡å›¾åƒæ¥æŒ‡å¯¼æ•´ä¸ªå»å™ªè¿‡ç¨‹ã€‚

blip[14]ã€‚ è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŸºäºç¨³å®šæ‰©æ•£ï¼Œè¿™æ˜¯ä¸€ç§æ¡ä»¶é™å™ªå™¨ï¼Œéœ€è¦é€‚å½“çš„æ–‡æœ¬æ¡ä»¶æ‰èƒ½è·å¾—è‰¯å¥½çš„ç»“æœã€‚ æˆ‘ä»¬ä½¿ç”¨ Blip è‡ªåŠ¨æä¾›å­—å¹•ä»¥é¿å…äººä¸ºå½±å“ã€‚

. We apply Blip to automatically provide the captions to avoid manmade influence.


![alt text](assets/624628/image-12.png)


) SDM [7]ï¼šSDM å°†åºåˆ—çš„åˆå§‹å¸§å’Œç»ˆæ­¢å¸§è§†ä¸ºæ¡ä»¶è¾“å…¥ï¼Œä¸åˆå§‹ç½‘ç»œå±‚çš„ä¸Šä¸‹æ–‡åˆå¹¶ã€‚ å®ƒåœ¨è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬ WebVid [3] å’Œç”µå­å•†åŠ¡ [7]ã€‚  3ï¼‰M3DDM [7]ï¼šM3DDM æ˜¯è§†é¢‘å¤–ç»˜çš„åˆ›æ–°ç®¡é“ã€‚ å®ƒé‡‡ç”¨å±è”½æŠ€æœ¯ï¼Œå…è®¸åŸå§‹æºè§†é¢‘ä½œä¸ºå±è”½æ¡ä»¶ã€‚ æ­¤å¤–ï¼Œå®ƒä½¿ç”¨å…¨å±€æ¡†æ¶ç‰¹å¾è¿›è¡Œäº¤å‰æ³¨æ„æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå®ç°å…¨å±€å’Œè¿œç¨‹ä¿¡æ¯ä¼ é€’ã€‚ å®ƒç»è¿‡äº†æµ·é‡è§†é¢‘æ•°æ®ï¼ˆåŒ…æ‹¬ WebVid å’Œç”µå­å•†åŠ¡ï¼‰çš„å¼ºåŒ–è®­ç»ƒï¼Œå¹¶å…·æœ‰ä¸“é—¨ç”¨äºè§†é¢‘ç»˜åˆ¶çš„æ¶æ„è®¾è®¡ã€‚ è¿™æ ·ï¼ŒSDM å¯ä»¥è¢«è§†ä¸º M3DDM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½†å®ƒä¹Ÿç»è¿‡ç±»ä¼¼çš„å¼ºåŒ–è®­ç»ƒã€‚


K is set to 1/2 by default.

Setting K small will reduce the copying phenomena in the outpainting results, especially for the challenging setting with ratio 0.66.


## ç›®å‰æœ€å¤§å®¹é‡
32 frames, 4s (fps=8).

I noticed that currently, the inference does not support long video outpainting. The window size and stride parameters are not used in the inference pipeline.

41å¸§è¿›å»


sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise
RuntimeError: The size of tensor a (41) must match the size of tensor b (16) at non-singleton dimension 2
Steps:   0%|          | 0/800 [00:03<?, ?it/s]


with accelerator.accumulate(unet, loras):

save

    -rw-r--r-- 0 root 2000  40M Jul 10 03:29 optimizer.bin
    -rw-r--r-- 0 root 2000 6.3G Jul 10 03:29 pytorch_model.bin
    -rw-r--r-- 0 root 2000  31M Jul 10 03:29 pytorch_model_1.bin
    -rw-r--r-- 0 root 2000  15K Jul 10 03:29 random_states_0.pkl
    -rw-r--r-- 0 root 2000  557 Jul 10 03:29 scaler.pt
    -rw-r--r-- 0 root 2000  563 Jul 10 03:29 scheduler.bin


blipæ²¡çœ‹åˆ°åœ¨å“ªé‡Œä½¿ç”¨

è¿˜æœ‰i3d









# UltraPixel

https://jingjingrenabc.github.io/ultrapixel/#paper-info?ref=top.aibase.com

UltraPixel: Advancing Ultra-High-Resolution Image Synthesis to New Peaks
Jingjing Ren1 *, Wenbo Li2 *, Haoyu Chen1, Renjing Pei2, Bin Shao2, Yong Guo3, Long Peng2, Fenglong Song2, Lei Zhu1, 4 #

1 The Hong Kong University of Science and Technology (Guangzhou), 2 Huawei Noah's Ark Lab, 3 Max Planck Institute for Informatics, 4 The Hong Kong University of Science and Technology

Paper (Full ver.)

Arxiv (Compressed ver.)

GitHub (Coming soon)













# MultiDiffusion

æ–¹æ³•

æˆ‘ä»¬çš„ä¸»è¦æ€æƒ³æ˜¯åœ¨é¢„å…ˆè®­ç»ƒçš„å‚è€ƒæ‰©æ•£æ¨¡å‹ä¸Šå®šä¹‰ä¸€ä¸ªæ–°çš„ç”Ÿæˆè¿‡ç¨‹ã€‚ä»å™ªå£°å›¾åƒå¼€å§‹ï¼Œåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬è§£å†³ä¸€ä¸ªä¼˜åŒ–ä»»åŠ¡ï¼Œå…¶ç›®æ ‡æ˜¯ä½¿æ¯ä¸ªè£å‰ªå›¾åƒå°½å¯èƒ½æ¥è¿‘å…¶å»å™ªç‰ˆæœ¬ã€‚

![alt text](assets/624628/image-3.png)


è¯·æ³¨æ„ï¼Œè™½ç„¶æ¯ä¸ªå»å™ªæ­¥éª¤å¯èƒ½ä¼šæ‹‰å‘ä¸åŒçš„æ–¹å‘ï¼Œä½†æˆ‘ä»¬çš„æµç¨‹å°†è¿™äº›ä¸ä¸€è‡´çš„æ–¹å‘èåˆåˆ°å…¨å±€å»å™ªæ­¥éª¤ä¸­ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„æ— ç¼å›¾åƒã€‚


æ„Ÿè§‰è¿™ä¸ªç”¨æ¥outpaintä¼šæ¯”ä¼ ç»Ÿå¥½ä¸€äº›












# å…¶ä»–

## github gist

Gistï¼ˆhttps://gist.github.com/ï¼‰ï¼Œä¸­æ–‡è¦ç‚¹ï¼Œ/dÊ’Éªst/ï¼Œæ˜¯Githubæä¾›çš„ä¸€ä¸ªå­æœåŠ¡ï¼Œå¯ä»¥åœ¨çº¿åˆ†äº«æ¯”è¾ƒå°çš„ä»£ç ç‰‡æ®µï¼ŒåŒæ ·çš„ä»£ç å¦‚æœç”¨ä¼ ç»Ÿåˆ›å»ºä»“åº“çš„æ–¹å¼åˆ†äº«å¯èƒ½å°±æ˜¾å¾—ç¬¨é‡äº†ã€‚Gistå°±æ˜¯ä¸€ä¸ªç²¾ç®€ç‰ˆçš„Repositoryã€‚

Gists å¯ä»¥æ˜¯ä»£ç ç‰‡æ®µã€ç¬”è®°ã€é…ç½®æˆ–ä»»ä½•å…¶ä»–å½¢å¼çš„æ–‡æœ¬æ•°æ®ï¼Œä½¿å…¶æˆä¸ºå¼€å‘äººå‘˜åœ¨é¡¹ç›®ä¸­å¿«é€Ÿæ•è·å’Œå…±äº«æƒ³æ³•çš„å®è´µå·¥å…·ã€‚

2.1 ä»£ç ç‰‡æ®µå…±äº«ä¸åä½œ

GitHub Gists æ˜¯ä¸€é¡¹å¼ºå¤§çš„å·¥å…·ï¼Œå¯ç”¨äºå…±äº«å’Œåä½œå¤„ç†ä»£ç ç‰‡æ®µã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¹³å°ï¼Œè®©å¼€å‘äººå‘˜å¯ä»¥è½»æ¾åœ°ä¸ä»–äººåˆ†äº«ä»£ç ç‰‡æ®µï¼Œè€Œæ— éœ€åˆ›å»ºå®Œæ•´çš„å­˜å‚¨åº“ã€‚

ä»£ç ç‰‡æ®µå…±äº«

è¦å…±äº«ä»£ç ç‰‡æ®µï¼Œç”¨æˆ·åªéœ€åˆ›å»ºä¸€ä¸ªæ–°çš„ Gistï¼Œè¾“å…¥ä»£ç å¹¶æ·»åŠ æè¿°ã€‚ç„¶åï¼Œä»–ä»¬å¯ä»¥ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ URLï¼Œè¯¥ URL å¯ç”¨äºä¸ä»–äººå…±äº«ã€‚æ”¶ä»¶äººå¯ä»¥æŸ¥çœ‹ä»£ç ç‰‡æ®µï¼Œå¹¶å¯ä»¥é€‰æ‹©å°†å…¶å…‹éš†åˆ°è‡ªå·±çš„æœ¬åœ°è®¡ç®—æœºæˆ–åˆ›å»ºå‰¯æœ¬ã€‚



Gistæ˜¯ä¸€æ¬¾ç®€å•çš„Webåº”ç”¨ç¨‹åºï¼Œå¸¸è¢«å¼€å‘è€…ä»¬ç”¨æ¥å…±äº«ç¤ºä¾‹ä»£ç å’Œé”™è¯¯ä¿¡æ¯ã€‚
Gistæœ€å¤§çš„ç‰¹ç‚¹æ˜¯å¯ä»¥ä¸å…¶ä»–äººè½»æ¾åˆ†äº«ç¤ºä¾‹ä»£ç ã€‚


# kolors

## éƒ¨ç½²
5æœº myconda



python -u gradio_demo.py | tee -a log.txt

python -u gradio_demo.py 2>&1 | tee -a log.txt


set_seed(seed)
  File "/root/miniconda3/envs/myconda/lib/python3.10/site-packages/accelerate/utils/random.py", line 44, in set_seed
    np.random.seed(seed)
  File "numpy/random/mtrand.pyx", line 4805, in numpy.random.mtrand.seed
  File "numpy/random/mtrand.pyx", line 250, in numpy.random.mtrand.RandomState.seed
  File "_mt19937.pyx", line 168, in numpy.random._mt19937.MT19937._legacy_seeding
  File "_mt19937.pyx", line 182, in numpy.random._mt19937.MT19937._legacy_seeding
ValueError: Seed must be between 0 and 2**32 - 1

diffusersé—®é¢˜ï¼Ÿï¼Ÿ


è€Œä¸”ä¸ºä»€ä¹ˆgradioè¿”å›éƒ½ç”¨npå½¢å¼ï¼Ÿ       
ç„¶åå¤šbatchæœ‰æ•ˆå—ï¼Ÿï¼Ÿï¼Ÿ        

pilå¥½åƒä¸è¡Œ


cutçš„npåœ¨è¿™é‡Œç”¨ä¸äº†ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ


ç”¨sd3

 gr.on(
AttributeError: module 'gradio' has no attribute 'on'
IMPORTANT: You are using gradio version 3.41.2, however version 4.29.0 is available, please upgrade.


è¾“å‡ºnp

æ²¿ç”¨click ä½†æ˜¯ä¸ç”¨gallary ç”¨gr image è¿”å›æ­£å¸¸


results = process(prompt, image_width, image_height, num_samples=1, seed, steps, n_prompt, float(cfg), scheduler_type)
                                                                                                                         ^
SyntaxError: positional argument follows keyword argument

## æŠ€æœ¯æŠ¥å‘Š

ä¸Imagenå’ŒStable Diffusion 3ä¸­ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹T5ä¸åŒï¼ŒKolorsæ˜¯å»ºç«‹åœ¨é€šç”¨è¯­è¨€æ¨¡å‹ï¼ˆGLMï¼‰ä¹‹ä¸Šçš„ï¼Œè¿™å¢å¼ºäº†å…¶è‹±è¯­å’Œæ±‰è¯­çš„ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¥é‡ç°å¹¿æ³›çš„è®­ç»ƒæ•°æ®é›†ï¼Œä»¥å®ç°ç»†ç²’åº¦çš„æ–‡æœ¬ç†è§£ã€‚è¿™äº›ç­–ç•¥æ˜¾ç€æé«˜äº† Kolors ç†è§£å¤æ‚è¯­ä¹‰ï¼ˆå°¤å…¶æ˜¯æ¶‰åŠå¤šä¸ªå®ä½“çš„è¯­ä¹‰ï¼‰çš„èƒ½åŠ›ï¼Œå¹¶å¯ç”¨äº†å…¶é«˜çº§æ–‡æœ¬æ¸²æŸ“åŠŸèƒ½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†Kolorsçš„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¹¿æ³›çŸ¥è¯†çš„æ¦‚å¿µå­¦ä¹ é˜¶æ®µå’Œä¸“é—¨ç­–åˆ’çš„é«˜å®¡ç¾æ•°æ®çš„è´¨é‡æå‡é˜¶æ®µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å™ªå£°è°ƒåº¦çš„å…³é”®ä½œç”¨ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è°ƒåº¦æ¥ä¼˜åŒ–é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚è¿™äº›ç­–ç•¥å…±åŒå¢å¼ºäº†ç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒçš„è§†è§‰å¸å¼•åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç±»åˆ«å¹³è¡¡çš„åŸºå‡† KolorsPromptsï¼Œä½œä¸º Kolors åŸ¹è®­å’Œè¯„ä¼°çš„æŒ‡å—ã€‚å› æ­¤ï¼Œå³ä½¿é‡‡ç”¨å¸¸ç”¨çš„ U-Net ä¸»å¹²ç½‘ï¼ŒKolors åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œè¾¾åˆ°äº† Midjourney-v6 çº§åˆ«çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰å¸å¼•åŠ›æ–¹é¢ã€‚



ä¸ºäº†æé«˜å¯¹ä¸­æ–‡æç¤ºçš„ç†è§£ï¼Œå¼•å…¥äº†å‡ ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬ AltDiffusion [45]ã€PAI-Diffusion [39]ã€Taiyi-XL [42] å’Œ Hunyuan-DiT [19]ã€‚è¿™äº›æ–¹æ³•ä»ç„¶ä¾èµ– CLIP è¿›è¡Œä¸­æ–‡æ–‡æœ¬ç¼–ç ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™äº›æ¨¡å‹çš„ä¸­æ–‡æ–‡æœ¬ä¾ä»æ€§å’Œå›¾åƒç¾å­¦è´¨é‡ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚

Kolors é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•è¿›è¡ŒåŸ¹è®­ï¼ŒåŒ…æ‹¬æ¦‚å¿µå­¦ä¹ é˜¶æ®µï¼ˆåˆ©ç”¨å¹¿æ³›çš„çŸ¥è¯†ï¼‰å’Œè´¨é‡æ”¹è¿›é˜¶æ®µï¼ˆåˆ©ç”¨ç²¾å¿ƒç­–åˆ’çš„é«˜ç¾å­¦æ•°æ®ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ—¶é—´è¡¨æ¥ä¼˜åŒ–é«˜åˆ†è¾¨ç‡


ç»å…¸æ–¹æ³•ï¼Œä¾‹å¦‚ SD1.5 [32] å’Œ DALL-E 2 [30]ï¼Œé‡‡ç”¨ CLIP æ¨¡å‹çš„æ–‡æœ¬åˆ†æ”¯è¿›è¡Œæ–‡æœ¬è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç”±äº CLIP æ¥å—äº†å¯¹æ¯”æŸå¤±è®­ç»ƒï¼Œä»¥å°†æ•´ä¸ªå›¾åƒä¸æ–‡æœ¬æè¿°å¯¹é½ï¼Œå› æ­¤å®ƒå¾ˆéš¾ç†è§£æ¶‰åŠå¤šä¸ªä¸»é¢˜ã€ä½ç½®æˆ–é¢œè‰²çš„è¯¦ç»†å›¾åƒæè¿°ã€‚

. However, since CLIP is trained with a contrastive loss to align entire images with text descriptions, it struggles to understand detailed image descriptions involving multiple subjects, positions, or colors.

ä¸€äº›æ–¹æ³•ä»ç¼–ç å™¨-è§£ç å™¨ Transformer T5 ä¸­æå–æ–‡æœ¬åµŒå…¥ï¼Œå®ƒæºå¸¦æ›´ç»†ç²’åº¦çš„æœ¬åœ°ä¿¡æ¯ï¼Œä¾‹å¦‚ Imagen [34] å’Œ PixArt-Î± [5]ã€‚æ­¤å¤–ï¼Œå…¶ä»–æ–¹æ³•åˆ©ç”¨å¤šä¸ªæ–‡æœ¬ç¼–ç å™¨æ¥å¢å¼ºæ–‡æœ¬ç†è§£ã€‚ä¾‹å¦‚ï¼ŒeDiff-I [2] æå‡ºäº†ä¸€ç§ç»“åˆ CLIP å’Œ T5 çš„é›†æˆæ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå…¨å±€å’Œå±€éƒ¨æ–‡æœ¬è¡¨ç¤ºã€‚ SDXL [27] é‡‡ç”¨ä¸¤ä¸ª CLIP ç¼–ç å™¨ï¼Œå¹¶åœ¨å¼€æºç¤¾åŒºä¸­å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœã€‚ Stable Diffusion 3 [9] è¿›ä¸€æ­¥å°† T5-XXL æ–‡æœ¬ç¼–ç å™¨çº³å…¥å…¶æ¨¡å‹æ¶æ„ä¸­ï¼Œè¿™å¯¹äºå¤„ç†å¤æ‚çš„æç¤ºè‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼ŒLuminaT2X æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„ LLM æ¨¡å‹ LLama2 [38] å°†æ–‡æœ¬è½¬æ¢ä¸ºä»»ä½•æ¨¡æ€ã€‚

Recently, LuminaT2X proposes a unified framework to transform text into any modality by leveraging the pre-trained LLM model LLama2 [


 HunyuanDiT [19] é€šè¿‡é‡‡ç”¨åŒè¯­ CLIP å’Œå¤šè¯­è¨€ T5 [43] ç¼–ç å™¨æ¥ç”Ÿæˆä¸­æ–‡æ–‡æœ¬åˆ°å›¾åƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œä¸­æ–‡æ–‡æœ¬çš„è®­ç»ƒè¯­æ–™ä»…å å¤šè¯­è¨€T5æ•°æ®é›†çš„ä¸åˆ°2%ï¼Œå¹¶ä¸”åŒè¯­CLIPäº§ç”Ÿçš„æ–‡æœ¬åµŒå…¥ä»ç„¶ä¸è¶³ä»¥å¤„ç†å¤æ‚çš„æ–‡æœ¬æç¤ºã€‚


ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬é€‰æ‹©é€šç”¨è¯­è¨€æ¨¡å‹ï¼ˆGLMï¼‰[8] ä½œä¸º Kolors ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ã€‚ GLM æ˜¯ä¸€ç§åŸºäºè‡ªå›å½’å¡«ç©ºç›®æ ‡çš„åŒè¯­ï¼ˆè‹±è¯­å’Œä¸­æ–‡ï¼‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­æ˜¾ç€ä¼˜äº BERT å’Œ T5ã€‚æˆ‘ä»¬è®¤ä¸ºé¢„è®­ç»ƒçš„ ChatGLM3-6B-Base æ¨¡å‹æ›´é€‚åˆæ–‡æœ¬è¡¨ç¤ºï¼Œè€Œç»è¿‡äººç±»åå¥½å¯¹é½è®­ç»ƒçš„èŠå¤©æ¨¡å‹ ChatGLM3-6B åœ¨æ–‡æœ¬æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å› æ­¤ï¼Œåœ¨Kolorsä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼€æºçš„ChatGLM3-6B-Baseä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œå®ƒå·²ç»è¿‡è¶…è¿‡1.4ä¸‡äº¿ä¸ªåŒè¯­æ ‡è®°çš„é¢„è®­ç»ƒï¼Œä»è€Œäº§ç”Ÿäº†å¼ºå¤§çš„ä¸­æ–‡ç†è§£èƒ½åŠ›ã€‚



ä¸ CLIP çš„ 77 ä¸ª token ç›¸æ¯”ï¼Œæˆ‘ä»¬ç›´æ¥å°† ChatGLM3 çš„æ–‡æœ¬é•¿åº¦è®¾ç½®ä¸º 256ï¼Œä»¥å®ç°è¯¦ç»†çš„å¤æ‚æ–‡æœ¬ç†è§£ã€‚è¯·æ³¨æ„ï¼Œç”±äº ChatGLM3 çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œå¾ˆå®¹æ˜“æ”¯æŒé•¿æ–‡æœ¬ã€‚ï¼ŒChatGLM3 çš„å€’æ•°ç¬¬äºŒä¸ªè¾“å‡ºç”¨äºæ–‡æœ¬è¡¨ç¤ºã€‚

è®­ç»ƒæ–‡æœ¬-å›¾åƒå¯¹é€šå¸¸æ¥è‡ªäº’è”ç½‘ï¼Œå¹¶ä¸”éšé™„çš„å›¾åƒæ ‡é¢˜ä¸å¯é¿å…åœ°å­˜åœ¨å™ªå£°ä¸”ä¸å‡†ç¡®ã€‚  DALL-E 3 [36] é€šè¿‡é‡æ–°å­—å¹•è§£å†³äº†è¿™ä¸ªé—®é¢˜ ä½¿ç”¨ä¸“é—¨çš„å›¾åƒå­—å¹•å™¨ã€‚ ä¸ºäº†å¢å¼º Kolors çš„æç¤ºè·Ÿéšèƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸ DALL-E 3 ç±»ä¼¼çš„æ–¹æ³•ï¼Œç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) é‡æ–°æè¿°æ–‡æœ¬-å›¾åƒå¯¹ã€‚

![alt text](assets/624628/image-5.png)



è¿™ä¸ªå¾ˆæ­£å¸¸å§ å¾®è°ƒéƒ½è¿™æ ·åš

æˆ‘ä»¬å»ºè®®æ ¹æ®ä»¥ä¸‹äº”ä¸ªæ ‡å‡†æ¥è¯„ä¼°æ–‡æœ¬æè¿°çš„è´¨é‡ï¼š
 
é•¿åº¦ï¼šæ±‰å­—æ€»æ•°ã€‚  â€¢       
 å®Œæ•´æ€§ï¼šæ–‡æœ¬æè¿°åŒ…å«æ•´ä¸ªå›¾åƒçš„ç¨‹åº¦ã€‚ å¦‚æœæ–‡æœ¬æè¿°äº†å›¾åƒä¸­çš„æ‰€æœ‰å¯¹è±¡ï¼Œåˆ™è¯„åˆ†ä¸º 5ï¼›å¦‚æœæ–‡æœ¬æè¿°äº†å°‘äº 30% çš„å¯¹è±¡ï¼Œåˆ™è¯„åˆ†ä¸º 1ã€‚  â€¢      
è¿™ä¸ªå¦‚ä½•åšï¼Ÿå¤šæ¨¡å‹äº’è¯„ï¼Ÿ


 ç›¸å…³æ€§ï¼šæ–‡æœ¬æè¿°åœ¨è¡¨ç¤ºå›¾åƒå‰æ™¯å…ƒç´ æ–¹é¢çš„å‡†ç¡®æ€§ã€‚ å¦‚æœæ–‡æœ¬æè¿°äº†æ‰€æœ‰å‰æ™¯å¯¹è±¡ï¼Œåˆ™è¯„åˆ†ä¸º 5ï¼›å¦‚æœæ–‡æœ¬è¦†ç›–çš„å‰æ™¯å¯¹è±¡å°‘äº 30%ï¼Œåˆ™è¯„åˆ†ä¸º 1ã€‚  â€¢       
 äººå·¥ï¼Ÿ    

 å¹»è§‰ï¼šæ–‡æœ¬ä¸­æåˆ°çš„ç»†èŠ‚æˆ–å®ä½“åœ¨å›¾åƒä¸­ä¸å­˜åœ¨çš„æ¯”ä¾‹ã€‚  5 åˆ†è¡¨ç¤ºæ–‡æœ¬ä¸­æ²¡æœ‰å¹»è§‰ï¼Œå¦‚æœè¶…è¿‡ 50% çš„æ–‡æœ¬æ˜¯å¹»è§‰ï¼Œåˆ™åˆ†é… 1 åˆ†ã€‚  â€¢        
 ä¸»è§‚æ€§ï¼šæ–‡æœ¬åç¦»æè¿°å›¾åƒè§†è§‰å†…å®¹è€Œæ˜¯ä¼ è¾¾ä¸»è§‚å°è±¡çš„ç¨‹åº¦ã€‚ ä¾‹å¦‚ï¼Œâ€œç»™äººä¸€ç§è½»æ¾ã€å®é™çš„æ„Ÿè§‰ï¼Œè®©äººæ„Ÿè§‰èˆ’æœâ€è¿™æ ·çš„å¥å­è¢«è®¤ä¸ºæ˜¯ä¸»è§‚çš„ã€‚ å¦‚æœæ²¡æœ‰ä¸»è§‚æ–‡æœ¬ï¼Œåˆ™å¾—åˆ†ä¸º 5ï¼›å¦‚æœè¶…è¿‡ 50% çš„æ–‡æœ¬ç”±ä¸»è§‚å¥å­ç»„æˆï¼Œåˆ™å¾—åˆ†ä¸º 1ã€‚  â€¢   
 è¿™å°±æ˜¯ä¸€äº›æ‰“æ ‡æ¨¡å‹    

 å¹³å‡å€¼ï¼šæ ¹æ®å®Œæ•´æ€§ã€ç›¸å…³æ€§ã€å¹»è§‰å’Œä¸»è§‚æ€§å¾—å‡ºçš„å¹³å‡åˆ†æ•°ã€‚



éƒ½æ˜¯çœ¼ç›çœ‹çš„å§

æˆ‘ä»¬é€‰æ‹©äº† 5 ä¸ªè‘—åçš„ MLLM æ¨¡å‹ï¼Œå¹¶è˜è¯·äº† 10 åè¯„ä¼°å‘˜æ¥è¯„ä¼° 500 å¼ å›¾åƒã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLLaVA1.5 [22]ã€CogAgent [14] å’Œ CogVLM [40] æ”¯æŒä¸­æ–‡æ–‡æœ¬ã€‚ ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç”Ÿæˆçš„ä¸­æ–‡å­—å¹•ä¸å¦‚è‹±æ–‡å­—å¹•ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆç”Ÿæˆè‹±æ–‡å›¾åƒæ ‡é¢˜ï¼Œç„¶åå°†å…¶ç¿»è¯‘æˆä¸­æ–‡ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ InternLM-XComposer-7B [49] å’Œ GPT-4V çš„ä¸­æ–‡æç¤ºã€‚

è¿™ä¼šæœ‰ä¸€äº›é—®é¢˜

è¡¨ 2 æ€»ç»“äº†äº”ä¸ªæ¨¡å‹çš„å­—å¹•æ€§èƒ½ã€‚å¾ˆæ˜æ˜¾ï¼ŒGPT4V å®ç°äº†æœ€é«˜çš„æ€§èƒ½ã€‚ ç„¶è€Œï¼Œä½¿ç”¨ GPT-4V å¤„ç†æ•°äº¿å¼ å›¾åƒçš„æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚ åœ¨å‰©ä¸‹çš„å››ä¸ªå¼€æºæ¨¡å‹ä¸­ï¼Œä¸ CogAgent-VQA å’Œ CogVLM-1.1-chat ç›¸æ¯”ï¼ŒLLaVA1.5 å’Œ InternLM-XComposer åœ¨å®Œæ•´æ€§å’Œç›¸å…³æ€§æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾è¾ƒå·®çš„æ€§èƒ½ï¼Œè¡¨æ˜é«˜åº¦è¯¦ç»†çš„æè¿°è´¨é‡è¾ƒä½ã€‚ æ­¤å¤–ï¼ŒCogVLM-1.1chat ç”Ÿæˆçš„å­—å¹•æ˜¾ç¤ºå‡ºè¾ƒå°‘çš„å¹»è§‰å’Œä¸»è§‚æ€§å®ä¾‹ã€‚ åŸºäºè¿™äº›è¯„ä¼°ï¼Œæˆ‘ä»¬é€‰æ‹©äº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ CogVLM-1.1-chat æ¥ç”Ÿæˆç»¼åˆè¯¦ç»†çš„

![alt text](assets/624628/image-6.png)

![alt text](assets/624628/image-7.png)

æˆ‘ä»¬å¹¿æ³›çš„è®­ç»ƒæ•°æ®é›†çš„æ ‡é¢˜ã€‚ è€ƒè™‘åˆ° MLLM å¯èƒ½æ— æ³•è¯†åˆ«å›¾åƒä¸­ä¸å­˜åœ¨äºå…¶çŸ¥è¯†åº“ä¸­çš„ç‰¹å®šæ¦‚å¿µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä½¿ç”¨ 50% åŸå§‹æ–‡æœ¬ä¸ 50% åˆæˆå­—å¹•æ¯”ä¾‹çš„ç­–ç•¥ã€‚ è¿™ä¸ç¨³å®šæ‰©æ•£ 3 çš„é…ç½®ä¸€è‡´ã€‚

Considering that MLLMs may fail to identify specific concepts in images that are not present in their knowledge corpus, we employ a strategy of using a 50% original text to 50% synthetic captions ratio.


å…·ä½“æ€ä¹ˆåš æ˜¯çŸ­è¾¹ä¸ºæ ‡å‡†è£å‰ªé•¿è¾¹ï¼Ÿ   
è¿˜æ˜¯ä¸¤ä¸ªdropä¸€åŠï¼Ÿï¼Ÿï¼Ÿ   
è¿˜æ˜¯1+1ã€‹ï¼Ÿï¼Ÿï¼Ÿ

é€šè¿‡åˆ©ç”¨ç»†ç²’åº¦çš„åˆæˆå­—å¹•ï¼ŒKolors å±•ç¤ºäº†ç†è§£å¤æ‚ä¸­æ–‡æ–‡æœ¬çš„å¼ºå¤§èƒ½åŠ›ã€‚ å¦‚å›¾ 2 æ‰€ç¤ºï¼Œæˆ‘ä»¬å±•ç¤ºäº† Kolors åœ¨å¤æ‚æç¤ºä¸Šä½¿ç”¨ä¸åŒæ–‡æœ¬ç¼–ç å™¨çš„ç»“æœã€‚ æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒKolors åœ¨ä½¿ç”¨ GLM æ—¶ï¼Œåœ¨å¤šä¸ªä¸»é¢˜å’Œè¯¦ç»†å±æ€§ä¸Šè¡¨ç°è‰¯å¥½ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¸¦æœ‰ CLIP çš„ Kolors æ— æ³•åœ¨é¡¶éƒ¨æç¤ºä¸­ç”Ÿæˆä¾›åº”å•†å’Œç”µè¯ï¼Œå¹¶ä¸”åº•éƒ¨æç¤ºä¸­çš„é¢œè‰²ä¹Ÿå­˜åœ¨æ··ä¹±ã€‚


### 2.1.3 ä¸­æ–‡æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›å¢å¼ºã€‚
 æ–‡æœ¬æ¸²æŸ“é•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚  DALL-E 3 [3]å’ŒStable Diffusion 3 [9]ç­‰å…ˆè¿›æ–¹æ³•åœ¨æ¸²æŸ“è‹±æ–‡æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºäº†å‡ºè‰²çš„èƒ½åŠ›ã€‚ ç„¶è€Œï¼Œå½“å‰çš„æ¨¡å‹åœ¨å‡†ç¡®æ¸²æŸ“ä¸­æ–‡æ–‡æœ¬æ–¹é¢é‡åˆ°äº†é‡å¤§æŒ‘æˆ˜ã€‚ é€ æˆè¿™äº›å›°éš¾çš„æ ¹æœ¬åŸå› å¦‚ä¸‹ï¼š

ä¸è‹±æ–‡ç›¸æ¯”ï¼Œå¤§é‡çš„æ±‰å­—å’Œè¿™äº›å­—ç¬¦çš„å¤æ‚çº¹ç†æœ¬è´¨ä¸Šä½¿å¾—ä¸­æ–‡æ–‡æœ¬çš„æ¸²æŸ“æ›´å…·æŒ‘æˆ˜æ€§ã€‚ 

 2.ç¼ºä¹è¶³å¤Ÿçš„åŒ…æ‹¬ä¸­æ–‡æ–‡æœ¬å’Œç›¸å…³å›¾åƒçš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒå’Œæ‹Ÿåˆèƒ½åŠ›ä¸è¶³ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦æ¥è§£å†³ã€‚ é¦–å…ˆï¼Œå¯¹äºä¸­æ–‡è¯­æ–™åº“ï¼Œæˆ‘ä»¬é€‰å–â€‹â€‹æœ€å¸¸ç”¨çš„5ä¸‡ä¸ªå•è¯ï¼Œæ„å»ºåƒä¸‡çº§çš„è®­ç»ƒæ•°æ®é›†

é€šè¿‡æ•°æ®åˆæˆè¿›è¡Œå›¾åƒ-æ–‡æœ¬å¯¹ã€‚ ä¸ºäº†ç¡®ä¿æœ‰æ•ˆçš„å­¦ä¹ ï¼Œè¿™äº›ç»¼åˆæ•°æ®ä»…åœ¨æ¦‚å¿µå­¦ä¹ é˜¶æ®µçº³å…¥ã€‚ å…¶æ¬¡ï¼Œä¸ºäº†å¢å¼ºç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿï¼Œæˆ‘ä»¬åˆ©ç”¨ OCR å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸ºç°å®ä¸–ç•Œå›¾åƒï¼ˆä¾‹å¦‚æµ·æŠ¥å’Œåœºæ™¯æ–‡æœ¬ï¼‰ç”Ÿæˆæ–°çš„æè¿°ï¼Œä»è€Œäº§ç”Ÿå¤§çº¦æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬ã€‚

Secondly, to enhance the realism of the generated images, we utilize OCR and multimodal language models to generate new descriptions for real-world images, such as posters and scene texts, resulting in approximately millions samples.

æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè™½ç„¶è®­ç»ƒæ•°æ®é›†ä¸­çš„åˆæˆæ•°æ®æœ€åˆç¼ºä¹çœŸå®æ„Ÿï¼Œä½†å°†çœŸå®æ•°æ®å’Œé«˜è´¨é‡æ–‡æœ¬å›¾åƒçº³å…¥è®­ç»ƒè¿‡ç¨‹åï¼Œç”Ÿæˆçš„æ–‡æœ¬å›¾åƒçš„çœŸå®æ„Ÿæ˜¾ç€æé«˜ã€‚ å³ä½¿æŸäº›å­—ç¬¦ä»…å‡ºç°åœ¨åˆæˆæ•°æ®ä¸­ï¼Œè¿™ç§å¢å¼ºä¹Ÿæ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚ å›¾ 3 æä¾›äº†å…¶ä»–å¯è§†åŒ–æ•ˆæœã€‚


é€šè¿‡æ•´åˆåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®æ¥ç³»ç»Ÿåœ°è§£å†³è®­ç»ƒæ•°æ®çš„å±€é™æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç€æé«˜äº†ä¸­æ–‡æ–‡æœ¬æ¸²æŸ“çš„è´¨é‡ï¼Œä»è€Œä¸ºä¸­æ–‡æ–‡æœ¬å›¾åƒç”Ÿæˆçš„æ–°è¿›å±•é“ºå¹³äº†é“è·¯ã€‚


æé«˜è§†è§‰å¸å¼•åŠ›    
è™½ç„¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å·²è¢«æ™®é€šç”¨æˆ·å’Œä¸“ä¸šè®¾è®¡å¸ˆå¹¿æ³›é‡‡ç”¨ï¼Œä½†å…¶å›¾åƒç”Ÿæˆè´¨é‡é€šå¸¸éœ€è¦é¢å¤–çš„åå¤„ç†æ­¥éª¤ï¼Œä¾‹å¦‚å›¾åƒæ”¾å¤§å’Œé¢éƒ¨æ¢å¤ã€‚ æé«˜ LDM çš„è§†è§‰è´¨é‡ä»ç„¶æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚ åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ”¹è¿›æ•°æ®å’ŒåŸ¹è®­æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

é«˜è´¨é‡æ•°æ®       

Kolors çš„åŸ¹è®­åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„é˜¶æ®µï¼šæ¦‚å¿µå­¦ä¹ é˜¶æ®µå’Œè´¨é‡æ”¹è¿›é˜¶æ®µã€‚ åœ¨æ¦‚å¿µå­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹ä¸»è¦ä»åŒ…å«æ•°åäº¿å›¾æ–‡å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸­è·å–å…¨é¢çš„çŸ¥è¯†å’Œæ¦‚å¿µã€‚ æ­¤é˜¶æ®µçš„æ•°æ®æ¥è‡ªå…¬å…±æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼ŒLAION [35]ã€DataComp [11]ã€JourneyDB [37]ï¼‰ä»¥åŠä¸“æœ‰æ•°æ®é›†ã€‚ é€šè¿‡é‡‡ç”¨ç±»åˆ«å¹³è¡¡ç­–ç•¥ï¼Œè¯¥æ•°æ®é›†ç¡®ä¿å¹¿æ³›è¦†ç›–å„ç§è§†è§‰æ¦‚å¿µã€‚ 

By employing a category-balanced strategy, this dataset ensures extensive coverage of a wide range of visual concepts. In the


åœ¨è´¨é‡æ”¹è¿›é˜¶æ®µï¼Œé‡ç‚¹è½¬å‘å¢å¼ºé«˜åˆ†è¾¨ç‡ä¸‹çš„å›¾åƒç»†èŠ‚å’Œç¾æ„Ÿã€‚ ä¹‹å‰çš„å·¥ä½œ [6, 18] ä¹Ÿå¼ºè°ƒäº†è¿™ä¸€é˜¶æ®µæ•°æ®è´¨é‡çš„è‡³å…³é‡è¦æ€§ã€‚


ä¸ºäº†è·å–é«˜è´¨é‡çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œæˆ‘ä»¬é¦–å…ˆå°†ä¼ ç»Ÿçš„è¿‡æ»¤å™¨ï¼ˆä¾‹å¦‚åˆ†è¾¨ç‡ã€OCR å‡†ç¡®æ€§ã€äººè„¸è®¡æ•°ã€æ¸…æ™°åº¦å’Œç¾è§‚è¯„åˆ†ï¼‰åº”ç”¨äºæˆ‘ä»¬çš„æ•°æ®é›†ï¼Œä»è€Œå°†å…¶å‡å°‘åˆ°å¤§çº¦æ•°åƒä¸‡å¼ å›¾åƒã€‚ éšåå¯¹è¿™äº›å›¾åƒè¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šï¼Œæ³¨é‡Šåˆ†ä¸ºäº”ä¸ªä¸åŒçš„çº§åˆ«ã€‚ ä¸ºäº†å‡è½»ä¸»è§‚åè§ï¼Œæ¯ä¸ªå›¾åƒéƒ½ä¼šè¢«æ³¨é‡Šä¸‰æ¬¡ï¼Œå¹¶é€šè¿‡æŠ•ç¥¨è¿‡ç¨‹ç¡®å®šæœ€ç»ˆçº§åˆ«ã€‚ ä¸åŒçº§åˆ«å›¾åƒçš„ç‰¹å¾å¦‚ä¸‹ï¼š

. These images are subsequently subjected to manual annotation, with the annotations categorized into five distinct levels.


â€¢ 1 çº§ï¼šè¢«è§†ä¸ºä¸å®‰å…¨çš„å†…å®¹åŒ…æ‹¬æç»˜è‰²æƒ…ã€æš´åŠ›ã€è¡€è…¥æˆ–ææ€–çš„å›¾åƒã€‚   
â€¢ çº§åˆ« 2ï¼šå­˜åœ¨äººå·¥åˆæˆè¿¹è±¡çš„å›¾åƒï¼Œä¾‹å¦‚å­˜åœ¨å¾½æ ‡ã€æ°´å°ã€é»‘ç™½è¾¹æ¡†ã€æ‹¼æ¥å›¾åƒç­‰ã€‚   

â€¢ çº§åˆ« 3ï¼šå­˜åœ¨å‚æ•°é”™è¯¯çš„å›¾åƒï¼Œä¾‹å¦‚æ¨¡ç³Šã€æ›å…‰è¿‡åº¦ã€æ›å…‰ä¸è¶³æˆ–ç¼ºä¹ç»†èŠ‚ã€‚ ä¸€ä¸ªæ˜ç¡®çš„ä¸»é¢˜ã€‚    
â€¢ 4 çº§ï¼šä¸èµ·çœ¼çš„ç…§ç‰‡ï¼Œç±»ä¼¼äºæœªç»å¤ªå¤šè€ƒè™‘è€Œæ‹æ‘„çš„å¿«ç…§ã€‚     
â€¢ 5çº§ï¼šå…·æœ‰è¾ƒé«˜å®¡ç¾ä»·å€¼çš„ç…§ç‰‡ï¼Œè¿™æ„å‘³ç€å›¾åƒä¸ä»…åº”å…·æœ‰é€‚å½“çš„æ›å…‰ã€å¯¹æ¯”åº¦ã€è‰²è°ƒå¹³è¡¡å’Œè‰²å½©é¥±å’Œåº¦ï¼Œè€Œä¸”è¿˜åº”å…·æœ‰å™äº‹æ„Ÿã€‚


è¯¥æ–¹æ³•æœ€ç»ˆäº§ç”Ÿæ•°ç™¾ä¸‡å¼  5 çº§é«˜ç¾è§‚å›¾åƒï¼Œç”¨äºè´¨é‡å¢å¼ºé˜¶æ®µã€‚


é«˜åˆ†è¾¨ç‡è®­ç»ƒ    


ç”±äºåœ¨å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­å›¾åƒç ´åä¸å……åˆ†ï¼Œæ‰©æ•£æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡ä¸‹é€šå¸¸è¡¨ç°ä¸ä½³ã€‚ å¦‚å›¾ 4 æ‰€ç¤ºï¼Œå½“æŒ‰ç…§ SDXL [27] ä¸­æä¾›çš„æ—¶é—´è¡¨æ·»åŠ å™ªå£°æ—¶ï¼Œä½åˆ†è¾¨ç‡å›¾åƒä¼šé€€åŒ–ä¸ºå‡ ä¹çº¯å™ªå£°ï¼Œè€Œé«˜åˆ†è¾¨ç‡å›¾åƒå¾€å¾€ä¼šåœ¨æœ€ç»ˆé˜¶æ®µä¿ç•™ä½é¢‘åˆ†é‡ã€‚          
ç”±äºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¿…é¡»ä»çº¯é«˜æ–¯å™ªå£°å¼€å§‹ï¼Œå› æ­¤è¿™ç§å·®å¼‚å¯èƒ½ä¼šå¯¼è‡´é«˜åˆ†è¾¨ç‡ä¸‹è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„ä¸ä¸€è‡´ã€‚ æœ€è¿‘çš„ç ”ç©¶ [20, 15] æå‡ºäº†è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ã€‚

Diffusion models often underperform at high resolutions due to inadequate disruption of the image during the forward diffusion process. As illustrated in Figure 4, when noise is added following the schedule provided in SDXL [27], low-resolution images degrade into nearly pure noise, whereas highresolution images tend to retain low-frequency components at the terminal stage. Since the model must start from pure Gaussian noise during inference, this discrepancy can cause inconsistencies between training and inference at high resolutions. Recent studies [20, 15] have proposed methods to address this issue.

![alt text](assets/624628/image-8.png)



[15] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 13213â€“13232. PMLR, 2023.

[15] åŸƒç±³å°”Â·èƒ¡æ ¼åšå§†ã€ä¹”çº³æ£®Â·å¸Œå…‹å’Œè’‚å§†Â·è¨åˆ©æ›¼ã€‚ ç®€å•æ‰©æ•£ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒçš„ç«¯åˆ°ç«¯æ‰©æ•£ã€‚ å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œç¬¬ 13213-13232 é¡µã€‚  PMLRï¼Œ2023ã€‚


[20] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed, 2024.


åœ¨ Kolorsï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäº DDPM çš„è®­ç»ƒæ–¹æ³• [13] å’Œ epsilon é¢„æµ‹ç›®æ ‡ã€‚ åœ¨æ¦‚å¿µå­¦ä¹ çš„ä½åˆ†è¾¨ç‡è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸ SDXL [27] ç›¸åŒçš„å™ªå£°æ–¹æ¡ˆã€‚ 

In Kolors, we adopt a DDPM-based training approach [13] with an epsilon prediction objective.

å¯¹äºé«˜åˆ†è¾¨ç‡è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è°ƒåº¦ï¼Œç®€å•åœ°å°†æ­¥æ•°ä»åŸæ¥çš„ 1000 æ‰©å±•åˆ° 1100ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå®ç°æ›´ä½çš„ç»ˆç«¯ä¿¡å™ªæ¯”ã€‚ 

achieve a lower terminal signal-to-noise ratio.


æ­¤å¤–ï¼Œæˆ‘ä»¬è°ƒæ•´ Î² çš„å€¼ä»¥ä¿æŒ Î±t æ›²çº¿çš„å½¢çŠ¶ï¼Œå…¶ä¸­ Î±t ç¡®å®š xt = âˆšÎ±tx0 + âˆš1 âˆ’ Î±tÎµã€‚ å¦‚å›¾ 5 æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„ Î±t è½¨è¿¹å®Œå…¨åŒ…å«äº†åŸºæœ¬è°ƒåº¦çš„è½¨è¿¹ï¼Œ    

è€Œå…¶ä»–æ–¹æ³•çš„è½¨è¿¹åˆ™è¡¨ç°å‡ºæ˜¾ç€çš„åå·®ã€‚ è¿™è¡¨æ˜ï¼Œå½“ä»ä½åˆ†è¾¨ç‡ä½¿ç”¨çš„åŸºæœ¬æ—¶é—´è¡¨è½¬æ¢æ—¶ï¼Œä¸å…¶ä»–æ—¶é—´è¡¨ç›¸æ¯”ï¼Œæ–°æ—¶é—´è¡¨çš„é€‚åº”å’Œå­¦ä¹ éš¾åº¦é™ä½äº†ã€‚


![alt text](assets/624628/image-9.png)

![alt text](assets/624628/image-10.png)

å¦‚å›¾ 6 æ‰€ç¤ºï¼Œé€šè¿‡å°†é«˜è´¨é‡è®­ç»ƒæ•°æ®ä¸ä¼˜åŒ–çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒæŠ€æœ¯ç›¸ç»“åˆ (Improving Visual Appeal
)ï¼Œç”Ÿæˆå›¾åƒçš„è´¨é‡å¾—åˆ°äº†æ˜¾ç€æé«˜ã€‚ æ­¤å¤–ï¼Œ

Improving Visual Appeal

ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸åŒé•¿å®½æ¯”çš„å›¾åƒï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†NovelAIçš„åˆ†æ¡¶é‡‡æ ·æ–¹æ³•[25]ã€‚ ä¸ºäº†èŠ‚çœè®­ç»ƒèµ„æºï¼Œè¯¥ç­–ç•¥ä»…åœ¨é«˜åˆ†è¾¨ç‡è®­ç»ƒæœŸé—´åº”ç”¨ã€‚ å›¾ 1 å’Œå›¾ 9 æ˜¾ç¤ºäº†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒç¤ºä¾‹ã€‚


ä¸ºäº†å‡†ç¡®è¯„ä¼° Kolors çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸‰ä¸ªåŸºæœ¬è¯„ä¼°æŒ‡æ ‡ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªæ–°çš„åŸºå‡† KolorsPromptsï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªç±»åˆ«å’Œä¸åŒæŒ‘æˆ˜çš„æç¤ºé›†ã€‚ ç„¶åæˆ‘ä»¬ä½¿ç”¨ KolorsPrompts æ ¹æ®äººç±»åå¥½è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬è®¡ç®—ä¸¤ä¸ªè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼š(a) å¤šç»´åå¥½å¾—åˆ† (MPS) [50] å’Œ (b) ä¼—æ‰€å‘¨çŸ¥çš„å›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡ FIDã€‚ æˆ‘ä»¬å°† Kolors ä¸å¸‚åœºä¸Šå¯ç”¨çš„å¼€æºæ¨¡å‹å’Œä¸“æœ‰ç³»ç»Ÿè¿›è¡Œæ¯”è¾ƒã€‚

Human Evaluation 

We provide three human evaluation metrics to assess the modelâ€™s performance:

è§†è§‰å¸å¼•åŠ›ã€‚ è§†è§‰å¸å¼•åŠ›æ˜¯æŒ‡ç”Ÿæˆå›¾åƒçš„æ•´ä½“ç¾å­¦è´¨é‡ï¼ŒåŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶ã€çº¹ç†å’Œæ„å›¾ç­‰å„ç§è§†è§‰å…ƒç´ ï¼Œä»¥åˆ›é€ ä»¤äººæ„‰æ‚¦å’Œå¼•äººå…¥èƒœçš„å¤–è§‚ã€‚ åœ¨æœ¬æ¬¡è¯„æµ‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æç¤ºå‘ç”¨æˆ·å±•ç¤ºä¸åŒæ¨¡å‹ç”Ÿæˆçš„å›¾åƒï¼Œè€Œä¸æ˜¾ç¤ºç›¸åº”çš„æ–‡å­—æè¿°ã€‚ è¿™ç§æ–¹æ³•å…è®¸ç”¨æˆ·ä¸“æ³¨äºå›¾åƒçš„è§†è§‰å¸å¼•åŠ›ã€‚ æ¯ä½è¯„ä¼°å‘˜å¯¹ä¸€å¼ å›¾åƒè¿›è¡Œè¯„åˆ†ï¼Œè¯„åˆ†èŒƒå›´ä¸º 1 åˆ° 5ï¼Œå…¶ä¸­ 5 è¡¨ç¤ºå®Œç¾ï¼Œ1 è¡¨ç¤ºè´¨é‡æœ€ä½ã€‚    

â€¢ æ–‡æœ¬å¿ å®åº¦ã€‚ æ–‡æœ¬å¿ å®åº¦è¡¡é‡ç”Ÿæˆçš„å›¾åƒä¸å…¶éšé™„æç¤ºçš„å¯¹åº”ç¨‹åº¦ã€‚ è¯„ä¼°äººå‘˜è¢«è¦æ±‚å¿½ç•¥å›¾åƒè´¨é‡ï¼Œåªå…³æ³¨æ–‡æœ¬æè¿°å’Œå›¾åƒä¹‹é—´çš„ç›¸å…³æ€§ã€‚ è¯„åˆ†èŒƒå›´ä¸º 1 åˆ° 5ã€‚    


â€¢ æ€»ä½“æ»¡æ„åº¦ã€‚ æ€»ä½“æ»¡æ„åº¦ä»£è¡¨å¯¹å›¾åƒçš„æ•´ä½“è¯„ä¼°ã€‚ åœ¨æ­¤è¯„ä¼°ä¸­ï¼Œæç¤ºå°†æ˜¾ç¤ºåœ¨å›¾åƒæ—è¾¹ã€‚ è¯„ä¼°è€…æ ¹æ®å›¾åƒçš„è´¨é‡ã€è§†è§‰å¸å¼•åŠ›ä»¥åŠæç¤ºä¸å›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§æ¥è¯„ä¼°å›¾åƒï¼Œå¹¶æŒ‰ 1 åˆ° 5 çš„ç­‰çº§è¿›è¡Œè¯„åˆ†ã€‚




æ­£åœ¨è¯„ä¼°çš„æ¨¡å‹æ¯ä¸ªæç¤ºç”Ÿæˆå››ä¸ªå›¾åƒã€‚ æˆ‘ä»¬è˜è¯·äº†å¤§çº¦ 50 åä¸“ä¸šè¯„å®¡å‘˜ï¼Œæ ¹æ®æŒ‡å®šçš„æŒ‡å—å¯¹æ¯å¼ å›¾åƒè¿›è¡Œäº†äº”æ¬¡è¯„ä¼°ã€‚ å›¾åƒçš„æœ€ç»ˆåˆ†æ•°è®¡ç®—ä¸ºè¿™äº”æ¬¡è¯„ä¼°çš„å¹³å‡è¯„åˆ†ã€‚ å› æ­¤ï¼Œæ¯å¼ å›¾åƒéƒ½ä¼šåœ¨è§†è§‰å¸å¼•åŠ›ã€æ–‡æœ¬å¿ å®åº¦å’Œæ•´ä½“æ»¡æ„åº¦æ–¹é¢è·å¾—ä¸‰ä¸ªä¸åŒçš„åˆ†æ•°ã€‚ æ‰€æœ‰å›¾åƒå‡ä»¥ 1024Ã—1024 åƒç´ çš„åˆ†è¾¨ç‡æ¸²æŸ“ã€‚


![alt text](assets/624628/image-11.png)


è‡ªåŠ¨è¯„ä¼°åŸºå‡† 

3.3.1 å¤šç»´äººç±»åå¥½è¯„åˆ†ï¼ˆMPSï¼‰ 

å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ä¸»è¦ä¾èµ–äºå•ä¸€æµ‹é‡ï¼ˆä¾‹å¦‚ï¼ŒFIDã€CLIP è¯„åˆ† [28]ï¼‰ï¼Œè¿™ä¸è¶³ä»¥æ•æ‰äººç±»åå¥½ã€‚ å¤šç»´äººç±»åå¥½è¯„åˆ†ï¼ˆMPSï¼‰[50]è¢«æå‡ºæ¥ä»äººç±»åå¥½çš„å¤šä¸ªç»´åº¦è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨æ–‡æœ¬åˆ°å›¾åƒè¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ MPS åœ¨ KolorsPrompts åŸºå‡†ä¸Šè¯„ä¼°ä¸Šè¿°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚


MPS çš„ç»“æœå¦‚è¡¨ 3 æ‰€ç¤ºã€‚æ®è§‚å¯Ÿï¼ŒKolors å®ç°äº†æœ€é«˜æ€§èƒ½ï¼Œä¸äººç±»è¯„ä¼°ä¸€è‡´ã€‚ è¿™ç§ä¸€è‡´æ€§è¡¨æ˜äººç±»åå¥½ä¸ KolorsPrompts åŸºå‡†ä¸Šçš„ MPS åˆ†æ•°ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚



MPS çš„ç»“æœå¦‚è¡¨ 3 æ‰€ç¤ºã€‚æ®è§‚å¯Ÿï¼ŒKolors å®ç°äº†æœ€é«˜æ€§èƒ½ï¼Œä¸äººç±»è¯„ä¼°ä¸€è‡´ã€‚ è¿™ç§ä¸€è‡´æ€§è¡¨æ˜äººç±»åå¥½ä¸ KolorsPrompts åŸºå‡†ä¸Šçš„ MPS åˆ†æ•°ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚

COCO æ•°æ®é›†çš„ä¿çœŸåº¦è¯„ä¼° 

æˆ‘ä»¬è¿˜ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼ˆå³ MS-COCO 256 Ã— 256 [21] éªŒè¯æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬ FID-30Kï¼‰è¯„ä¼° Kolorsã€‚ è¡¨ 4 æ˜¾ç¤ºäº† Kolors ä¸å…¶ä»–ç°æœ‰æ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒã€‚  Kolors è·å¾—äº†ç•¥é«˜çš„ FID åˆ†æ•°ï¼Œè¿™å¯èƒ½ä¸ä¼šè¢«è§†ä¸ºå…·æœ‰é«˜åº¦ç«äº‰åŠ›çš„ç»“æœã€‚ 

ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸º FID å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå®Œå…¨é€‚åˆè¯„ä¼°å›¾åƒè´¨é‡çš„æŒ‡æ ‡ï¼Œå› ä¸ºè¾ƒé«˜çš„åˆ†æ•°å¹¶ä¸ä¸€å®šä¸ç”Ÿæˆçš„å›¾åƒè´¨é‡è¾ƒé«˜ç›¸å…³ã€‚



å¤§é‡ç ”ç©¶[5,7,27,17,50]è¡¨æ˜COCOä¸Šçš„é›¶æ ·æœ¬FIDä¸è§†è§‰ç¾è§‚è´Ÿç›¸å…³ï¼Œå¹¶ä¸”äººç±»è¯„ä¼°è€…æ¯”äººå·¥è¯„ä¼°è€…æ›´å‡†ç¡®åœ°è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç”Ÿæˆæ€§èƒ½ ç»Ÿè®¡æŒ‡æ ‡ã€‚ è¿™äº›å‘ç°å¼ºè°ƒäº†å»ºç«‹ç¬¦åˆäººç±»çœŸå®åå¥½çš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿçš„å¿…è¦æ€§ï¼Œä¾‹å¦‚ MPS [21]ã€‚

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† Kolorsï¼Œä¸€ç§åŸºäºç»å…¸ U-Net æ¶æ„æ„å»ºçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ [27]ã€‚ é€šè¿‡åˆ©ç”¨é€šç”¨è¯­è¨€æ¨¡å‹ (GLM) å’Œ CogVLM ç”Ÿæˆçš„ç»†ç²’åº¦æ ‡é¢˜ï¼ŒKolors æ“…é•¿ç†è§£å¤æ‚çš„è¯­ä¹‰ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¤šä¸ªå®ä½“çš„è¯­ä¹‰ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„æ–‡æœ¬æ¸²æŸ“åŠŸèƒ½ã€‚ æ­¤å¤–ï¼ŒKolors çš„åŸ¹è®­ç»å†äº†ä¸¤ä¸ªä¸åŒçš„é˜¶æ®µï¼šæ¦‚å¿µå­¦ä¹ é˜¶æ®µå’Œè´¨é‡æ”¹è¿›é˜¶æ®µã€‚ é€šè¿‡åˆ©ç”¨é«˜ç¾å­¦æ•°æ®å¹¶é‡‡ç”¨æ–°çš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæ–¹æ¡ˆï¼Œæ‰€å¾—é«˜åˆ†è¾¨ç‡å›¾åƒçš„è§†è§‰å¸å¼•åŠ›æ˜¾ç€å¢å¼ºã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç±»åˆ«å¹³è¡¡åŸºå‡† KolorsPrompts æ¥å…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚  Kolors åœ¨äººç±»è¯„ä¼°ä¸­å–å¾—äº†å‡ºè‰²çš„è¡¨ç°ï¼Œè¶…è¶Šäº†å¤§å¤šæ•°å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œä¾‹å¦‚ Stable Diffusion 3ã€Playground-v2.5 å’Œ DALL-E 3ï¼Œå¹¶è¡¨ç°å‡ºä¸ Midjourney-v6 ç›¸å½“çš„æ€§èƒ½ã€‚



æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒå…¬å¼€å‘å¸ƒ Kolors çš„æ¨¡å‹æƒé‡å’Œä»£ç ã€‚ åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€æ­¥å‘å¸ƒ Kolors çš„å„ç§åº”ç”¨ç¨‹åºå’Œæ’ä»¶ï¼ŒåŒ…æ‹¬ ControlNet [48]ã€IP-Adapter [46] å’Œ LCM [23] ç­‰ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬æ‰“ç®—å‘å¸ƒä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„æ–°çš„ä¸“æœ‰æ‰©æ•£æ¨¡å‹ [26]ã€‚ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®© Kolors æ¨åŠ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆç¤¾åŒºçš„è¿›æ­¥ï¼Œå¹¶è‡´åŠ›äºä¸ºå¼€æºç”Ÿæ€ç³»ç»Ÿåšå‡ºé‡å¤§è´¡çŒ®ã€‚





cnä¸ºä»€ä¹ˆä¸èƒ½é€‚é…  ç»´åº¦å—

ä¸éƒ½æ˜¯unet???





## ç»“æ„

### text_encoder

    "_name_or_path": "THUDM/chatglm3-6b-base",

    "architectures": [
        "ChatGLMModel"
    ],
    
    "padded_vocab_size": 65024,
    "post_layer_norm": true,
    "rmsnorm": true,
    "seq_length": 32768,

    "ffn_hidden_size": 13696,
    "fp32_residual_connection": false,
    "hidden_dropout": 0.0,
    "hidden_size": 4096,
    "kv_channels": 128,

sdxl     
text_encoder1 

    "architectures": [
        "CLIPTextModel"
    ],
    hidden_size": 768,
    "initializer_factor": 1.0,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "layer_norm_eps": 1e-05,
    "max_position_embeddings": 77,

    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "pad_token_id": 1,
    "projection_dim": 768,

text_encoder2

    "architectures": [
        "CLIPTextModelWithProjection"
    ],

    "hidden_size": 1280,

    "intermediate_size": 5120,
    "layer_norm_eps": 1e-05,
    "max_position_embeddings": 77,

    "num_attention_heads": 20,
    "num_hidden_layers": 32,
    "pad_token_id": 1,
    "projection_dim": 1280,
    "torch_dtype": "float16",
    "transformers_version": "4.32.0.dev0",
    "vocab_size": 49408


### scheduler


    "_class_name": "EulerDiscreteScheduler",
    "_diffusers_version": "0.18.0.dev0",
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "beta_end": 0.014,
    "clip_sample": false,
    "clip_sample_range": 1.0,
    "dynamic_thresholding_ratio": 0.995,
    "interpolation_type": "linear",
    "num_train_timesteps": 1100,
    "prediction_type": "epsilon",

sdxl


    "_class_name": "EulerDiscreteScheduler",
    "_diffusers_version": "0.19.0.dev0",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "interpolation_type": "linear",
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "sample_max_value": 1.0,
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 1,
    "timestep_spacing": "leading",
    "trained_betas": null,
    "use_karras_sigmas": false

æ‰¾ä¸åˆ°edmè®ºæ–‡ä¸­çš„å¯¹åº”


playground

    "_class_name": "EDMDPMSolverMultistepScheduler",
    "_diffusers_version": "0.27.0.dev0",
    "algorithm_type": "dpmsolver++",
    "dynamic_thresholding_ratio": 0.995,
    "euler_at_final": false,
    "final_sigmas_type": "zero",
    "lower_order_final": true,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "rho": 7.0,
    "sample_max_value": 1.0,
    "sigma_data": 0.5,
    "sigma_max": 80.0,
    "sigma_min": 0.002,
    "solver_order": 2,
    "solver_type": "midpoint",
    "thresholding": false


ç¡®å®å¯¹åº”äº†


![alt text](assets_picture/624628/image-7.png)

edmé¢„æµ‹ç±»å‹ä¹Ÿæ˜¯eps

æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äº SDXLã€Playground v2ã€PixArt-Î±ã€DALL-E 3 å’Œ Midjourney 5.2ã€‚

æ­¤æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒã€‚å®ƒæ˜¯ä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨ä¸¤ä¸ªå›ºå®šçš„ã€é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆOpenCLIP-ViT/G å’Œ CLIP-ViT/Lï¼‰ã€‚å®ƒéµå¾ªä¸ Stable Diffusion XL ç›¸åŒçš„æ¶æ„ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œç®¡é“ä½¿ç”¨è®¡åˆ’ç¨‹åºï¼Œä»¥è·å¾—æ›´æ¸…æ™°ã€æ›´ç²¾ç»†çš„ç»†èŠ‚ã€‚å®ƒæ˜¯ DPM++ 2M Karras è°ƒåº¦å™¨çš„ EDM å…¬å¼ã€‚ æ˜¯æ­¤è°ƒåº¦ç¨‹åºçš„è‰¯å¥½é»˜è®¤å€¼ã€‚

EDMDPMSolverMultistepSchedulerguidance_scale=3.0

ç®¡é“è¿˜æ”¯æŒè®¡åˆ’ç¨‹åºã€‚å®ƒæ˜¯ Euler è°ƒåº¦å™¨çš„ EDM å…¬å¼ã€‚ æ˜¯æ­¤è°ƒåº¦ç¨‹åºçš„è‰¯å¥½é»˜è®¤å€¼ã€‚EDMEulerSchedulerguidance_scale=5.0










# lllyasviel/Paints-UNDO

è¯¥æ¨¡å‹æ˜¯ SD1.5 çš„æ”¹è¿›æ¶æ„ï¼Œåœ¨ä¸åŒçš„ beta è°ƒåº¦å™¨ã€å‰ªè¾‘è·³è¿‡å’Œä¸Šè¿°æ¡ä»¶ä¸‹è¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ä»¥ä¸‹ beta è¿›è¡Œè®­ç»ƒï¼šoperation step

betas = torch.linspace(0.00085, 0.020, 1000, dtype=torch.float64)

ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸå§‹ SD1.5 ä½¿ç”¨ä»¥ä¸‹ beta è¿›è¡Œè®­ç»ƒï¼š

betas = torch.linspace(0.00085 ** 0.5, 0.012 ** 0.5, 1000, dtype=torch.float64) ** 2

æ‚¨å¯ä»¥æ³¨æ„åˆ°ç»“æŸ beta å’Œå·²åˆ é™¤æ–¹å—çš„å·®å¼‚ã€‚æ­¤è°ƒåº¦å™¨çš„é€‰æ‹©åŸºäºæˆ‘ä»¬çš„å†…éƒ¨ç”¨æˆ·ç ”ç©¶ã€‚



æ–‡æœ¬ç¼–ç å™¨ CLIP ViT-L/14 çš„æœ€åä¸€å±‚è¢«æ°¸ä¹…ç§»é™¤ã€‚ç°åœ¨ï¼Œå§‹ç»ˆå°† CLIP Skip è®¾ç½®ä¸º 2ï¼ˆå¦‚æœæ‚¨ä½¿ç”¨æ‰©æ•£å™¨ï¼‰ï¼Œè¿™åœ¨æ•°å­¦ä¸Šæ˜¯ä¸€è‡´çš„ã€‚

è¯¥æ¡ä»¶ä»¥ç±»ä¼¼äº SDXL çš„é¢å¤–åµŒå…¥çš„æ–¹å¼æ·»åŠ åˆ°å±‚åµŒå…¥ä¸­ã€‚operation step


## æ¨¡å‹æ¶æ„ ï¼ˆpaints_undo_multi_frameï¼‰
è¯¥æ¨¡å‹æ˜¯é€šè¿‡ä» VideoCrafter ç³»åˆ—ä¸­æ¢å¤æ¥è®­ç»ƒçš„ï¼Œä½†æœªä½¿ç”¨åŸå§‹çš„ Crafterï¼Œæ‰€æœ‰è®­ç»ƒ/æ¨ç†ä»£ç éƒ½å®Œå…¨ä»å¤´å¼€å§‹å®ç°ã€‚ï¼ˆé¡ºä¾¿è¯´ä¸€å¥ï¼Œç°åœ¨ä»£ç åŸºäºç°ä»£æ‰©æ•£å™¨ã€‚è™½ç„¶ä»VideoCrafteræ¢å¤äº†æœ€åˆçš„æƒé‡ï¼Œä½†ç¥ç»ç½‘ç»œçš„æ‹“æ‰‘ç»“æ„è¿›è¡Œäº†å¤§é‡ä¿®æ”¹ï¼Œç°åœ¨çš„ç½‘ç»œè¡Œä¸ºåœ¨ç»è¿‡å¤§é‡è®­ç»ƒåä¸åŸå§‹Crafteræœ‰å¾ˆå¤§ä¸åŒã€‚lvdm


æ•´ä½“æ¶æ„å°±åƒ Crafterï¼Œæœ‰ 5 ä¸ªç»„ä»¶ï¼Œ3D-UNetã€VAEã€CLIPã€CLIP-Visionã€Image Projectionã€‚


VAEï¼šVAE æ˜¯ä» ToonCrafter ä¸­æå–çš„å®Œå…¨ç›¸åŒçš„åŠ¨æ¼« VAEã€‚éå¸¸æ„Ÿè°¢ ToonCrafter ä¸º Crafters æä¾›å‡ºè‰²çš„åŠ¨æ¼« Temporal VAEã€‚


3D-UNetï¼š3D-UNet æ˜¯ä» Crafters ä¿®æ”¹è€Œæ¥çš„ï¼Œä½†å¯¹æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œäº†ä¿®è®¢ã€‚é™¤äº†ä»£ç ä¸­çš„ä¸€äº›å¾®å°å˜åŒ–å¤–ï¼Œä¸»è¦çš„å˜åŒ–æ˜¯ç°åœ¨UNetå·²ç»è¿‡è®­ç»ƒï¼Œå¹¶æ”¯æŒç©ºé—´è‡ªæˆ‘æ³¨æ„å±‚ä¸­çš„æ—¶é—´çª—å£ã€‚æ‚¨å¯ä»¥æ›´æ”¹ å’Œ æ¿€æ´»ä¸‰ç§ç±»å‹çš„æ³¨æ„çª—å£ä¸­çš„ä»£ç ï¼šlvdmdiffusers_vdm.attention.CrossAttention.temporal_window_for_spatial_self_attentiontemporal_window_type

    â€œPRVâ€æ¨¡å¼ï¼šæ¯å¸§çš„ç©ºé—´è‡ªæ³¨æ„åŠ›ä¹Ÿå…³æ³¨å…¶å‰ä¸€å¸§çš„å…¨éƒ¨ç©ºé—´ä¸Šä¸‹æ–‡ã€‚ç¬¬ä¸€å¸§åªå‚ä¸è‡ªèº«ã€‚
    â€œfirstâ€ï¼šæ¯ä¸€å¸§çš„ç©ºé—´è‡ªæ³¨æ„ä¹Ÿå…³æ³¨æ•´ä¸ªåºåˆ—çš„ç¬¬ä¸€å¸§çš„å…¨éƒ¨ç©ºé—´ä¸Šä¸‹æ–‡ã€‚ç¬¬ä¸€å¸§åªå…³æ³¨å®ƒè‡ªå·±ã€‚
    â€œrollâ€ï¼šæ ¹æ® çš„é¡ºåºï¼Œæ¯ä¸ªå¸§çš„ç©ºé—´è‡ªæ³¨æ„ä¹Ÿå…³æ³¨å…¶å‰ä¸€å¸§å’Œä¸‹ä¸€å¸§çš„å®Œæ•´ç©ºé—´ä¸Šä¸‹æ–‡ã€‚torch.roll
è¯·æ³¨æ„ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤åŠŸèƒ½åœ¨æ¨ç†ä¸­å¤„äºç¦ç”¨çŠ¶æ€ï¼Œä»¥èŠ‚çœ GPU å†…å­˜ã€‚


CLIPï¼šSD2.1çš„å‰ªè¾‘ã€‚

CLIP-Visionï¼šæˆ‘ä»¬çš„ Clip Vision ï¼ˆViT/Hï¼‰ å®ç°ï¼Œé€šè¿‡æ’å€¼ä½ç½®åµŒå…¥æ¥æ”¯æŒä»»æ„çºµæ¨ªæ¯”ã€‚åœ¨å°è¯•äº†çº¿æ€§æ’å€¼ã€æœ€è¿‘é‚»å’Œæ—‹è½¬ä½ç½®ç¼–ç  ï¼ˆRoPEï¼‰ ä¹‹åï¼Œæˆ‘ä»¬çš„æœ€ç»ˆé€‰æ‹©æ˜¯æœ€è¿‘é‚»ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸å°†å›¾åƒå¤§å°è°ƒæ•´æˆ–å±…ä¸­è£å‰ªä¸º 224x224 çš„ Crafter æ–¹æ³•ä¸åŒã€‚

å›¾åƒæŠ•å½±ï¼šæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå¾®å‹è½¬æ¢å™¨ï¼Œè¯¥è½¬æ¢å™¨å°†ä¸¤å¸§ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸ºæ¯å¸§è¾“å‡º 16 ä¸ªå›¾åƒåµŒå…¥ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ä»…ä½¿ç”¨ä¸€ä¸ªå›¾åƒçš„ Crafter æ–¹æ³•ä¸åŒã€‚




è¿›å…¥Gradioç•Œé¢åï¼š

ç¬¬ 0 æ­¥ï¼šä¸Šä¼ å›¾ç‰‡æˆ–ç›´æ¥å•å‡»é¡µé¢åº•éƒ¨çš„ç¤ºä¾‹å›¾ç‰‡ã€‚

æ­¥éª¤1ï¼šåœ¨æ ‡é¢˜ä¸ºâ€œæ­¥éª¤1â€çš„UIä¸­ï¼Œç‚¹å‡»ç”Ÿæˆæç¤ºï¼Œè·å–å…¨å±€æç¤ºã€‚

æ­¥éª¤2ï¼šåœ¨æ ‡é¢˜ä¸ºâ€œæ­¥éª¤2â€çš„UIä¸­ï¼Œå•å‡»â€œç”Ÿæˆå…³é”®å¸§â€ã€‚æ‚¨å¯ä»¥åœ¨å·¦ä¾§æ›´æ”¹ç§å­æˆ–å…¶ä»–å‚æ•°ã€‚

æ­¥éª¤3ï¼š åœ¨æ ‡é¢˜ä¸ºâ€œæ­¥éª¤3â€çš„UIä¸­ï¼Œå•å‡»â€œç”Ÿæˆè§†é¢‘â€ã€‚æ‚¨å¯ä»¥åœ¨å·¦ä¾§æ›´æ”¹ç§å­æˆ–å…¶ä»–å‚æ•°ã€‚

10å°æ—¶å‰ 7.9

æˆ‘ç”¨å®ƒæ¥å¯¹ç¬¬ä¸€æ–¹çš„ææ¡ˆè¿›è¡Œçµæ„Ÿåˆ†æï¼Œå› ä¸ºç¬¬ä¸€æ–¹çš„ç»å¤§å¤šæ•°éƒ½æ— æ³•æ¸…æ¥šåœ°è¡¨è¾¾ä»–ä»¬çš„éœ€æ±‚ï¼Œä¾‹å¦‚å¾½æ ‡å’Œå»ºç­‘è®¾è®¡ç­‰ã€‚è¿™æ²¡æœ‰å¤ªå¤šçš„è‰ºæœ¯å†…å®¹ï¼Œä½†æˆ‘è®¤ä¸ºè¿™å¯¹ç¬¬ä¸€æ–¹å’Œç¬¬äºŒæ–¹ä¹‹é—´çš„æ²Ÿé€šéå¸¸æœ‰å¸®åŠ©

Understand Human Behavior to Align True Needs


I use it for an inspiration analysis of the first party's proposal, as the vast majority of the first party cannot express their needs clearly, such as logo and architectural design, etc. This does not have much artistic content, but I think it is very helpful for communication between the first and second parties











# ç»“å°¾
# sd3éƒ¨ç½²
å†™æ³•ä¸€èˆ¬ç›´æ¥é‡‡ç”¨hug spaceä¸Šçš„     
æˆ–è€…å…³æ³¨ä¸€ä¸ªäººï¼Œç»å¸¸å†™jupyterå®ç°comfyuiå’Œwebuiçš„é‡‡æ ·ï¼Œæˆ–è€…ç›´æ¥ç”¨ä»–çš„ï¼Œä»–ä¸€èˆ¬éƒ½ä¼šå»å®ç°        

éƒ¨ç½²ä½ç½®    
teams     
/newlytest/stable-diffusion-3-medium/app.py

diffusers from_pretrained     
é»˜è®¤ cache_dir none     
PosixPath('/root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3-medium-diffusers/snapshots/b1148b4028b9ec56ebd36444c193d56aeff7ab56')

![alt text](assets/6367/image-3.png)    
![alt text](assets/6367/image-4.png)

éƒ½æ˜¯è½¯è¿æ¥    

repo = "stabilityai/stable-diffusion-3-medium-diffusers"
pipe = StableDiffusion3Pipeline.from_pretrained(repo, torch_dtype=torch.float16, token='').to(device)


åŠŸèƒ½æœ‰ï¼šç›‘æ§æ—¶é—´å’Œè¾“å…¥ä¿¡æ¯    
ç®€å•æ”¹å†™scheduler    


gradio

æ¨¡å‹ä¸‹è½½å®Œåè¿è¡Œï¼Œtokenizerä¸åŒ¹é…     
å¾ˆå¤šåŒ…è¦æœ€æ–°çš„      
æ‰‹åŠ¨è£…ï¼Œæœ€årequeståŒ…è¿˜æ˜¯ä¸åŒ¹é…     


huggingface tokenä¸‹è½½      

20g 1024*1024

ç›´æ¥ä½¿ç”¨ huggingface spaceçš„ gradioéƒ¨ç½²


Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.

dpmpp 2m ç›´æ¥é…ç½®è¿˜æ˜¯é»‘å›¾     
å‡ºä¸äº†æ­£å¸¸å›¾    
diffusersé‡‡æ ·å™¨ä¸çŸ¥é“å¦‚ä½•æ”¹


ä¸ºä»€ä¹ˆiclightèƒ½ä½¿ç”¨ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ


æ™®é€šç”Ÿå›¾æ‰‹æŒ‡ç•¸å½¢


a green sign that says "Very Deep learning" and is at the edge of the Grand Canyon

![alt text](assets/6367/image.png)


A portrait photo of a kangaroo wearing an orange hoodie
and blue sunglasses standing on the grass in front of the Sydney
Opera House holding a sign on the chest that says "WUJIE"!

![alt text](assets/6367/image-1.png)


A portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses standing on the grass in front of the Sydney Opera House holding a sign on the chest that says "é™†"!

![alt text](assets/6367/image-2.png)

å¦‚æœæ˜¯comfyuiä¹Ÿè®¸ä¼šå¥½ä¸€äº›    

## webui sd3åˆ†æ”¯æµ‹è¯•

å¯åŠ¨æ—¶å€™

a1111webui193/stable-diffusion-webui/modules/sd_disable_initialization.py", line 68, in CLIPTextModel_from_pretrained
res = self.CLIPTextModel_from_pretrained(None, *model_args, config=pretrained_model_name_or_path, state_dict={}, **kwargs)
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3051, in from_pretrained
resolved_config_file = cached_file(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/transformers/utils/hub.py", line 422, in cached_file
raise EnvironmentError(
OSError: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with huggingface-cli login or by passing token=<your_token>

Failed to create model quickly; will retry using slow method


åŠ è½½æ¨¡å‹æ—¶å€™

Creating model from config: /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/configs/sd3-inference.yaml
mmdit initializing with: input_size=None, patch_size=2, in_channels=16, depth=24, mlp_ratio=4.0, learn_sigma=False, adm_in_channels=2048, context_embedder_config={'target': 'torch.nn.Linear', 'params': {'in_features': 4096, 'out_features': 1536}}, register_length=0, attn_mode='torch', rmsnorm=False, scale_mod_only=False, swiglu=False, out_channels=None, pos_embed_scaling_factor=None, pos_embed_offset=None, pos_embed_max_size=192, num_patches=36864, qk_norm=None, qkv_bias=True, dtype=torch.float16, device='cpu'
[2024-06-17 06:46:03,468][DEBUG][filelock] - Attempting to acquire lock 140004667105552 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
[2024-06-17 06:46:03,469][DEBUG][filelock] - Lock 140004667105552 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
tokenizer_config.json: 100%|â–ˆ| 1.86k/1.86k [00:00<00:00, 6.9
[2024-06-17 06:46:03,763][DEBUG][filelock] - Attempting to release lock 140004667105552 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
[2024-06-17 06:46:03,763][DEBUG][filelock] - Lock 140004667105552 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
[2024-06-17 06:46:04,056][DEBUG][filelock] - Attempting to acquire lock 140004667097536 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
[2024-06-17 06:46:04,056][DEBUG][filelock] - Lock 140004667097536 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792k/792k [00:01<00:00, 700kB/s]
[2024-06-17 06:46:05,478][DEBUG][filelock] - Attempting to release lock 140004667097536 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
[2024-06-17 06:46:05,478][DEBUG][filelock] - Lock 140004667097536 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
[2024-06-17 06:46:06,385][DEBUG][filelock] - Attempting to acquire lock 140004667108096 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
[2024-06-17 06:46:06,385][DEBUG][filelock] - Lock 140004667108096 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
special_tokens_map.json: 100%|â–ˆ| 1.79k/1.79k [00:00<00:00, 7
[2024-06-17 06:46:06,667][DEBUG][filelock] - Attempting to release lock 140004667108096 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
[2024-06-17 06:46:06,668][DEBUG][filelock] - Lock 140004667108096 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
[2024-06-17 06:46:06,945][DEBUG][filelock] - Attempting to acquire lock 140004667103248 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
[2024-06-17 06:46:06,946][DEBUG][filelock] - Lock 140004667103248 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 593/593 [00:00<00:00, 2.38MB/s]
[2024-06-17 06:46:07,233][DEBUG][filelock] - Attempting to release lock 140004667103248 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
[2024-06-17 06:46:07,233][DEBUG][filelock] - Lock 140004667103248 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
Downloading: "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/clip_g.safetensors" to /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/models/CLIP/clip_g.safetensors

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29G/1.29G [00:59<00:00, 23.4MB/s]
Downloading: "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/clip_l.safetensors" to /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/models/CLIP/clip_l.safetensors

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235M/235M [00:12<00:00, 20.3MB/s


å¥½åƒæ²¡æœ‰ä¸‹è½½ t5 xxl     





Downloading VAEApprox model to: /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/models/VAE-approx/vaeapprox-sd3.pt
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228k/228k [00:00<00:00, 1.21MB/s]
*** Error running process_before_every_sampling: /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/extensions/sd-webui-ic-light/scripts/ic_light_script.py
    Traceback (most recent call last):
      File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/scripts.py", line 840, in process_before_every_sampling
        script.process_before_every_sampling(p, *script_args, **kwargs)
      File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/extensions/sd-webui-ic-light/scripts/ic_light_script.py", line 387, in process_before_every_sampling
        assert self.backend_type == BackendType.Forge
    AssertionError


### è¾“å…¥promptè®¡æ•°æŠ¥é”™
Traceback (most recent call last):
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/routes.py", line 488, in run_predict
output = await app.get_blocks().process_api(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/blocks.py", line 1431, in process_api
result = await self.call_function(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/blocks.py", line 1103, in call_function
prediction = await anyio.to_thread.run_sync(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/anyio/to_thread.py", line 33, in run_sync
return await get_asynclib().run_sync_in_worker_thread(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
return await future
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 807, in run
result = context.run(func, *args)
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/utils.py", line 707, in wrapper
response = f(*args, **kwargs)
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/call_queue.py", line 14, in f
res = func(*args, **kwargs)
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/ui.py", line 185, in update_token_counter
token_count, max_length = max([model_hijack.get_prompt_lengths(prompt) for prompt in prompts], key=lambda args: args[0])
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/ui.py", line 185, in
token_count, max_length = max([model_hijack.get_prompt_lengths(prompt) for prompt in prompts], key=lambda args: args[0])
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/sd_hijack.py", line 328, in get_prompt_lengths
_, token_count = self.clip.process_texts([text])
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in getattr
raise AttributeError(f"'{type(self).name}' object has no attribute '{name}'")
AttributeError: 'SD3Cond' object has no attribute 'process_texts'




### ddimæŠ¥é”™
 modules.devices.NansException: A tensor with NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the "Upcast cross attention layer to float32" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.

aysä¹Ÿæœ‰è¿™ä¸ªé”™

ç›®å‰åªèƒ½ç”¨eular


## sd3-ref

https://github.com/mcmonkey4eva/sd3-ref/

https://github.com/mcmonkey4eva/sd3-ref/blob/master/mmdit.py

è¿™ä¸ªå±äºæ¯”è¾ƒåŸç”Ÿçš„æ¨ç†ä»£ç      
å¾ˆå¤šå¹³å°éƒ½æœ‰ï¼Œä»Šå¤©617    
ä½†æ˜¯è¿™ä¸ªäº‹3æœˆå‰å‘å¸ƒ

æˆ‘æ˜¯FreneticLLCçš„é¦–å¸­æ‰§è¡Œå®˜ã€ Stability.AIçš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä»¥åŠDenizenScriptçš„é¡¹ç›®è´Ÿè´£äººã€‚

å®˜æ–¹å‘å¸ƒ

æˆ‘ä»å°å°±é€šè¿‡æ¸¸æˆæ¨¡ç»„å­¦ä¹ ç¼–ç¨‹ï¼Œä»é‚£ä»¥åæˆ‘å°±å†ä¹Ÿæ²¡æœ‰åœæ­¢è¿‡ã€‚ç°åœ¨æˆ‘çš„ç»éªŒç›¸å½“ä¸°å¯Œã€‚


æ‹¥æœ‰åå¤šå¹´ç»éªŒçš„è½¯ä»¶å¼€å‘äººå‘˜ï¼Œä¸»è¦ä½¿ç”¨ C# å’Œ Javaã€‚









# MixDQ
MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization

æˆ‘ä»¬è®¾è®¡äº† MixDQï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆç²¾åº¦é‡åŒ–æ¡†æ¶ï¼ŒæˆåŠŸè§£å†³äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å‡ æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é‡åŒ–é—®é¢˜ã€‚åœ¨è§†è§‰è´¨é‡ä¸‹é™å’Œå†…å®¹å˜åŒ–å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡çš„æƒ…å†µä¸‹ï¼ŒMixDQ å¯ä»¥å®ç° W4A8ï¼ŒåŒæ—¶å†…å­˜å‹ç¼©ç‡ç›¸å½“äº 3.4 å€ï¼Œå»¶è¿ŸåŠ é€Ÿç‡ç›¸å½“äº 1.5 å€ã€‚

https://github.com/A-suozhang/MixDQ


MixDQ æ˜¯ä¸€ç§æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œå¯åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å‹ç¼©æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å†…å­˜å’Œè®¡ç®—ä½¿ç”¨é‡ã€‚å®ƒæ”¯æŒå°‘æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚ SDXL-turboã€LCM-loraï¼‰ï¼Œä»¥æ„å»ºå¿«é€Ÿå’Œå¾®å°æ‰©æ•£æ¨¡å‹ã€‚æä¾›é«˜æ•ˆçš„ CUDA å†…æ ¸å®ç°ï¼Œä»¥èŠ‚çœå®é™…èµ„æºã€‚


![alt text](assets/6367/image-5.png)


åœ¨å‡ ä¹ä¸å½±å“è§†è§‰è´¨é‡ä¸‹é™å’Œå†…å®¹å˜åŒ–çš„æƒ…å†µä¸‹ï¼ŒMixDQ å¯ä»¥å®ç° W4A8ï¼ŒåŒæ—¶å†…å­˜å‹ç¼©ç‡ç›¸å½“äº3.4 å€ï¼Œå»¶è¿ŸåŠ é€Ÿç‡ç›¸å½“äº 1.5 å€ã€‚


æ‰©æ•£æ¨¡å‹å·²ç»å®ç°äº†æ˜¾è‘—çš„è§†è§‰ç”Ÿæˆè´¨é‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ç»™å®ƒä»¬åœ¨èµ„æºå—é™çš„ç§»åŠ¨è®¾å¤‡ç”šè‡³æ¡Œé¢ GPU ä¸Šçš„åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ€è¿‘çš„å‡ æ­¥æ‰©æ•£æ¨¡å‹é€šè¿‡å‡å°‘å»å™ªæ­¥éª¤æ¥ç¼©çŸ­æ¨ç†æ—¶é—´ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å†…å­˜æ¶ˆè€—ä»ç„¶è¿‡å¤§ã€‚

è®­ç»ƒåé‡åŒ– (PTQ) ç”¨ä½ä½æ•´æ•°å€¼ (INT4/8) ä»£æ›¿é«˜ä½å®½ FP è¡¨ç¤ºï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„é™ä½å†…å­˜æˆæœ¬çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºå°‘æ­¥æ‰©æ•£æ¨¡å‹æ—¶ï¼Œç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨ä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ··åˆç²¾åº¦é‡åŒ–æ¡†æ¶ - MixDQã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸“é—¨çš„ BOS æ„ŸçŸ¥é‡åŒ–æ–¹æ³•ï¼Œç”¨äºé«˜åº¦æ•æ„Ÿçš„æ–‡æœ¬åµŒå…¥é‡åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œåº¦é‡è§£è€¦çµæ•åº¦åˆ†ææ¥æµ‹é‡æ¯ä¸€å±‚çš„çµæ•åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ•´æ•°è§„åˆ’çš„æ–¹æ³•æ¥è¿›è¡Œä½å®½åˆ†é…ã€‚

å°½ç®¡ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨ W8A8 ä¸Šè¾¾ä¸åˆ°è¦æ±‚ï¼Œä½† MixDQ å¯ä»¥åœ¨ä¸æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹å®ç° W8A8ï¼Œåœ¨å‡ ä¹ä¸å½±å“è§†è§‰æ•ˆæœçš„æƒ…å†µä¸‹å®ç° W4A8ã€‚ä¸ FP16 ç›¸æ¯”ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å¤§å°å’Œå†…å­˜æˆæœ¬é™ä½äº†3-4 å€ï¼Œå¹¶å°†å»¶è¿ŸåŠ é€Ÿäº†1.45 å€ã€‚


æˆ‘ä»¬é€šè¿‡å®éªŒå‘ç°ï¼Œä¸å¤šæ­¥æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œå°‘æ­¥æ‰©æ•£æ¨¡å‹å¯¹é‡åŒ–æ›´æ•æ„Ÿï¼Œè€Œç°æœ‰çš„æ‰©æ•£é‡åŒ–æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚Q-æ‰©æ•£ W8A8 é‡åŒ–æ¨¡å‹åœ¨å°‘æ­¥ä¸‹é¢ä¸´ä¸¥é‡çš„è´¨é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œå³ä½¿æ˜¯å¤šæ­¥æ¨¡å‹ï¼Œé‡åŒ–ä¹Ÿä¼šæŸå®³æ–‡æœ¬-å›¾åƒå¯¹é½ã€‚


![alt text](assets/6367/image-6.png)


æˆ‘ä»¬è¿›è¡Œäº†åˆæ­¥å®éªŒï¼Œæ·±å…¥æ¢è®¨äº†é‡åŒ–å¤±è´¥çš„åŸå› ï¼Œå¹¶å‘ç°äº†ä¸¤ä¸ªæœ‰å¯å‘æ€§çš„å‘ç°ï¼šï¼ˆ1ï¼‰é‡åŒ–è¢«ä¸€äº›é«˜åº¦æ•æ„Ÿçš„å±‚â€œç“¶é¢ˆåŒ–â€ã€‚ ï¼ˆ2ï¼‰é‡åŒ–æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†åˆ†åˆ«å½±å“ç”Ÿæˆçš„å›¾åƒè´¨é‡å’Œå†…å®¹ã€‚


![alt text](assets/6367/image-7.png)


æ··åˆç²¾åº¦é‡åŒ–æ¡†æ¶ MixDQ ï¼š

![alt text](assets/6367/image-8.png)

BOS æ„ŸçŸ¥æ–‡æœ¬åµŒå…¥é‡åŒ–    
æˆ‘ä»¬å‘ç° CLIP æ–‡æœ¬åµŒå…¥çš„ç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯é˜»ç¢é‡åŒ–çš„å¼‚å¸¸å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯å¥é¦– (BOS) æ ‡è®°ï¼Œå¯¹äºä¸åŒçš„æç¤ºï¼Œå®ƒä¿æŒä¸å˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç¦»çº¿é¢„å…ˆè®¡ç®—å®ƒå¹¶è·³è¿‡å®ƒçš„é‡åŒ–ã€‚


åº¦é‡è§£è€¦çµæ•åº¦åˆ†æ    
å½“ä»…ä¿ç•™å¯¼è‡´æœ€å¤§é‡åŒ–è¯¯å·®FP16çš„å±‚æ—¶ï¼Œæˆ‘ä»¬å‘ç°ç”Ÿæˆçš„å›¾åƒä»ç„¶é¢ä¸´è´¨é‡ä¸‹é™çš„é—®é¢˜ï¼Œè¿™è¡¨æ˜ç°æœ‰çš„é‡åŒ–çµæ•åº¦åˆ†æçš„å‡†ç¡®æ€§éœ€è¦æé«˜ã€‚å—é‡åŒ–å¯¹å›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½çš„å½±å“çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åº¦é‡è§£è€¦çµæ•åº¦åˆ†ææ–¹æ³•ã€‚æˆ‘ä»¬å°†å„å±‚åˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«å¯¹å®ƒä»¬è¿›è¡Œå…·æœ‰ä¸åŒåº¦é‡çš„çµæ•åº¦åˆ†æã€‚



åŸºäºæ•´æ•°è§„åˆ’çš„ä½å®½åˆ†é…    
åœ¨è·å¾—é‡åŒ–çµæ•åº¦ä¹‹åï¼Œæˆ‘ä»¬å°†ä½å®½åˆ†é…é—®é¢˜è½¬åŒ–ä¸ºæ•´æ•°è§„åˆ’æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨ç°æˆçš„æ±‚è§£å™¨æœ‰æ•ˆåœ°æ±‚è§£ã€‚


![alt text](assets/6367/image-9.png)

æˆ‘ä»¬ç»™å‡ºäº†ä¸€äº›å®šæ€§ç»“æœï¼Œå°†ç»Ÿè®¡åº¦é‡å€¼ä¸ç”Ÿæˆçš„å›¾åƒè”ç³»èµ·æ¥ã€‚å¯ä»¥çœ‹å‡ºï¼Œä¸ Q-Diffusion å’Œæœ´ç´  minmax é‡åŒ–ç›¸æ¯”ï¼ŒMixDQ-W4A8 å¯ä»¥ç”Ÿæˆä¸ FP å›¾åƒå‡ ä¹ç›¸åŒçš„å›¾åƒï¼Œè€Œå…¶ä»–æ–¹æ³•æ— æ³•ä¸º W8A8 ç”Ÿæˆå¯è¯»å›¾åƒã€‚


ä¸å…¶ä»–ç°æœ‰æ‰©æ•£æ¨¡å‹é‡åŒ–å·¥å…·ç›¸æ¯”ï¼Œåªæœ‰é—­å¼ TensorRT INT8 å®ç°å®ç°äº†å®é™…çš„å»¶è¿ŸåŠ é€Ÿã€‚MixDQ æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å°‘æ­¥æ‰©æ•£æ¨¡å‹å®ç°å®é™…å†…å­˜å’Œå»¶è¿Ÿä¼˜åŒ–çš„å·¥å…·ï¼Œå¯å®ç°â€œå¾®å°è€Œå¿«é€Ÿâ€çš„å›¾åƒç”Ÿæˆã€‚


![alt text](assets/6367/image-10.png)


![alt text](assets/6367/image-11.png)


è‡´è°¢    
æˆ‘ä»¬çš„ä»£ç æ˜¯åŸºäºQ-Diffusionå’ŒDiffusers Librarayå¼€å‘çš„ã€‚


å¾…åŠäº‹é¡¹    
è¯„ä¼°è„šæœ¬ï¼ˆFIDã€ClipScoreã€ImageRewardï¼‰   
é«˜æ•ˆçš„ INT8 GPU å†…æ ¸å®ç°  



æ™®é€š fp16 æ²¡æœ‰ 

AttributeError: 'StableDiffusionXLPipeline' object has no attribute 'set_cuda_graph'


AttributeError: 'StableDiffusionXLPipeline' object has no attribute 'run_for_test'


## åŸç†

    def quantize_unet(
            self,
            w_bit=None,
            a_bit=None,
            bos=False,
            # cuda_graph_only=True,
            # run_pipeline=True,
            # compile=False,
        ):
        r"""
        This function helps quantize the UNet in the SDXL Pipeline
        Now we only support quantization with the setting W8A8

        Args:
            w_bit: (`str`):
                the bit width of weight
            a_bit: (`str`):
                the bit width of activation
            bos: (`bool`):
                if to use bos technique
            cuda_graph_only: (`bool`):
                if to use cuda_graph
            run_pipeline: (`bool`):
                run the full pipeline or just the unet
        """

path = hf_hub_download(
            repo_id="Stein-Fun/mixdq_test",
            filename="quant_para_wsym_fp16.pt",
            revision="version_0",
        )













## diffusersä½¿ç”¨å®‰è£…ç¯å¢ƒ
    3  pip install mixdq-extension
    4  pip uninstall torchaudio
    5  pip install xformers==0.0.25 å¯¹æ ‡ torch 2.2.1

AttributeError: 'StableDiffusionXLPipeline' object has no attribute 'quantize_unet'

AttributeError: type object 'DiffusionPipeline' has no attribute 'from_single_file'


    class MixDQ_SDXLTurbo_Pipeline_W8A8(
        DiffusionPipeline,
        FromSingleFileMixin,
        StableDiffusionXLLoraLoaderMixin,
        TextualInversionLoaderMixin,
        IPAdapterMixin,
    ):
        r"""
        Pipeline for text-to-image generation using Stable Diffusion XL.

        This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
        library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)

        In addition the pipeline inherits the following loading methods:
            - *LoRA*: [`loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`]
            - *Ckpt*: [`loaders.FromSingleFileMixin.from_single_file`]

å…ˆè½¬diffusersä½¿ç”¨DiffusionPipelineåŠ è½½sdxl


    IndexError                                Traceback (most recent call last)
    Cell In[48], line 2
        1 # quant the UNet
    ----> 2 pipe.quantize_unet(
        3                 w_bit = 8, 
        4                 a_bit = 8, 
        5                 bos=True, 
        6                 )

    File ~/.cache/huggingface/modules/diffusers_modules/local/pipeline.py:2766, in MixDQ_SDXLTurbo_Pipeline_W8A8.quantize_unet(self, w_bit, a_bit, bos)
    2756 ckpt = torch.load(path, map_location='cpu')
    2758 register_qconfig_from_input_files(
    2759     self.unet,
    2760     # args,
    (...)
    2764     bos_dict=bos_dict
    2765 )
    -> 2766 convert_to_quantized(self.unet, ckpt)

    File ~/.cache/huggingface/modules/diffusers_modules/local/pipeline.py:1924, in convert_to_quantized(unet, ckpt)
    1922 def convert_to_quantized(unet, ckpt):
    1923     # from quantize import convert
    -> 1924     convert(unet,
    1925             mapping={nn.Linear: QuantizedLinear,
    ...
    --> 791         _split = _SPLIT[_NUM]
        792         _NUM = _NUM + 1
        793         # num = num + 1

    IndexError: list index out of range

32ç²¾åº¦çš„é—®é¢˜     
è¿˜æ˜¯sdxlä¸æ”¯æŒï¼Ÿï¼Ÿï¼Ÿ       

IndexError: list index out of range

fp16ä¸è¡Œ

RuntimeError: Not all keys in weight yaml map to a module in UNet.

sd1.5ä¸è¡Œ

åŸä»£ç ä½¿ç”¨

python scr
ipts/txt2img.py \ --config ./configs/stable-diffusion/$config_name \ --base_path $BASE_PATH --batch_size 2 --num_imgs 8  --prompt  "a vanilla and chocolate mixing icecream cone, ice background" \ --fp16

usage: txt2img.py [-h] [--prompt [PROMPT]]
                  [--base_path [BASE_PATH]]
                  [--batch_size BATCH_SIZE] [--cfg CFG]
                  [--config CONFIG]
                  [--image_folder IMAGE_FOLDER]
                  [--num_imgs NUM_IMGS] [--seed SEED]
                  [--fp16]
txt2img.py: error: unrecognized arguments:  --config ./configs/stable-diffusion/sdxl.yaml  --base_path ./logs/debug_fp  --fp16

python scripts/txt2img.py  --config ./configs/stable-diffusion/$config_name  --base_path $BASE_PATH --batch_size 2 --num_imgs 8  --prompt  "a vanilla and chocolate mixing icecream cone, ice background"  --fp16

å¯ä»¥äº†



python scripts/txt2img.py  --config ./configs/stable-diffusion/$config_name  --base_path $BASE_PATH --batch_size 1 --num_imgs 2  --prompt  "(8k, RAW photo,masterpiece
),(realistic, photo-realistic:1.37),ID photo,jk,teen
agers,woman,solo,looking at viewer,simple background
,brown eyes,necktie,upper body"  --fp16


å…¬å¸è‡ªå·±æ¨¡å‹     

python scripts/gen_calib_data.py --config ./configs/stable-diffusion/$config_name.yaml --save_image_path ./debug_imgs


FileNotFoundError: [Errno 2] No such file or directory: './scripts/utils/captions_val2014.json'



Traceback (most recent call last):
  newlytest/MixDQ/scripts/gen_calib_data.py", line 131, in <module>
    main()
  newlytest/MixDQ/scripts/gen_calib_data.py", line 51, in main
    prompt_list, image_path = prepare_coco_text_and_image(json_file=json_file)
  newlytest/MixDQ/quant_utils/qdiff/utils.py", line 587, in prepare_coco_text_and_image
    info = json.load(open(json_file, 'r'))
FileNotFoundError: [Errno 2] No such file or directory: './scripts/utils/captions_val2014.json'







# HelloWorld 7.0 æ›´æ–° - 2024 å¹´ 6 æœˆ 13 æ—¥




HelloWorld 7.0æ˜¯è¿­ä»£ä¼˜åŒ–çš„ç‰ˆæœ¬ï¼Œæ‹¥æœ‰å…¨ç³»åˆ—æœ€ä¼˜çš„æœ¬ä½“è¡¨ç°ï¼Œæ¦‚å¿µèŒƒå›´ä¸ç»†èŠ‚ä¸°å¯Œåº¦è¿›ä¸€æ­¥å¢å¼ºã€‚

æ›´æ–°è¯¦ç»†ä¿¡æ¯ï¼š

é€šè¿‡æ·»åŠ è´Ÿé¢è®­ç»ƒå›¾ç‰‡ã€åŠ å¼ºå§¿åŠ¿è®­ç»ƒã€ä¼˜åŒ–clipæ¨¡å‹ï¼Œæ¨¡å‹è‚¢ä½“å’Œæ‰‹éƒ¨å‡†ç¡®ç‡è¾ƒä¹‹å‰ç‰ˆæœ¬æœ‰æ‰€æå‡ï¼Œæ¨èçš„è´Ÿé¢æç¤ºè¯ä¸ºï¼šâ€œbad handã€bad anatomyã€worst qualityã€ai generated imagesã€low qualityã€average qualityâ€ã€‚

ä»å®˜æ–¹çš„ SPO æ¨¡å‹ä¸­æå–äº†ç»è¿‡å¾®è°ƒçš„ LoRA ï¼Œå¹¶å°†å…¶çº³å…¥ HelloWorld 7.0ã€‚SPO æ˜¯å¯¹ DPO æ–¹æ³•çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä½¿ç”¨ SPO åŸºç¡€æ¨¡å‹æ¯” DPO XL åŸºç¡€æ¨¡å‹å’ŒåŸå§‹ SDXL åŸºç¡€æ¨¡å‹æ€§èƒ½æ›´å¥½ã€‚SPO LoRA å¯ä»¥å¢å¼ºå›¾åƒç»†èŠ‚å’Œå¯¹æ¯”åº¦å¹¶ç¾åŒ–å›¾åƒã€‚æ„Ÿè°¢ SPO èƒŒåçš„æŠ€æœ¯å›¢é˜Ÿã€‚

ç»§ç»­æ‰©å¤§è®­ç»ƒé›†çš„æ¦‚å¿µèŒƒå›´ï¼Œä½†å¯¹è®­ç»ƒé›†è¿›è¡Œäº†ä¼˜åŒ–å’Œç²¾ç®€ï¼ˆå¤§è®­ç»ƒé›†å¾®è°ƒå¤ªè´µï¼Œè€Œä¸”æœ€è¿‘H800ç§Ÿèµ·æ¥ä¹Ÿéš¾ï¼Œè´Ÿæ‹…ä¸èµ·æœ¬åœ°è®­ç»ƒçš„æ—¶é—´ï¼‰ã€‚ç›®å‰æ€»è®­ç»ƒé›†ä¸º20821å¼ å›¾ç‰‡ã€‚è®­ç»ƒé›†åˆ†è¾¨ç‡åˆ†å¸ƒå¦‚ä¸‹ï¼Œå»ºè®®ä½¿ç”¨å›¾ç‰‡æ•°é‡è¾ƒå¤šçš„å‡ ç§åˆ†è¾¨ç‡è¿›è¡Œè¾“å‡ºï¼š

    (832, 1248) - Count: 7128
    (896, 1152) - Count: 6250
    (1248, 832) - Count: 2402
    (1024, 1024) - Count: 1639
    (1360, 768) - Count: 928
    (1152, 896) - Count: 870
    (768, 1360) - Count: 432
    (960, 1088) - Count: 506
    (992, 1056) - Count: 162
    (1088, 960) - Count: 140
    (704, 1472) - Count: 120
    (1056, 992) - Count: 122
    (1472, 704) - Count: 115
    (1632, 640) - Count: 75
    (640, 1632) - Count: 12
ä½¿ç”¨ GPT4O å¯¹æ‰€æœ‰æ•°æ®é›†è¿›è¡Œé‡æ–°æ ‡æ³¨ã€‚æœ¬æ¬¡é‡‡ç”¨äº†ç»“æ„åŒ–çš„æ ‡æ³¨æ–¹æ³•ï¼Œå…·ä½“ç»“æ„ä¸ºï¼šâ€œä¸€å¥è¯æ¦‚æ‹¬æè¿°+å¤šä¸ªå›¾å…ƒæ ‡ç­¾+çµæ„Ÿæ¥è‡ª XXX+ç¾å­¦å“è´¨æè¿°è¯â€ï¼Œå…¶ä¸­ç¾å­¦å“è´¨æè¿°è¯åˆ†ä¸ºäº”ä¸ªç­‰çº§ï¼šæœ€å·®å“è´¨ã€ä½å“è´¨ã€ä¸€èˆ¬å“è´¨ã€æœ€å¥½å“è´¨ã€æ°ä½œã€‚å…¸å‹çš„æ ‡æ³¨ç¤ºä¾‹å¦‚ä¸‹ï¼š

conceptual art featuring a human hand wrapped in red and beige ribbons, isolated against a plain, light background, realistic style, minimalist color scheme, smooth textures, elongated and surreal aesthetic, inspired by salvador dalÃ­'s surrealist works, masterpiece
Inspired by XXX for HelloWorld 7.0ç‰ˆæœ¬æ‰€æ¶‰åŠçš„â€œé«˜é¢‘æ ‡æ³¨è¯è¡¨â€å’Œâ€œé«˜é¢‘è‰ºæœ¯é£æ ¼è¡¨â€ä»…æä¾›ç»™å•†ä¸šæˆæƒç”¨æˆ·ï¼Œä»¥å¾€è´­ä¹°è¿‡Helloworld XLç³»åˆ—æ¨¡å‹æˆæƒçš„ä¼™ä¼´ï¼Œå¦‚æœ‰é—æ¼è¯·è”ç³»æˆ‘å…è´¹è·å–ã€‚

å„ä½ç©å®¶å¯ä»¥å‚è€ƒHelloWorld 6.0é«˜é¢‘æ ‡æ³¨è¯è¡¨ï¼Œå¦å¤–æˆ‘åœ¨å›¾åº“ä¸­ä¹Ÿæä¾›äº†150+å¼ é«˜è´¨é‡çš„HelloWorld 7.0ç¤ºä¾‹å›¾ï¼Œå¯ä»¥ä½œä¸ºå¤§å®¶è¾“å‡ºçš„å‚è€ƒã€‚æ¨¡å‹åˆ¶ä½œä¸æ˜“ï¼Œæ„Ÿè°¢å„ä½ç©å®¶çš„ç†è§£ä¸åŒ…å®¹ï¼


LEOSAM HelloWorld 6.0 Top 250 High-Frequency Tagging Word List 


The main body of the HelloWorld 6.0 training set employs GPT4v tagging. For images that GPT4v cannot tag, cogVQA guided by blip2-opt-6.7b is used for tagging. The tagging language style of these multimodal models differs significantly from the traditional WD1.4 tagger. To facilitate more accurate triggering of different concepts in the training set, I have compiled the top 250 high-frequency tagging words from the HelloWorld 6.0 training set. 

**æ‘„å½±æŠ€æœ¯å’Œç¾å­¦/Photography Techniques and Aesthetics:**

- æ¨¡æ‹Ÿèƒ¶ç‰‡æ‘„å½±ç¾å­¦ (film photography aesthetic)
- æ—¶å°šæ‘„å½± (Fashion photography, Fashion portrait)
- äººåƒæ‘„å½± (Portrait photography, portrait photography, Elegant portrait photography)
- é‡ç”ŸåŠ¨ç‰©æ‘„å½± (wildlife photography)
- ç§æˆ¿æ‘„å½± (Intimate boudoir photography)
- é«˜åˆ†è¾¨ç‡ (high-resolution, high-resolution clarity, high-resolution image, high-resolution portrait)
- æç®€ä¸»ä¹‰é£æ ¼ (minimalist aesthetic, minimalistic aesthetic, minimalistic style, minimalist style, minimalist design)
- å¤å¤ç¾å­¦ (vintage aesthetic)



**æ„å›¾å’ŒèƒŒæ™¯/Composition and Background:**

- ç‰¹å†™ (Close-up, Close-up portrait)
- é¸Ÿç°è§†è§’ (Aerial perspective, Aerial view)
- å¯¹ç§°æ„å›¾ (symmetrical composition)
- æç®€ä¸»ä¹‰æ„å›¾ (minimalist composition)
- æµ…æ™¯æ·± (shallow depth of field)



**è‰²å½©:/Color:**

- é»‘ç™½ (Black and white portrait, Monochrome portrait)
- æš–è‰²è°ƒ (warm color palette, Warm)
- å†·è‰²è°ƒ (cool color palette)
- æŸ”å’Œçš„è‰²å½© (muted color palette, pastel color palette, neutral color palette, earthy tones)
- é²œè‰³çš„è‰²å½© (Vibrant color palette, vibrant colors, rich)

**äººç‰©ç‰¹å¾/Characteristics:**

- å¹´é¾„èŒƒå›´:
    - åå‡ å²åˆ°20å²åˆ (late teens to early twenties, late teens or early twenties)
    - 20å¤šå² (early twenties, early to mid-20s, likely in her 20s, mid-20s, likely in her twenties)
- å¥³æ€§è§’è‰² (young female character,female subject, young adult female, female model)
- å¹´è½»äºšæ´²å¥³æ€§ (young Asian woman, young Asian female, Asian woman, Asian female)
- ç”·æ€§ä¸»ä½“ (young Asian male, male subject)



**æ’å›¾å’Œæ•°å­—è‰ºæœ¯/Illustrations and Digital Art:**

- åŠ¨æ¼«é£æ ¼æ’å›¾ (Anime-style illustration, Vibrant anime-style illustration)
- æ•°å­—æ’å›¾ (Vibrant digital illustration, Digital illustration, Vibrant digital artwork)
- æ‰‹ç»˜æ’å›¾ (Hand-drawn illustration)
- åŠ¨æ¼«é£æ ¼ (anime style)
- æ•°å­—è‰ºæœ¯ (digital art)
- ç®€æ´çš„çº¿æ¡ (clean lines)

**å…¶ä»–/Others:**

- ä¼˜é›… (Elegant, elegance, timeless elegance, casual elegance, sophisticated)
- æ¢¦å¹» (ethereal, ethereal quality, ethereal ambiance, ethereal atmosphere, ethereal aesthetic)
- å®é™ (serene, serene ambiance, serene atmosphere, tranquil ambiance, tranquil atmosphere)



**å‘å‹/Hairstyle:**

- é•¿é»‘å‘ (long dark hair, long black hair)
- çŸ­é»‘å‘ (short black hair)
- é•¿è€Œé£˜é€¸çš„å¤´å‘ (long flowing hair)

**æœè£…/Clothing:**

- ç™½è‰²è¡¬è¡« (white shirt, crisp white shirt)
- ä¼‘é—²è£…æ‰® (casual attire, casual fashion, casual style)
- ç™½è‰²è¿åŠ¨é‹ (white sneakers)
- åŠè†è¢œ (knee-high socks)
- ç™½è‰²è¢œå­ (white socks)
- æ²¡æœ‰æ˜æ˜¾çš„é…é¥° (no visible accessories)


**è¡¨æƒ…/Expression:**

- å¹³é™çš„è¡¨æƒ… (serene expression, serene facial expression)
- æ²‰æ€çš„è¡¨æƒ… (contemplative expression, thoughtful expression, pensive expression, contemplative mood)
- ä¸“æ³¨çš„è¡¨æƒ… (focused expression)
- æ¸©å’Œçš„è¡¨æƒ… (gentle expression)
- å¾®ç¬‘ (slight smile, subtle smile, gentle smile, radiant smile)
- ä¸¥è‚ƒçš„è¡¨æƒ… (solemn expression)
- å¿«ä¹çš„è¡¨æƒ… (joyful expression, playful expression)
- ä¸­æ€§è¡¨æƒ… (neutral expression)
- å†…çœçš„æƒ…ç»ª (introspective mood)

**å§¿åŠ¿å’Œæ€åº¦/Posture and Attitude:**

- ç›´è§† (direct gaze, intense gaze, piercing gaze, focused gaze, soft gaze, contemplative gaze, gentle gaze, introspective gaze)
- é—­çœ¼ (eyes closed, closed eyes)
- ä¾§é¢ (side profile)
- æ”¾æ¾çš„å§¿åŠ¿ (relaxed pose, relaxed posture, relaxed demeanor)
- å¤§æ–¹å¾—ä½“çš„å§¿æ€ (poised stance, poised demeanor, poised expression, confident stance)


**é¢éƒ¨ç‰¹å¾/Facial Features:**

- ç»†è…»çš„é¢éƒ¨ç‰¹å¾ (delicate features, delicate facial features, clear complexion)
- äºšæ´²é¢å­”ç‰¹å¾ (Asian features)
- ç™½çš™è‚Œè‚¤ (fair skin, fair complexion, pale skin, light skin, porcelain skin)
- å…‰æ»‘çš„çš®è‚¤ (smooth skin, flawless skin, clear skin)
- å¾®å¦† (subtle makeup, minimal makeup, natural makeup)
    - å¼ºè°ƒè‡ªç„¶ç¾çš„å¾®å¦† (subtle makeup emphasizing natural beauty, subtle makeup highlighting natural beauty, subtle makeup enhancing natural beauty, subtle makeup highlighting natural features)
- è„¸é¢Šä¸Šæ·¡æ·¡çš„è…®çº¢ (subtle blush on cheeks)






## SPO-Diffusion-Models/SPO-SDXL_4k-p_10ep

æ­¥éª¤æ„ŸçŸ¥åå¥½ä¼˜åŒ–ï¼šåœ¨æ¯ä¸€æ­¥ä¸­å°†åå¥½ä¸å»å™ªæ€§èƒ½ç›¸ç»“åˆ

Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step

æœ€è¿‘ï¼Œç›´æ¥åå¥½ä¼˜åŒ– (DPO) å·²å°†å…¶æˆåŠŸä»å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ‰©å±•åˆ°å¯¹é½æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½ã€‚ä¸å¤§å¤šæ•°ç°æœ‰çš„ DPO æ–¹æ³•å‡è®¾æ‰€æœ‰æ‰©æ•£æ­¥éª¤ä¸æœ€ç»ˆç”Ÿæˆçš„å›¾åƒå…±äº«ä¸€è‡´çš„åå¥½é¡ºåºä¸åŒï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§å‡è®¾å¿½ç•¥äº†ç‰¹å®šäºæ­¥éª¤çš„å»å™ªæ€§èƒ½ï¼Œå¹¶ä¸”åå¥½æ ‡ç­¾åº”è¯¥æ ¹æ®æ¯ä¸ªæ­¥éª¤çš„è´¡çŒ®è¿›è¡Œé‡èº«å®šåˆ¶ã€‚

ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ­¥è¿›æ„ŸçŸ¥åå¥½ä¼˜åŒ– (SPO)ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒæ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ­¥è¿›æ„ŸçŸ¥åå¥½æ¨¡å‹å’Œæ­¥è¿›é‡é‡‡æ ·å™¨æ¥ç¡®ä¿å‡†ç¡®çš„æ­¥è¿›æ„ŸçŸ¥ç›‘ç£ï¼Œä»è€Œç‹¬ç«‹è¯„ä¼°å’Œè°ƒæ•´æ¯ä¸€æ­¥çš„å»å™ªæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬éƒ½ä¼šä»ä¸€ç»„å›¾åƒä¸­æŠ½æ ·ï¼Œæ‰¾åˆ°åˆé€‚çš„èƒœè´Ÿå¯¹ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œä»æ± ä¸­éšæœºé€‰æ‹©ä¸€å¼ å›¾åƒæ¥åˆå§‹åŒ–ä¸‹ä¸€ä¸ªå»å™ªæ­¥éª¤ã€‚è¿™ä¸ªæ­¥è¿›é‡é‡‡æ ·è¿‡ç¨‹å¯ç¡®ä¿ä¸‹ä¸€ä¸ªèƒœè´Ÿå›¾åƒå¯¹æ¥è‡ªåŒä¸€å›¾åƒï¼Œä½¿èƒœè´Ÿæ¯”è¾ƒç‹¬ç«‹äºä¸Šä¸€æ­¥ã€‚ä¸ºäº†è¯„ä¼°æ¯ä¸€æ­¥çš„åå¥½ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå•ç‹¬çš„æ­¥è¿›æ„ŸçŸ¥åå¥½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åº”ç”¨äºå˜ˆæ‚å›¾åƒå’Œå¹²å‡€å›¾åƒã€‚

æˆ‘ä»¬å¯¹ Stable Diffusion v1.5 å’Œ SDXL çš„å®éªŒè¡¨æ˜ï¼ŒSPO åœ¨å°†ç”Ÿæˆçš„å›¾åƒä¸å¤æ‚ã€è¯¦ç»†çš„æç¤ºå¯¹é½ä»¥åŠå¢å¼ºç¾æ„Ÿæ–¹é¢æ˜æ˜¾ä¼˜äºæœ€æ–°çš„ Diffusion-DPOï¼ŒåŒæ—¶è®­ç»ƒæ•ˆç‡ä¹Ÿæé«˜äº† 20 å€ä»¥ä¸Šã€‚ä»£ç å’Œæ¨¡å‹ï¼šhttps://rockeycoss.github.io/spo.github.io/

è¯¥æ¨¡å‹æ˜¯ä»stable-diffusion-xl-base-1.0å¾®è°ƒè€Œæ¥çš„ã€‚å®ƒå·²é’ˆå¯¹ 4,000 ä¸ªæç¤ºè¿›è¡Œäº† 10 ä¸ªæ—¶æœŸçš„è®­ç»ƒã€‚

è¿™æ˜¯ä¸€ä¸ªåˆå¹¶æ£€æŸ¥ç‚¹ï¼Œå°† LoRA æ£€æŸ¥ç‚¹ä¸åŸºç¡€æ¨¡å‹stable-diffusion-xl-base-1.0ç›¸ç»“åˆã€‚å¦‚æœæ‚¨æƒ³è®¿é—® LoRA æ£€æŸ¥ç‚¹ï¼Œè¯·è®¿é—®SPO-SDXL_4k-p_10ep_LoRA ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸stable-diffusion-webuiå…¼å®¹çš„ LoRA æ£€æŸ¥ç‚¹ï¼Œå¯åœ¨æ­¤å¤„è®¿é—®ã€‚



![alt text](assets/6367/image-12.png)


# EasyAnimate

easyphotoå‡ºå“    

å¯ç”¨äºç”ŸæˆAIå›¾ç‰‡ä¸è§†é¢‘ã€è®­ç»ƒDiffusion Transformerçš„åŸºçº¿æ¨¡å‹ä¸Loraæ¨¡å‹ï¼Œæˆ‘ä»¬æ”¯æŒä»å·²ç»è®­ç»ƒå¥½çš„EasyAnimateæ¨¡å‹ç›´æ¥è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆä¸åŒåˆ†è¾¨ç‡ï¼Œ6ç§’å·¦å³ã€fps24çš„è§†é¢‘ï¼ˆ1 ~ 144å¸§, æœªæ¥ä¼šæ”¯æŒæ›´é•¿çš„è§†é¢‘ï¼‰ï¼Œä¹Ÿæ”¯æŒç”¨æˆ·è®­ç»ƒè‡ªå·±çš„åŸºçº¿æ¨¡å‹ä¸Loraæ¨¡å‹ï¼Œè¿›è¡Œä¸€å®šçš„é£æ ¼å˜æ¢ã€‚


æ–°ç‰¹æ€§ï¼š

æ›´æ–°åˆ°v2ç‰ˆæœ¬ï¼Œæœ€å¤§æ”¯æŒ144å¸§(768x768, 6s, 24fps)ç”Ÿæˆã€‚[ 2024.05.26 ]


åŠŸèƒ½æ¦‚è§ˆï¼š

    æ•°æ®é¢„å¤„ç†
    è®­ç»ƒVAE
    è®­ç»ƒDiT
    æ¨¡å‹ç”Ÿæˆ


pixart a åŸºç¡€æ¨¡å‹     
è®­ç»ƒ   


1. å¼•å…¥è¿åŠ¨æ¨¡å—ï¼ˆMotion Moduleï¼‰ï¼Œä»¥å®ç°ä»2Då›¾åƒåˆ°3Dè§†é¢‘çš„æ‰©å±•   
2. å¼•å…¥slice VAEå‹ç¼©æ—¶é—´è½´ï¼Œæœ‰åŠ©äºé•¿è§†é¢‘ç”Ÿæˆã€‚




# MV-VTON


PyTorch implementation of MV-VTON: Multi-View Virtual Try-On with Diffusion Models


https://github.com/hywang2002/MV-VTON

ğŸ”¥The first multi-view virtual try-on dataset MVG is now available.
ğŸ”¥Checkpoints on both frontal-view and multi-view virtual try-on tasks are released.

   [æäº¤äº 2024 å¹´ 4 æœˆ 26 æ—¥ï¼ˆv1ï¼‰ï¼Œæœ€åä¿®è®¢äº 2024 å¹´ 4 æœˆ 29 æ—¥ï¼ˆæ­¤ç‰ˆæœ¬ï¼Œv2ï¼‰]     
MV-VTONï¼šé‡‡ç”¨æ‰©æ•£æ¨¡å‹çš„å¤šè§†å›¾è™šæ‹Ÿè¯•ç©¿   
ç‹æµ©å®‡ã€å¼ å¿—ç¦„ã€é‚¸ä¸œæ—ã€å¼ ä¸–è‰¯ã€å·¦ç‹çŒ›    
åŸºäºå›¾åƒçš„è™šæ‹Ÿè¯•ç©¿çš„ç›®æ ‡æ˜¯ç”Ÿæˆç›®æ ‡äººç‰©è‡ªç„¶ç©¿ç€ç»™å®šæœè£…çš„å›¾åƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…å…³æ³¨ä½¿ç”¨æ­£é¢æœè£…çš„æ­£é¢è¯•ç©¿ã€‚å½“æœè£…å’Œäººç‰©çš„è§†å›¾æ˜æ˜¾ä¸ä¸€è‡´æ—¶ï¼Œç‰¹åˆ«æ˜¯å½“äººç‰©çš„è§†å›¾ä¸æ˜¯æ­£é¢æ—¶ï¼Œç»“æœå¹¶ä¸ä»¤äººæ»¡æ„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šè§†å›¾è™šæ‹Ÿè¯•ç©¿ (MV-VTON)ï¼Œæ—¨åœ¨ä½¿ç”¨ç»™å®šçš„æœè£…ä»å¤šä¸ªè§†å›¾é‡å»ºäººç‰©çš„ç©¿è¡£ç»“æœã€‚ä¸€æ–¹é¢ï¼Œç”±äºå•è§†å›¾æœè£…ä¸º MV-VTON æä¾›çš„ä¿¡æ¯ä¸è¶³ï¼Œæˆ‘ä»¬æ”¹ä¸ºä½¿ç”¨ä¸¤å¹…å›¾åƒï¼Œå³æœè£…çš„æ­£é¢å’ŒèƒŒé¢è§†å›¾ï¼Œä»¥å°½å¯èƒ½åœ°æ¶µç›–å®Œæ•´çš„è§†å›¾ã€‚å¦ä¸€æ–¹é¢ï¼Œé‡‡ç”¨è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹æ¥æ‰§è¡Œæˆ‘ä»¬çš„ MV-VTONã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†å›¾è‡ªé€‚åº”é€‰æ‹©æ–¹æ³•ï¼Œå…¶ä¸­ç¡¬é€‰æ‹©å’Œè½¯é€‰æ‹©åˆ†åˆ«åº”ç”¨äºå…¨å±€å’Œå±€éƒ¨æœè£…ç‰¹å¾æå–ã€‚è¿™ç¡®ä¿æœè£…ç‰¹å¾å¤§è‡´ç¬¦åˆäººçš„è§†è§’ã€‚éšåï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨è”åˆæ³¨æ„åŠ›æ¨¡å—æ¥å¯¹é½å’Œèåˆæœè£…ç‰¹å¾ä¸äººç‰©ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ª MV-VTON æ•°æ®é›†ï¼Œå³å¤šè§†å›¾æœè£… (MVG)ï¼Œå…¶ä¸­æ¯ä¸ªäººéƒ½æœ‰å¤šå¼ å…·æœ‰ä¸åŒè§†è§’å’Œå§¿åŠ¿çš„ç…§ç‰‡ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ä»…åœ¨ä½¿ç”¨æˆ‘ä»¬çš„ MVG æ•°æ®é›†çš„ MV-VTON ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³ç»“æœï¼Œè€Œä¸”åœ¨ä½¿ç”¨ VITON-HD å’Œ DressCode æ•°æ®é›†çš„æ­£é¢è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸Šä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨æ­¤ https URLä¸Šå…¬å¼€å‘å¸ƒã€‚

![alt text](assets/6367/image-13.png)

![alt text](assets/6367/image-14.png)

å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦

ä¼°è®¡æ˜¯å‡†å¤‡ä¸Šç ”ä¸€ å¤§å››åˆšæ¯•ä¸š ç›´æ¥ä¿    

Acknowledgements   
Our code is heavily borrowed from Paint-by-Example and DCI-VTON. We also thank previous work PF-AFN, GP-VTON, LaDI-VTON and StableVITON.

å®ä¹ 

ç†æƒ³æ±½è½¦æ˜¯ä¸­å›½æ–°èƒ½æºæ±½è½¦å¸‚åœºçš„é¢†å¯¼è€…ã€‚æœ¬å…¬å¸è®¾è®¡ã€ç ”å‘ã€åˆ¶é€ å’Œé”€å”®è±ªåæ™ºèƒ½ç”µåŠ¨è½¦ã€‚ç†æƒ³æ±½è½¦çš„ä½¿å‘½æ˜¯åˆ›é€ ç§»åŠ¨çš„å®¶ï¼Œåˆ›é€ å¹¸ç¦çš„å®¶ã€‚é€šè¿‡äº§å“ã€æŠ€æœ¯å’Œä¸šåŠ¡æ¨¡å¼çš„åˆ›æ–°ï¼Œæœ¬å…¬å¸ä¸ºå®¶åº­ç”¨æˆ·æä¾›å®‰å…¨ã€ä¾¿æ·ã€èˆ’é€‚çš„äº§å“ä¸æœåŠ¡ã€‚åœ¨ä¸­å›½ï¼Œç†æƒ³æ±½è½¦æ˜¯æˆåŠŸå°†å¢ç¨‹å¼ç”µåŠ¨è½¦å•†ä¸šåŒ–çš„å…ˆé©±ã€‚ç†æƒ³æ±½è½¦äº2019å¹´11æœˆå¼€å§‹é‡äº§ã€‚



å¯ä»¥å‡†å¤‡å»æ·˜å®äº¬ä¸œäº†    


# LCM-LoRA


æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ (LCM) é€šå¸¸åªéœ€ 2-4 ä¸ªæ­¥éª¤å³å¯ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä»è€Œå¯ä»¥åœ¨å‡ ä¹å®æ—¶çš„è®¾ç½®ä¸­ä½¿ç”¨æ‰©æ•£æ¨¡å‹ã€‚


åªéœ€ 4,000 ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆçº¦ 32 ä¸ª A100 GPU å°æ—¶ï¼‰å³å¯ä»ä»»ä½•é¢„å…ˆè®­ç»ƒçš„ç¨³å®šæ‰©æ•£ (SD) ä¸­æå– LCMï¼Œä»è€Œä»¥ 2~4 ä¸ªæ­¥éª¤ç”šè‡³ä¸€ä¸ªæ­¥éª¤ç”Ÿæˆé«˜è´¨é‡çš„ 768 x 768 åˆ†è¾¨ç‡å›¾åƒï¼Œæ˜¾è‘—åŠ å¿«æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆé€Ÿåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ LCM ä»…ç”¨ 4,000 æ¬¡è®­ç»ƒè¿­ä»£å°±æå–äº† Dreamshaper-V7 ç‰ˆæœ¬çš„ SDã€‚

ä½†æ˜¯ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½éœ€è¦å•ç‹¬è¿›è¡Œè’¸é¦æ‰èƒ½è¿›è¡Œæ½œåœ¨ä¸€è‡´æ€§è’¸é¦ã€‚LCM-LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯åªè®­ç»ƒå‡ ä¸ªé€‚é…å™¨å±‚ï¼Œåœ¨æœ¬ä¾‹ä¸­é€‚é…å™¨å°±æ˜¯ LoRAã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±ä¸å¿…è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥æ§åˆ¶å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚ç„¶åå¯ä»¥å°†ç”Ÿæˆçš„ LoRA åº”ç”¨äºæ¨¡å‹çš„ä»»ä½•å¾®è°ƒç‰ˆæœ¬ï¼Œè€Œæ— éœ€å•ç‹¬è’¸é¦å®ƒä»¬ã€‚æ­¤å¤–ï¼ŒLoRA å¯ä»¥åº”ç”¨äºå›¾åƒåˆ°å›¾åƒã€ControlNet/T2I-Adapterã€ä¿®å¤ã€AnimateDiff ç­‰ã€‚LCM-LoRA è¿˜å¯ä»¥ä¸å…¶ä»– LoRA ç»“åˆä½¿ç”¨ï¼Œä»¥æå°‘çš„æ­¥éª¤ï¼ˆ4-8ï¼‰ç”Ÿæˆé£æ ¼åŒ–å›¾åƒã€‚

LCM-LoRA é€‚ç”¨äºstable-diffusion-v1-5ã€stable-diffusion-xl-base-1.0å’ŒSSD-1Bæ¨¡å‹ã€‚æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½å¯ä»¥åœ¨æ­¤é›†åˆä¸­æ‰¾åˆ°ã€‚

    åŠ è½½ç‰¹å®šä»»åŠ¡çš„ç®¡é“å’Œæ¨¡å‹ã€‚
    å°†è°ƒåº¦ç¨‹åºè®¾ç½®ä¸ºLCMSchedulerã€‚
    ä¸ºæ¨¡å‹åŠ è½½ LCM-LoRA æƒé‡ã€‚
    å‡å°‘guidance_scaleé—´éš”[1.0, 2.0]å¹¶å°†num_inference_stepsé—´éš”è®¾ç½®ä¸º[4, 8]ã€‚
    ä½¿ç”¨å¸¸ç”¨å‚æ•°é€šè¿‡ç®¡é“æ‰§è¡Œæ¨ç†ã€‚


é¦–å…ˆï¼Œç¡®ä¿æ‚¨å·²å®‰è£…peftï¼Œä»¥è·å¾—æ›´å¥½çš„ LoRA æ”¯æŒã€‚

å·²å¤åˆ¶
pip å®‰è£…-U peft


æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°æˆ‘ä»¬è®¾ç½®äº†guidance_scale=1.0ï¼Œè¿™å°†ç¦ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼ã€‚è¿™æ˜¯å› ä¸º LCM-LoRA æ˜¯åœ¨æŒ‡å¯¼ä¸‹è®­ç»ƒçš„ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹æ‰¹å¤„ç†å¤§å°ä¸å¿…åŠ å€ã€‚è¿™å¯ä»¥ç¼©çŸ­æ¨ç†æ—¶é—´ï¼Œä½†ç¼ºç‚¹æ˜¯è´Ÿé¢æç¤ºå¯¹å»å™ªè¿‡ç¨‹æ²¡æœ‰ä»»ä½•å½±å“ã€‚

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ LCM-LoRA è¿›è¡ŒæŒ‡å¯¼ï¼Œä½†ç”±äºè®­ç»ƒçš„æ€§è´¨ï¼Œæ¨¡å‹å¯¹å€¼éå¸¸æ•æ„Ÿguidance_scaleï¼Œé«˜å€¼å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„å›¾åƒä¸­å‡ºç°ä¼ªå½±ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°æœ€ä½³å€¼åœ¨ [1.0, 2.0] èŒƒå›´å†…ã€‚

This is because the LCM-LoRA is trained with guidance, so the batch size does not have to be doubled in this case. 

ä¸é£æ ¼åŒ–çš„ LoRA ç›¸ç»“åˆ     
LCM-LoRA å¯ä»¥ä¸å…¶ä»– LoRA ç»“åˆä½¿ç”¨ï¼Œåªéœ€å‡ ä¸ªæ­¥éª¤ï¼ˆ4-8ï¼‰å³å¯ç”Ÿæˆæ ·å¼å›¾åƒã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LCM-LoRA å’Œå‰ªçº¸ LoRA


loraæ›´åƒæ˜¯ä¸€ç§è¿‡æ‹Ÿåˆçš„ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒ     
åŒ…æ‹¬åŠ é€Ÿlora     
å…¶æœ¬è´¨å°±æ˜¯ä¸‹æ¬¡è¾“å…¥çš„æç¤ºè¯å¾ˆå°‘å°±æœ‰åŸæ¥çš„é£æ ¼     
åŒ…æ‹¬å¾®è°ƒæ¨¡å‹ï¼Œç°åœ¨åŸºæœ¬ä¸Šå°†å…ˆéªŒè®¾ç½®æˆäº†ç¾ä¸½å’Œå¥½çœ‹


åŠ¨ç”»å·®å¼‚   
AnimateDiffå…è®¸æ‚¨ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸ºå›¾åƒåˆ¶ä½œåŠ¨ç”»ã€‚ä¸ºäº†è·å¾—è‰¯å¥½çš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆå¤šä¸ªå¸§ï¼ˆ16-24ï¼‰ï¼Œè€Œä½¿ç”¨æ ‡å‡† SD æ¨¡å‹æ‰§è¡Œæ­¤æ“ä½œå¯èƒ½ä¼šéå¸¸æ…¢ã€‚LCM-LoRA å¯ç”¨äºæ˜¾è‘—åŠ å¿«è¯¥è¿‡ç¨‹ï¼Œå› ä¸ºæ‚¨åªéœ€ä¸ºæ¯ä¸ªå¸§æ‰§è¡Œ 4-8 ä¸ªæ­¥éª¤ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ LCM-LoRA å’Œ AnimateDiff æ‰§è¡ŒåŠ¨ç”»ã€‚


    import torch
    from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler
    from diffusers.utils import export_to_gif

    adapter = MotionAdapter.from_pretrained("diffusers/animatediff-motion-adapter-v1-5")
    pipe = AnimateDiffPipeline.from_pretrained(
        "frankjoshua/toonyou_beta6",
        motion_adapter=adapter,
    ).to("cuda")

    # set scheduler
    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

    # load LCM-LoRA
    pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5", adapter_name="lcm")
    pipe.load_lora_weights("guoyww/animatediff-motion-lora-zoom-in", weight_name="diffusion_pytorch_model.safetensors", adapter_name="motion-lora")

    pipe.set_adapters(["lcm", "motion-lora"], adapter_weights=[0.55, 1.2])

    prompt = "best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress"
    generator = torch.manual_seed(0)
    frames = pipe(
        prompt=prompt,
        num_inference_steps=5,
        guidance_scale=1.25,
        cross_attention_kwargs={"scale": 1},
        num_frames=24,
        generator=generator
    ).frames[0]
    export_to_gif(frames, "animation.gif")

## åŸç†
æŠ½è±¡çš„     
æ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDM) åœ¨åˆæˆé«˜åˆ†è¾¨ç‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œè¿­ä»£é‡‡æ ·è¿‡ç¨‹è®¡ç®—é‡å·¨å¤§ï¼Œå¯¼è‡´ç”Ÿæˆé€Ÿåº¦ç¼“æ…¢ã€‚å—ä¸€è‡´æ€§æ¨¡å‹ (song ç­‰äºº) çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ (LCM)ï¼Œèƒ½å¤Ÿåœ¨ä»»ä½•é¢„å…ˆè®­ç»ƒçš„ LDMï¼ˆåŒ…æ‹¬ç¨³å®šæ‰©æ•£ (rombach ç­‰äºº)ï¼‰ä¸Šä»¥æœ€å°‘çš„æ­¥éª¤è¿›è¡Œå¿«é€Ÿæ¨ç†ã€‚å°†å¼•å¯¼çš„é€†æ‰©æ•£è¿‡ç¨‹è§†ä¸ºæ±‚è§£å¢å¼ºæ¦‚ç‡æµ ODE (PF-ODE)ï¼ŒLCM æ—¨åœ¨ç›´æ¥é¢„æµ‹æ­¤ç±» ODE åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è§£ï¼Œä»è€Œæ— éœ€å¤šæ¬¡è¿­ä»£å¹¶å…è®¸å¿«é€Ÿã€é«˜ä¿çœŸé‡‡æ ·ã€‚ä»é¢„å…ˆè®­ç»ƒçš„æ— åˆ†ç±»å™¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¸­æœ‰æ•ˆæç‚¼è€Œæ¥ï¼Œé«˜è´¨é‡çš„ 768 x 768 2~4 æ­¥ LCM ä»…éœ€ 32 ä¸ª A100 GPU å°æ—¶è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨ä¸€è‡´æ€§å¾®è°ƒ (LCF)ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºåœ¨å®šåˆ¶å›¾åƒæ•°æ®é›†ä¸Šå¾®è°ƒ LCMã€‚åœ¨ LAION-5B-Aesthetics æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLCM ä»…éœ€å‡ æ­¥æ¨ç†å³å¯å®ç°æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://latent-consistency-models.github.io/










# sd3 canny



# ç»“å°¾
# sd3éƒ¨ç½²
å†™æ³•ä¸€èˆ¬ç›´æ¥é‡‡ç”¨hug spaceä¸Šçš„     
æˆ–è€…å…³æ³¨ä¸€ä¸ªäººï¼Œç»å¸¸å†™jupyterå®ç°comfyuiå’Œwebuiçš„é‡‡æ ·ï¼Œæˆ–è€…ç›´æ¥ç”¨ä»–çš„ï¼Œä»–ä¸€èˆ¬éƒ½ä¼šå»å®ç°        

éƒ¨ç½²ä½ç½®    
teams     
/newlytest/stable-diffusion-3-medium/app.py

diffusers from_pretrained     
é»˜è®¤ cache_dir none     
PosixPath('/root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3-medium-diffusers/snapshots/b1148b4028b9ec56ebd36444c193d56aeff7ab56')

![alt text](assets/6367/image-3.png)    
![alt text](assets/6367/image-4.png)

éƒ½æ˜¯è½¯è¿æ¥    

repo = "stabilityai/stable-diffusion-3-medium-diffusers"
pipe = StableDiffusion3Pipeline.from_pretrained(repo, torch_dtype=torch.float16, token='').to(device)


åŠŸèƒ½æœ‰ï¼šç›‘æ§æ—¶é—´å’Œè¾“å…¥ä¿¡æ¯    
ç®€å•æ”¹å†™scheduler    


gradio

æ¨¡å‹ä¸‹è½½å®Œåè¿è¡Œï¼Œtokenizerä¸åŒ¹é…     
å¾ˆå¤šåŒ…è¦æœ€æ–°çš„      
æ‰‹åŠ¨è£…ï¼Œæœ€årequeståŒ…è¿˜æ˜¯ä¸åŒ¹é…     


huggingface tokenä¸‹è½½      

20g 1024*1024

ç›´æ¥ä½¿ç”¨ huggingface spaceçš„ gradioéƒ¨ç½²


Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.

dpmpp 2m ç›´æ¥é…ç½®è¿˜æ˜¯é»‘å›¾     
å‡ºä¸äº†æ­£å¸¸å›¾    
diffusersé‡‡æ ·å™¨ä¸çŸ¥é“å¦‚ä½•æ”¹


ä¸ºä»€ä¹ˆiclightèƒ½ä½¿ç”¨ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ


æ™®é€šç”Ÿå›¾æ‰‹æŒ‡ç•¸å½¢


a green sign that says "Very Deep learning" and is at the edge of the Grand Canyon

![alt text](assets/6367/image.png)


A portrait photo of a kangaroo wearing an orange hoodie
and blue sunglasses standing on the grass in front of the Sydney
Opera House holding a sign on the chest that says "WUJIE"!

![alt text](assets/6367/image-1.png)


A portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses standing on the grass in front of the Sydney Opera House holding a sign on the chest that says "é™†"!

![alt text](assets/6367/image-2.png)

å¦‚æœæ˜¯comfyuiä¹Ÿè®¸ä¼šå¥½ä¸€äº›    

## webui sd3åˆ†æ”¯æµ‹è¯•

å¯åŠ¨æ—¶å€™

a1111webui193/stable-diffusion-webui/modules/sd_disable_initialization.py", line 68, in CLIPTextModel_from_pretrained
res = self.CLIPTextModel_from_pretrained(None, *model_args, config=pretrained_model_name_or_path, state_dict={}, **kwargs)
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3051, in from_pretrained
resolved_config_file = cached_file(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/transformers/utils/hub.py", line 422, in cached_file
raise EnvironmentError(
OSError: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with huggingface-cli login or by passing token=<your_token>

Failed to create model quickly; will retry using slow method

è¿™ä¸ªä¸çŸ¥é“æœ‰æ²¡æœ‰å½±å“     

åŠ è½½æ¨¡å‹æ—¶å€™

Creating model from config: /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/configs/sd3-inference.yaml
mmdit initializing with: input_size=None, patch_size=2, in_channels=16, depth=24, mlp_ratio=4.0, learn_sigma=False, adm_in_channels=2048, context_embedder_config={'target': 'torch.nn.Linear', 'params': {'in_features': 4096, 'out_features': 1536}}, register_length=0, attn_mode='torch', rmsnorm=False, scale_mod_only=False, swiglu=False, out_channels=None, pos_embed_scaling_factor=None, pos_embed_offset=None, pos_embed_max_size=192, num_patches=36864, qk_norm=None, qkv_bias=True, dtype=torch.float16, device='cpu'
[2024-06-17 06:46:03,468][DEBUG][filelock] - Attempting to acquire lock 140004667105552 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
[2024-06-17 06:46:03,469][DEBUG][filelock] - Lock 140004667105552 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
tokenizer_config.json: 100%|â–ˆ| 1.86k/1.86k [00:00<00:00, 6.9
[2024-06-17 06:46:03,763][DEBUG][filelock] - Attempting to release lock 140004667105552 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
[2024-06-17 06:46:03,763][DEBUG][filelock] - Lock 140004667105552 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/b114c318caf72f6e89ea92e0755c41327a453198.lock
[2024-06-17 06:46:04,056][DEBUG][filelock] - Attempting to acquire lock 140004667097536 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
[2024-06-17 06:46:04,056][DEBUG][filelock] - Lock 140004667097536 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792k/792k [00:01<00:00, 700kB/s]
[2024-06-17 06:46:05,478][DEBUG][filelock] - Attempting to release lock 140004667097536 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
[2024-06-17 06:46:05,478][DEBUG][filelock] - Lock 140004667097536 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/4e28ff6ebdf584f5372d9de68867399142435d9a.lock
[2024-06-17 06:46:06,385][DEBUG][filelock] - Attempting to acquire lock 140004667108096 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
[2024-06-17 06:46:06,385][DEBUG][filelock] - Lock 140004667108096 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
special_tokens_map.json: 100%|â–ˆ| 1.79k/1.79k [00:00<00:00, 7
[2024-06-17 06:46:06,667][DEBUG][filelock] - Attempting to release lock 140004667108096 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
[2024-06-17 06:46:06,668][DEBUG][filelock] - Lock 140004667108096 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/881bdbffc06e471924ecea57f962bc5f8e2a9f21.lock
[2024-06-17 06:46:06,945][DEBUG][filelock] - Attempting to acquire lock 140004667103248 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
[2024-06-17 06:46:06,946][DEBUG][filelock] - Lock 140004667103248 acquired on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 593/593 [00:00<00:00, 2.38MB/s]
[2024-06-17 06:46:07,233][DEBUG][filelock] - Attempting to release lock 140004667103248 on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
[2024-06-17 06:46:07,233][DEBUG][filelock] - Lock 140004667103248 released on /root/.cache/huggingface/hub/.locks/models--google--t5-v1_1-xxl/5a439fe1a8c4a05c6b1e4bf6a11821b497bf0136.lock
Downloading: "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/clip_g.safetensors" to /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/models/CLIP/clip_g.safetensors

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29G/1.29G [00:59<00:00, 23.4MB/s]
Downloading: "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/clip_l.safetensors" to /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/models/CLIP/clip_l.safetensors

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235M/235M [00:12<00:00, 20.3MB/s


å¥½åƒæ²¡æœ‰ä¸‹è½½ t5 xxl     





Downloading VAEApprox model to: /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/models/VAE-approx/vaeapprox-sd3.pt
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228k/228k [00:00<00:00, 1.21MB/s]
*** Error running process_before_every_sampling: /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/extensions/sd-webui-ic-light/scripts/ic_light_script.py
    Traceback (most recent call last):
      File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/scripts.py", line 840, in process_before_every_sampling
        script.process_before_every_sampling(p, *script_args, **kwargs)
      File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/extensions/sd-webui-ic-light/scripts/ic_light_script.py", line 387, in process_before_every_sampling
        assert self.backend_type == BackendType.Forge
    AssertionError


### è¾“å…¥promptè®¡æ•°æŠ¥é”™
Traceback (most recent call last):
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/routes.py", line 488, in run_predict
output = await app.get_blocks().process_api(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/blocks.py", line 1431, in process_api
result = await self.call_function(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/blocks.py", line 1103, in call_function
prediction = await anyio.to_thread.run_sync(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/anyio/to_thread.py", line 33, in run_sync
return await get_asynclib().run_sync_in_worker_thread(
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
return await future
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py", line 807, in run
result = context.run(func, *args)
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/gradio/utils.py", line 707, in wrapper
response = f(*args, **kwargs)
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/call_queue.py", line 14, in f
res = func(*args, **kwargs)
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/ui.py", line 185, in update_token_counter
token_count, max_length = max([model_hijack.get_prompt_lengths(prompt) for prompt in prompts], key=lambda args: args[0])
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/ui.py", line 185, in
token_count, max_length = max([model_hijack.get_prompt_lengths(prompt) for prompt in prompts], key=lambda args: args[0])
File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/modules/sd_hijack.py", line 328, in get_prompt_lengths
_, token_count = self.clip.process_texts([text])
File "/root/miniconda3/envs/webui310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1695, in getattr
raise AttributeError(f"'{type(self).name}' object has no attribute '{name}'")
AttributeError: 'SD3Cond' object has no attribute 'process_texts'




### ddimæŠ¥é”™
 modules.devices.NansException: A tensor with NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the "Upcast cross attention layer to float32" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.

aysä¹Ÿæœ‰è¿™ä¸ªé”™

ç›®å‰åªèƒ½ç”¨eular


DPM++ 2M, Schedule type: Karras,

Sampler: Euler a, Schedule type: Automatic

è¿™äº›éƒ½æ— æ³•ä½¿ç”¨

DPM++ 2M sdeæ›´å·®

Do not use the ancestral sampler. You can use Euler, not Euler ancestral.


### a1111å®ç°

change CLIP links to allow anonymous downloading

modules/models/sd3/sd3_model.py

    CLIPG_URL = "https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/text_encoders/clip_g.safetensors"
    CLIPG_URL = "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/clip_g.safetensors"
    CLIPG_CONFIG = {
        "hidden_act": "gelu",
        "hidden_size": 1280,
    @@ -20,7 +20,7 @@
        "num_hidden_layers": 32,
    }

    CLIPL_URL = "https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/text_encoders/clip_l.safetensors"
    CLIPL_URL = "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/clip_l.safetensors"
    CLIPL_CONFIG = {
        "hidden_act": "quick_gelu",
        "hidden_size": 768,
    @@ -29,7 +29,7 @@
        "num_hidden_layers": 12,
    }

    T5_URL = "https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/text_encoders/t5xxl_fp16.safetensors"
    T5_URL = "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/t5xxl_fp16.safetensors"
    T5_CONFIG = {
        "d_ff": 10240,
        "d_model": 4096,
        "num_heads": 64,
        "num_layers": 24,
        "vocab_size": 32128,
    }


    class SD3Cond(torch.nn.Module):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.tokenizer = SD3Tokenizer()
            with torch.no_grad():
                self.clip_g = SDXLClipG(CLIPG_CONFIG, device="cpu", dtype=devices.dtype)
                self.clip_l = SDClipModel(layer="hidden", layer_idx=-2, device="cpu", dtype=devices.dtype, layer_norm_hidden_state=False, return_projected_pooled=False, textmodel_json_config=CLIPL_CONFIG)
                self.t5xxl = T5XXLModel(T5_CONFIG, device="cpu", dtype=devices.dtype)
            self.weights_loaded = False


ä¸»è¦æ˜¯å› ä¸ºè¿™äº›condéƒ½åŠ è½½è¿›äº†cpu    

    def forward(self, prompts: list[str]):
        res = []
        for prompt in prompts:
            tokens = self.tokenizer.tokenize_with_weights(prompt)
            l_out, l_pooled = self.clip_l.encode_token_weights(tokens["l"])
            g_out, g_pooled = self.clip_g.encode_token_weights(tokens["g"])
            t5_out, t5_pooled = self.t5xxl.encode_token_weights(tokens["t5xxl"])
            lg_out = torch.cat([l_out, g_out], dim=-1)
            lg_out = torch.nn.functional.pad(lg_out, (0, 4096 - lg_out.shape[-1]))
            lgt_out = torch.cat([lg_out, t5_out], dim=-2)
            vector_out = torch.cat((l_pooled, g_pooled), dim=-1)
            res.append({
                'crossattn': lgt_out[0].to(devices.device),
                'vector': vector_out[0].to(devices.device),
            })
        return res



    def load_weights(self):
        if self.weights_loaded:
            return
        clip_path = os.path.join(shared.models_path, "CLIP")
        clip_g_file = modelloader.load_file_from_url(CLIPG_URL, model_dir=clip_path, file_name="clip_g.safetensors")
        with safetensors.safe_open(clip_g_file, framework="pt") as file:
            self.clip_g.transformer.load_state_dict(SafetensorsMapping(file))
        clip_l_file = modelloader.load_file_from_url(CLIPL_URL, model_dir=clip_path, file_name="clip_l.safetensors")
        with safetensors.safe_open(clip_l_file, framework="pt") as file:
            self.clip_l.transformer.load_state_dict(SafetensorsMapping(file), strict=False)
        t5_file = modelloader.load_file_from_url(T5_URL, model_dir=clip_path, file_name="t5xxl_fp16.safetensors")
        with safetensors.safe_open(t5_file, framework="pt") as file:
            self.t5xxl.transformer.load_state_dict(SafetensorsMapping(file), strict=False)
        self.weights_loaded = True

add an option (on by default) to disable T5

    T5_URL = "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors"
    T5_URL = "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/t5xxl_fp16.safetensors"


    
    if shared.opts.sd3_enable_t5:
        self.t5xxl = T5XXLModel(T5_CONFIG, device="cpu", dtype=devices.dtype)
    else:
        self.t5xxl = None


    if self.t5xxl and shared.opts.sd3_enable_t5:
        t5_out, t5_pooled = self.t5xxl.encode_token_weights(tokens["t5xxl"])
    else:
        t5_out = torch.zeros(l_out.shape[0:2] + (4096,), dtype=l_out.dtype, device=l_out.device)


modules/shared_options.py



    options_templates.update(options_section(('sd3', "Stable Diffusion 3", "sd"), {
        "sd3_enable_t5": OptionInfo(False, "Enable T5").info("load T5 text encoder; increases VRAM use by a lot, potentially improving quality of generation; requires model reload to apply"),
    }))


### T5 optioné€‰æ‹©æµ‹è¯•
å¥½åƒå¹¶æ²¡æœ‰é€‰      

![alt text](assets/6367/image-18.png)


ç«Ÿç„¶æ˜¯å®šä¹‰åœ¨è¿™ä¸ªä½ç½®çš„       

Downloading: "https://huggingface.co/AUTOMATIC/stable-diffusion-3-medium-text-encoders/resolve/main/t5xxl_fp16.safetensors" to /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/a1111webui193/stable-diffusion-webui/models/CLIP/t5xxl_fp16.safetensors

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.12G/9.12G [05:19<00:00, 30.6MB/s]
Applying attention optimization: Doggettx... done.

å¼€å¯















## sd3-ref

https://github.com/mcmonkey4eva/sd3-ref/

https://github.com/mcmonkey4eva/sd3-ref/blob/master/mmdit.py

è¿™ä¸ªå±äºæ¯”è¾ƒåŸç”Ÿçš„æ¨ç†ä»£ç      
å¾ˆå¤šå¹³å°éƒ½æœ‰ï¼Œä»Šå¤©617    
ä½†æ˜¯è¿™ä¸ªäº‹3æœˆå‰å‘å¸ƒ

SD3 çš„ä»…æ¨ç†å¾®å‹å‚è€ƒå®ç°ã€‚

åŒ…å«æ–‡æœ¬ç¼–ç å™¨ï¼ˆOpenAI CLIP-L/14ã€OpenCLIP bigGã€Google T5-XXLï¼‰çš„ä»£ç ï¼ˆè¿™äº›æ¨¡å‹éƒ½æ˜¯å…¬å¼€çš„ï¼‰ã€VAE è§£ç å™¨ï¼ˆç±»ä¼¼äºä¹‹å‰çš„ SD æ¨¡å‹ï¼Œä½†æœ‰ 16 é€šé“ä¸”æ²¡æœ‰åé‡åŒ–è½¬æ¢æ­¥éª¤ï¼‰å’Œæ ¸å¿ƒ MM-DiTï¼ˆå…¨æ–°ï¼‰ã€‚


æ¨ç† SD3 æ‰€éœ€çš„ä¸€åˆ‡ï¼ˆæƒé‡æ–‡ä»¶é™¤å¤–ï¼‰ã€‚

æ³¨æ„ï¼šæ­¤ repo æ˜¯ä¸€ä¸ªæ—©æœŸå‚è€ƒåº“ï¼Œæ—¨åœ¨ååŠ©åˆä½œä¼™ä¼´ç»„ç»‡å®æ–½ SD3ã€‚å¯¹äºæ­£å¸¸æ¨ç†ï¼Œè¯·ä½¿ç”¨Comfyæˆ–åŸºäºå®ƒçš„ UIï¼Œä¾‹å¦‚Swarmã€‚


æ–‡ä»¶æŒ‡å—    

    sd3_infer.py- å…¥å£ç‚¹ï¼ŒæŸ¥çœ‹æ­¤å†…å®¹ä»¥äº†è§£æ‰©æ•£æ¨¡å‹å’Œä¸‰é‡ tenc cat çš„åŸºæœ¬ç”¨æ³•
    sd3_impls.py- åŒ…å« MMDiT å’Œ VAE çš„åŒ…è£…å™¨
    other_impls.py- åŒ…å« CLIP æ¨¡å‹ã€T5 æ¨¡å‹å’Œä¸€äº›å®ç”¨ç¨‹åº
    mmdit.py- åŒ…å« MMDiT æœ¬èº«çš„æ ¸å¿ƒ
    modelsåŒ…å«ä»¥ä¸‹æ–‡ä»¶çš„ æ–‡ä»¶å¤¹ï¼ˆå•ç‹¬ä¸‹è½½ï¼‰ï¼š
    clip_g.safetensorsï¼ˆopenclip bigGï¼Œä¸ SDXL ç›¸åŒï¼Œå¯è·å–å…¬å¼€å‰¯æœ¬ï¼‰
    clip_l.safetensorsï¼ˆOpenAI CLIP-Lï¼Œä¸ SDXL ç›¸åŒï¼Œå¯ä»¥è·å–å…¬å…±å‰¯æœ¬ï¼‰
    t5xxl.safetensorsï¼ˆè°·æ­Œ T5-v1.1-XXLï¼Œå¯ä»¥è·å–å…¬å¼€å‰¯æœ¬ï¼‰
    sd3_medium.safetensorsï¼ˆæˆ–ä»»ä½•ä¸» MMDiT æ¨¡å‹æ–‡ä»¶ï¼‰


ä»£ç æ¥æº   
æ­¤å¤„åŒ…å«çš„ä»£ç æºè‡ªï¼š

    Stability AI å†…éƒ¨ç ”ç©¶ä»£ç åº“ï¼ˆMM-DiTï¼‰
    å…¬å…±ç¨³å®šæ€§ AI å­˜å‚¨åº“ï¼ˆä¾‹å¦‚ VAEï¼‰
    Alex Goodwin ä¸º Stability AI ç¼–å†™çš„æ­¤å‚è€ƒ repo çš„ä¸€äº›ç‹¬ç‰¹ä»£ç 
    æ¥è‡ª SD3 çš„ ComfyUI å†…éƒ¨ç¨³å®šæ€§å®ç°çš„ä¸€äº›ä»£ç ï¼ˆç”¨äºä¸€äº›ä»£ç æ›´æ­£å’Œå¤„ç†ç¨‹åºï¼‰
    HuggingFace å’Œä¸Šæ¸¸æä¾›å•†ï¼ˆé’ˆå¯¹ CLIP/T5 ä»£ç éƒ¨åˆ†ï¼‰


ç«Ÿç„¶ä¸æ˜¯åŸºäºdiffusersæˆ–å…¶ä»–k diffusion    
çº¯è‡ªå·±å†™    

ä½†æ˜¯å®ç°è¿‡ç¨‹å‚è€ƒäº†å…¶ä»–åœ°æ–¹çš„


mmdit ä½†è¿™æ ·ç›´æ¥çœ‹æ²¡ä»€ä¹ˆæ„ä¹‰

    def forward_core_with_concat(self, x: torch.Tensor, c_mod: torch.Tensor, context: Optional[torch.Tensor] = None) -> torch.Tensor:
        if self.register_length > 0:
            context = torch.cat((repeat(self.register, "1 ... -> b ...", b=x.shape[0]), context if context is not None else torch.Tensor([]).type_as(x)), 1)

        # context is B, L', D
        # x is B, L, D
        for block in self.joint_blocks:
            context, x = block(context, x, c=c_mod)

        x = self.final_layer(x, c_mod)  # (N, T, patch_size ** 2 * out_channels)
        return x

    def forward(self, x: torch.Tensor, t: torch.Tensor, y: Optional[torch.Tensor] = None, context: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass of DiT.
        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)
        t: (N,) tensor of diffusion timesteps
        y: (N,) tensor of class labels
        """
        hw = x.shape[-2:]
        x = self.x_embedder(x) + self.cropped_pos_embed(hw)
        c = self.t_embedder(t, dtype=x.dtype)  # (N, D)
        if y is not None:
            y = self.y_embedder(y)  # (N, D)
            c = c + y  # (N, D)

        context = self.context_embedder(context)

        x = self.forward_core_with_concat(x, c, context)

        x = self.unpatchify(x, hw=hw)  # (N, out_channels, H, W)
        return x








### ä½œè€… 
æˆ‘æ˜¯FreneticLLCçš„é¦–å¸­æ‰§è¡Œå®˜ã€ Stability.AIçš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä»¥åŠDenizenScriptçš„é¡¹ç›®è´Ÿè´£äººã€‚

å®˜æ–¹å‘å¸ƒ

æˆ‘ä»å°å°±é€šè¿‡æ¸¸æˆæ¨¡ç»„å­¦ä¹ ç¼–ç¨‹ï¼Œä»é‚£ä»¥åæˆ‘å°±å†ä¹Ÿæ²¡æœ‰åœæ­¢è¿‡ã€‚ç°åœ¨æˆ‘çš„ç»éªŒç›¸å½“ä¸°å¯Œã€‚


æ‹¥æœ‰åå¤šå¹´ç»éªŒçš„è½¯ä»¶å¼€å‘äººå‘˜ï¼Œä¸»è¦ä½¿ç”¨ C# å’Œ Javaã€‚

æˆ‘å¼€å‘è¿‡çš„ä¸€äº›ç¼–ç¨‹ç›¸å…³å·¥å…·    

    IDEs: Visual Studio (C#, C, C++), Code::Blocks (C, C++), VS Code (variety of langs), NP++, IntelliJ IDEA (Java), Eclipse (past Java), NetBeans (past Java)
    ç¡¬ä»¶æ¥å£ï¼šx86ã€x86_64ã€OpenGLã€OpenALã€Nvidia CUDAã€Win32
    æ•°æ®è¯­æ³•ï¼šiniã€tomlã€quake cfgã€JSONã€HTMLã€XMLã€YAMLã€FDSï¼ˆæˆ‘è‡ªå·±åˆ›å»ºçš„ï¼Œå“ˆå“ˆï¼‰
    æ•°æ®åº“ï¼šSQL æœåŠ¡å™¨ã€SQLiteã€MongoDBã€Redisã€LiteDBï¼Œä»¥åŠä¸€äº›ä¸“æœ‰å…¬å¸äº§å“ï¼ˆotelã€AWS S3ã€datadogã€bigquery ......ï¼‰
    é€šä¿¡ï¼šç”µå­é‚®ä»¶ï¼ˆè‡ªåŠ¨åŒ–ã€è´¦æˆ·ç¡®è®¤å·¥å…·ç­‰ï¼‰ã€IRCï¼ˆæœºå™¨äººï¼‰ã€Discordï¼ˆæœºå™¨äººï¼‰ã€GitHubï¼ˆAPIï¼‰ã€xenforoï¼ˆä¿®æ”¹/æ‰©å±•ï¼‰ã€phpbbï¼ˆä¿®æ”¹/æ‰©å±•ï¼‰
    é«˜çº§å¼€å‘å·¥å…·ï¼šJava åç¼–è¯‘å™¨ã€Java å­—èŠ‚ç æŸ¥çœ‹å™¨ã€ILSpyã€HexEditã€YourKit
    æ“ä½œç³»ç»Ÿï¼šWindowsï¼ˆXP/7/8/8.1/10/11ã€Home å’Œ Proï¼‰ã€Linuxï¼ˆDebian æœåŠ¡å™¨/æ¡Œé¢ã€Raspbian æ¡Œé¢ã€Ubuntu æœåŠ¡å™¨/æ¡Œé¢ï¼ˆåŠè¡ç”Ÿäº§å“ï¼‰ã€CentOSï¼‰
    CLI å·¥å…·ï¼šWindows Batchã€Powershellã€é€šè¿‡ Linux ç»ˆç«¯çš„ Bashã€é€šè¿‡ SSH çš„ Bashã€gitï¼ˆæ˜¾ç„¶å“ˆå“ˆï¼‰ã€SQL/Mongo/python/ç­‰çš„å®æ—¶ shellã€‚
    æˆ‘ä¿®æ”¹è¿‡çš„æ¸¸æˆï¼šã€Šå¸å›½æˆ˜äº‰ã€‹ï¼ˆCã€XMLï¼‰ã€ã€Šç»åœ°æ­¦å£« 2ã€‹ï¼ˆCã€C++ï¼‰[1]ã€ã€Šç»åœ°å­¦é™¢ã€‹ï¼ˆCã€C++ï¼‰ã€ã€Šæˆ‘çš„ä¸–ç•Œã€‹ï¼ˆJavaï¼‰[1]ã€ã€ŠGarry's Modã€‹ï¼ˆLuaï¼‰ã€ã€Šæ­¦è£…çªè¢­ 3ã€‹ï¼ˆå†…éƒ¨ Bohemia Scriptï¼‰ï¼Œä»¥åŠå„ç§æˆ‘æ›¾å°è¯•ä¿®æ”¹ä½†ä»æœªå‘å¸ƒè¿‡å¤ªå¤šå†…å®¹çš„æ¸¸æˆ


æˆ‘çš„ç¼–ç¨‹è¯­è¨€ç»éªŒï¼ˆä»¥åŠé€‚ç”¨çš„å…¬å…±é¡¹ç›®é“¾æ¥ï¼‰ï¼š

    é‡å¤§/é•¿æœŸç»éªŒï¼šC# [1] [2] [3] [4] [5] [6]ï¼ŒJava [1] [2] [3]
    ç»éªŒä¸°å¯Œï¼šWebï¼ˆHTML/CSS/JavaScriptï¼‰[1] [2] [3] [4]ã€TypeScript [1]ã€Python [1] [2] [3]ã€GLSL [1]
    æ›¾ä½¿ç”¨è¿‡ä»¥ä¸‹ç¼–ç¨‹è¯­è¨€ï¼šx86_64 Assemblyã€C [1]ã€C++ã€Rust [1]ã€Luaã€Bashã€Batchã€Bohemia Scriptã€.NET CIL [1]ã€Java å­—èŠ‚ç 
    æ¶‰çŒè¿‡ï¼šSQLã€PHPã€Powershellã€HLSLã€å„ç§é¢†åŸŸç‰¹å®šè¯­è¨€
    æˆ‘è‡ªå·±åˆ›å»ºçš„ï¼šDenizenScript [1] [2]ï¼ŒFreneticScript [1]


æˆ‘æœ‰ç»éªŒçš„ä¸»é¢˜ï¼š

    Webdevï¼ˆç”¨äºå„ç§é¡¹ç›®çš„å¤šä¸ªç«™ç‚¹å’Œç«™ç‚¹ç”Ÿæˆå™¨ï¼‰
    æ¸¸æˆå¼€å‘ï¼ˆVoxaliaï¼‰
    æ¸¸æˆå¼•æ“å¼€å‘ (FreneticGameEngine)
    ä½çº§ç³»ç»Ÿï¼ˆæ„å»ºæ±‡ç¼–å’Œ C-API å·¥å…·ä»¥åŠæ“ä½œç³»ç»Ÿä¿®æ”¹çš„å®éªŒï¼‰
    ä½çº§ CPU/GPU ä»£ç ä¼˜åŒ–
    ç®—æ³•ä¼˜åŒ–
    è„šæœ¬è¯­è¨€å¼€å‘ï¼ˆDenizenScript/FreneticScriptï¼‰
    å¾®æœåŠ¡ï¼ˆæˆ‘çš„ä»“åº“åˆ—è¡¨ä¸­å…¨æ˜¯å¾®æœåŠ¡ï¼‰
    AI/ML æŠ€æœ¯ï¼ˆç¨³å®šæ‰©æ•£å’Œ LLM å·¥å…·ï¼ŒåŒ…æ‹¬æ¨ç†å’Œè®­ç»ƒå·¥ä½œï¼Œä»¥åŠä¸ºå…¶å¼€å‘ UI/UX å·¥å…·ï¼ŒåŒ…æ‹¬åœ¨Stability.AIçš„ä¸“ä¸šå·¥ä½œï¼‰ï¼ˆâ€œAI è‰ºæœ¯â€ä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œâ€œAI å›¾åƒç”Ÿæˆâ€æ˜¯ä¸€ç§å·¥å…·ï¼Œäººç±»å¯ä»¥ç”¨å®ƒæ¥åˆ›ä½œè‰ºæœ¯ï¼Œä½†é‚£æ˜¯ä»¥ AI ä¸ºå·¥å…·çš„äººç±»è‰ºæœ¯ï¼‰
    è½¯ä»¶å®‰å…¨ï¼ˆæ›¾å‚ä¸è¿‡æ•°æ®åŠ å¯†ã€è´¦æˆ·éšç§ä¿æŠ¤ç­‰é¡¹ç›®ï¼‰ï¼ˆæˆ‘è¿˜æ·±å…¥ç ”ç©¶ä¸å®‰å…¨é—®é¢˜ç›¸å…³çš„æ–°é—» - æ–°çš„æ¼æ´ã€æœªæ¥çš„å¨èƒï¼ˆå¦‚é‡å­ï¼‰ã€ä¿æŠ¤ç”¨æˆ·çš„æ–°æ–¹æ³•ç­‰ï¼‰ï¼ˆè¿˜æ›¾åº”é¡¹ç›®ä½œè€…çš„è¦æ±‚æ‹…ä»»å…¶ä»–é¡¹ç›®çš„çº¢é˜Ÿæˆå‘˜ï¼‰
    ç”¨æˆ·ç•Œé¢ï¼ˆUI/GUIï¼‰ï¼ˆä½†æˆ‘ä¸æ˜¯å¹³é¢è®¾è®¡å¸ˆï¼‰
    ç”¨æˆ·ä½“éªŒ (UX)ï¼ˆæˆ‘å¯¹å¥½çš„ç”¨æˆ·ä½“éªŒæœ‰éå¸¸å¼ºçƒˆçš„çœ‹æ³•ã€‚è½¯ä»¶å·¥å…·åº”è¯¥æœ€å¤§é™åº¦åœ°æé«˜ç”¨æˆ·è‡ªç”±åº¦ï¼ŒåŒæ—¶å°½é‡å‡å°‘æ··ä¹±ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”ç›®å‰è®¸å¤šæµè¡Œçš„è½¯ä»¶é¡¹ç›®åšå¾—æ›´å¥½ï¼‰
    æ•°æ®åº“ï¼ˆæˆ‘æœ€å–œæ¬¢çš„æ˜¯ Mongo å’Œ LiteDBï¼Œæˆ‘å¯ä»¥å¤„ç† SQLï¼Œä½†æˆ‘è®¤ä¸ºå®ƒæœ‰ç‚¹è¿‡åº¦äº†ï¼‰
    é«˜çº§å¼‚æ­¥ç¼–ç¨‹å’Œå¤šçº¿ç¨‹ï¼ˆä¸–ç•Œä¸Šå¤§å¤šæ•°å¼‚æ­¥ä»£ç éƒ½æ˜¯ç¼ºå°‘è®¿é—®é”çš„å®šæ—¶ç‚¸å¼¹ã€‚æˆ‘è¯•å›¾æ¯”å¤§å¤šæ•°äººæ›´å¥½åœ°é¿å…è¿™ç§æƒ…å†µï¼‰
    æ•°æ®å¤„ç†å’Œæµå¼ä¼ è¾“ï¼ŒåŒ…æ‹¬ä½çº§ï¼ˆä¾‹å¦‚ tarfileã€ç½‘ç»œæ•°æ®æµç­‰ï¼‰å’Œé«˜çº§ï¼ˆæ•°æ®åº“ã€æ•°æ®ç®¡ç†æ¥å£ç­‰ï¼‰

ç¨å¾®ç¦»è°±

æ¸¸æˆminecraftä¹Ÿæœ‰ç‚¹ç¦»è°±     
è¿˜åšai    

åŸæ¥è®¡ç®—æœº åšæ¸¸æˆ éƒ½è¿™ä¹ˆç¦»è°±


Frenetic LLC is hiring!


Positions:

C# Software Developer â€¢ Internship â€¢ Unpaid â€¢ College Credit â€¢ Remote

Open positions (up to 4 at a time) for college students in Software Development or Game Develop programs.
Gain work experience building real C# software in the exciting world of the Video Game industry.
Please note this position is compensated only in College Credits and details are subject to your individual college program.



ç–¯ç‹‚çš„æ¸¸æˆå¼•æ“
ä¸€ä¸ªå¼ºå¤§çš„åŸºäº C# çš„ 3D å’Œ 2D æ¸¸æˆå¼•æ“ã€‚

è¿™æ˜¯ä»€ä¹ˆ
FGE æ˜¯ä¸€ä¸ªæ¸¸æˆå¼•æ“ã€‚å®ƒæ˜¯æ¸¸æˆèƒŒåçš„å¼ºå¤§å¼•æ“ã€‚å®ƒæ”¶é›†å®šä¹‰æ¸¸æˆçš„æ‰€æœ‰æ•°æ®ï¼ˆæ¸¸æˆæœºåˆ¶ã€è‰ºæœ¯ä½œå“ç­‰ï¼‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºåŠŸèƒ½æ€§çš„ä¸œè¥¿ï¼å®ƒé€šè¿‡æä¾›å¼ºå¤§çš„æ¸²æŸ“ç³»ç»Ÿã€æ¸¸æˆå¯èƒ½éœ€è¦çš„æ— æ•°ä¾¿æ·å®ç”¨ç¨‹åºä»¥åŠä¸é«˜è´¨é‡ç‰©ç†å¼•æ“çš„ç›´æ¥é“¾æ¥æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

è¿™ä¸æ˜¯ä»€ä¹ˆ
FGE ä¸æ˜¯æ¸¸æˆã€‚æ‚¨æ— æ³•å¼€ç®±å³ç©ã€‚å®ƒæ²¡æœ‰å¤ªå¤šåŠŸèƒ½ï¼Œåªæ˜¯ç©ºè½¬ã€‚è¦è®© FGE åšä»»ä½•äº‹æƒ…ï¼Œæ‚¨ï¼ˆæˆ–ä»»ä½•å¼€å‘äººå‘˜ï¼‰å¿…é¡»é¦–å…ˆåˆ›å»ºæ¸¸æˆå†…å®¹ã€‚è¿™æ„å‘³ç€ä»è‰ºæœ¯ä½œå“åˆ°æ¸¸æˆæœºåˆ¶çš„æ‰€æœ‰å†…å®¹éƒ½å¿…é¡»ç”±æ‚¨æˆ–æ¸¸æˆå¼€å‘äººå‘˜æ·»åŠ ã€‚







# MixDQ


## è®ºæ–‡ä¿¡æ¯

https://arxiv.org/abs/2405.17873

[Submitted on 28 May 2024 (v1), last revised 30 May 2024 (this version, v2)]

ä»£ç å¼€æºæ—¶é—´6æœˆåˆ


èµµå¤©è¾°æ˜¯æ¸…åå¤§å­¦ç”µå­å·¥ç¨‹ç³»NICS-EFC å®éªŒå®¤çš„åšå£«ç”Ÿï¼ŒæŒ‡å¯¼è€å¸ˆæ˜¯ç‹å®‡æ•™æˆã€‚ä»–è¿˜ä¸å®é›ªé£åšå£«å¯†åˆ‡åˆä½œã€‚ä»–åˆ†åˆ«äº 2020 å¹´å’Œ 2023 å¹´åœ¨åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ç”µå­å·¥ç¨‹ç³»è·å¾—å­¦å£«å’Œç¡•å£«å­¦ä½ã€‚ä»–çš„ä¸»è¦ç ”ç©¶é‡ç‚¹æ˜¯é«˜æ•ˆæ·±åº¦å­¦ä¹ å’Œè½¯ç¡¬ä»¶ååŒä¼˜åŒ–ã€‚

æ¶ˆæ¯
[2024-03] æˆ‘ä»¬çš„è®ºæ–‡FlashEvalè¢«CVPR24æ¥å—
[2023-12] ä¸€ç¯‡åˆä½œè®ºæ–‡äºDATE24è¢«æ¥å—
[2023-09] åœ¨TechBeatä¸Šå‘è¡¨å…³äºæˆ‘ä»¬å…³äºé«˜æ•ˆ 3D æ„ŸçŸ¥çš„å·¥ä½œçš„æ¼”è®²ã€‚
[2023-09] æ­£å¼æˆä¸ºNICS-EFCå®éªŒå®¤çš„ä¸€å‘˜ï¼Œå¼€å§‹æˆ‘çš„åšå£«è¯¾ç¨‹ã€‚
[2023-07] æˆ‘ä»¬çš„è®ºæ–‡Ada3Dè¢«ICCV23æ¥å—ï¼Œè¯·æŸ¥çœ‹é¡¹ç›®é¡µé¢ã€‚
[2023-05] åŠ å…¥Infinigenceæ‹…ä»»ç ”ç©¶å®ä¹ ç”Ÿã€‚

æ•™è‚²
ç”µæ°”å·¥ç¨‹åšå£«ï¼Œ2023 å¹´

æ¸…åå¤§å­¦

ç”µæ°”å·¥ç¨‹ç¡•å£«ï¼Œ2020 å¹´

åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦

ç”µæ°”å·¥ç¨‹å­¦å£«ï¼Œ2016 å¹´

åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦

https://cloud.infini-ai.com/platform/ai


é«˜æ•ˆã€ä½æˆæœ¬çš„æ¨¡å‹è½åœ°

æ— é—®èŠ¯ç©¹


MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization

æˆ‘ä»¬è®¾è®¡äº† MixDQï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆç²¾åº¦é‡åŒ–æ¡†æ¶ï¼ŒæˆåŠŸè§£å†³äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å‡ æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é‡åŒ–é—®é¢˜ã€‚åœ¨è§†è§‰è´¨é‡ä¸‹é™å’Œå†…å®¹å˜åŒ–å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡çš„æƒ…å†µä¸‹ï¼ŒMixDQ å¯ä»¥å®ç° W4A8ï¼ŒåŒæ—¶å†…å­˜å‹ç¼©ç‡ç›¸å½“äº 3.4 å€ï¼Œå»¶è¿ŸåŠ é€Ÿç‡ç›¸å½“äº 1.5 å€ã€‚

https://github.com/A-suozhang/MixDQ


MixDQ æ˜¯ä¸€ç§æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œå¯åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å‹ç¼©æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å†…å­˜å’Œè®¡ç®—ä½¿ç”¨é‡ã€‚å®ƒæ”¯æŒå°‘æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚ SDXL-turboã€LCM-loraï¼‰ï¼Œä»¥æ„å»ºå¿«é€Ÿå’Œå¾®å°æ‰©æ•£æ¨¡å‹ã€‚æä¾›é«˜æ•ˆçš„ CUDA å†…æ ¸å®ç°ï¼Œä»¥èŠ‚çœå®é™…èµ„æºã€‚


![alt text](assets/6367/image-5.png)


åœ¨å‡ ä¹ä¸å½±å“è§†è§‰è´¨é‡ä¸‹é™å’Œå†…å®¹å˜åŒ–çš„æƒ…å†µä¸‹ï¼ŒMixDQ å¯ä»¥å®ç° W4A8ï¼ŒåŒæ—¶å†…å­˜å‹ç¼©ç‡ç›¸å½“äº3.4 å€ï¼Œå»¶è¿ŸåŠ é€Ÿç‡ç›¸å½“äº 1.5 å€ã€‚


æ‰©æ•£æ¨¡å‹å·²ç»å®ç°äº†æ˜¾è‘—çš„è§†è§‰ç”Ÿæˆè´¨é‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ç»™å®ƒä»¬åœ¨èµ„æºå—é™çš„ç§»åŠ¨è®¾å¤‡ç”šè‡³æ¡Œé¢ GPU ä¸Šçš„åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ€è¿‘çš„å‡ æ­¥æ‰©æ•£æ¨¡å‹é€šè¿‡å‡å°‘å»å™ªæ­¥éª¤æ¥ç¼©çŸ­æ¨ç†æ—¶é—´ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å†…å­˜æ¶ˆè€—ä»ç„¶è¿‡å¤§ã€‚

è®­ç»ƒåé‡åŒ– (PTQ) ç”¨ä½ä½æ•´æ•°å€¼ (INT4/8) ä»£æ›¿é«˜ä½å®½ FP è¡¨ç¤ºï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„é™ä½å†…å­˜æˆæœ¬çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºå°‘æ­¥æ‰©æ•£æ¨¡å‹æ—¶ï¼Œç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨ä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ··åˆç²¾åº¦é‡åŒ–æ¡†æ¶ - MixDQã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸“é—¨çš„ BOS æ„ŸçŸ¥é‡åŒ–æ–¹æ³•ï¼Œç”¨äºé«˜åº¦æ•æ„Ÿçš„æ–‡æœ¬åµŒå…¥é‡åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œåº¦é‡è§£è€¦çµæ•åº¦åˆ†ææ¥æµ‹é‡æ¯ä¸€å±‚çš„çµæ•åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ•´æ•°è§„åˆ’çš„æ–¹æ³•æ¥è¿›è¡Œä½å®½åˆ†é…ã€‚

å°½ç®¡ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨ W8A8 ä¸Šè¾¾ä¸åˆ°è¦æ±‚ï¼Œä½† MixDQ å¯ä»¥åœ¨ä¸æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹å®ç° W8A8ï¼Œåœ¨å‡ ä¹ä¸å½±å“è§†è§‰æ•ˆæœçš„æƒ…å†µä¸‹å®ç° W4A8ã€‚ä¸ FP16 ç›¸æ¯”ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å¤§å°å’Œå†…å­˜æˆæœ¬é™ä½äº†3-4 å€ï¼Œå¹¶å°†å»¶è¿ŸåŠ é€Ÿäº†1.45 å€ã€‚


æˆ‘ä»¬é€šè¿‡å®éªŒå‘ç°ï¼Œä¸å¤šæ­¥æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œå°‘æ­¥æ‰©æ•£æ¨¡å‹å¯¹é‡åŒ–æ›´æ•æ„Ÿï¼Œè€Œç°æœ‰çš„æ‰©æ•£é‡åŒ–æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚Q-æ‰©æ•£ W8A8 é‡åŒ–æ¨¡å‹åœ¨å°‘æ­¥ä¸‹é¢ä¸´ä¸¥é‡çš„è´¨é‡ä¸‹é™ã€‚æ­¤å¤–ï¼Œå³ä½¿æ˜¯å¤šæ­¥æ¨¡å‹ï¼Œé‡åŒ–ä¹Ÿä¼šæŸå®³æ–‡æœ¬-å›¾åƒå¯¹é½ã€‚


![alt text](assets/6367/image-6.png)


æˆ‘ä»¬è¿›è¡Œäº†åˆæ­¥å®éªŒï¼Œæ·±å…¥æ¢è®¨äº†é‡åŒ–å¤±è´¥çš„åŸå› ï¼Œå¹¶å‘ç°äº†ä¸¤ä¸ªæœ‰å¯å‘æ€§çš„å‘ç°ï¼šï¼ˆ1ï¼‰é‡åŒ–è¢«ä¸€äº›é«˜åº¦æ•æ„Ÿçš„å±‚â€œç“¶é¢ˆåŒ–â€ã€‚ ï¼ˆ2ï¼‰é‡åŒ–æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†åˆ†åˆ«å½±å“ç”Ÿæˆçš„å›¾åƒè´¨é‡å’Œå†…å®¹ã€‚


![alt text](assets/6367/image-7.png)


æ··åˆç²¾åº¦é‡åŒ–æ¡†æ¶ MixDQ ï¼š

![alt text](assets/6367/image-8.png)

BOS æ„ŸçŸ¥æ–‡æœ¬åµŒå…¥é‡åŒ–    
æˆ‘ä»¬å‘ç° CLIP æ–‡æœ¬åµŒå…¥çš„ç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯é˜»ç¢é‡åŒ–çš„å¼‚å¸¸å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯å¥é¦– (BOS) æ ‡è®°ï¼Œå¯¹äºä¸åŒçš„æç¤ºï¼Œå®ƒä¿æŒä¸å˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç¦»çº¿é¢„å…ˆè®¡ç®—å®ƒå¹¶è·³è¿‡å®ƒçš„é‡åŒ–ã€‚


åº¦é‡è§£è€¦çµæ•åº¦åˆ†æ    
å½“ä»…ä¿ç•™å¯¼è‡´æœ€å¤§é‡åŒ–è¯¯å·®FP16çš„å±‚æ—¶ï¼Œæˆ‘ä»¬å‘ç°ç”Ÿæˆçš„å›¾åƒä»ç„¶é¢ä¸´è´¨é‡ä¸‹é™çš„é—®é¢˜ï¼Œè¿™è¡¨æ˜ç°æœ‰çš„é‡åŒ–çµæ•åº¦åˆ†æçš„å‡†ç¡®æ€§éœ€è¦æé«˜ã€‚å—é‡åŒ–å¯¹å›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½çš„å½±å“çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åº¦é‡è§£è€¦çµæ•åº¦åˆ†ææ–¹æ³•ã€‚æˆ‘ä»¬å°†å„å±‚åˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«å¯¹å®ƒä»¬è¿›è¡Œå…·æœ‰ä¸åŒåº¦é‡çš„çµæ•åº¦åˆ†æã€‚



åŸºäºæ•´æ•°è§„åˆ’çš„ä½å®½åˆ†é…    
åœ¨è·å¾—é‡åŒ–çµæ•åº¦ä¹‹åï¼Œæˆ‘ä»¬å°†ä½å®½åˆ†é…é—®é¢˜è½¬åŒ–ä¸ºæ•´æ•°è§„åˆ’æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨ç°æˆçš„æ±‚è§£å™¨æœ‰æ•ˆåœ°æ±‚è§£ã€‚


![alt text](assets/6367/image-9.png)

æˆ‘ä»¬ç»™å‡ºäº†ä¸€äº›å®šæ€§ç»“æœï¼Œå°†ç»Ÿè®¡åº¦é‡å€¼ä¸ç”Ÿæˆçš„å›¾åƒè”ç³»èµ·æ¥ã€‚å¯ä»¥çœ‹å‡ºï¼Œä¸ Q-Diffusion å’Œæœ´ç´  minmax é‡åŒ–ç›¸æ¯”ï¼ŒMixDQ-W4A8 å¯ä»¥ç”Ÿæˆä¸ FP å›¾åƒå‡ ä¹ç›¸åŒçš„å›¾åƒï¼Œè€Œå…¶ä»–æ–¹æ³•æ— æ³•ä¸º W8A8 ç”Ÿæˆå¯è¯»å›¾åƒã€‚


ä¸å…¶ä»–ç°æœ‰æ‰©æ•£æ¨¡å‹é‡åŒ–å·¥å…·ç›¸æ¯”ï¼Œåªæœ‰é—­å¼ TensorRT INT8 å®ç°å®ç°äº†å®é™…çš„å»¶è¿ŸåŠ é€Ÿã€‚MixDQ æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å°‘æ­¥æ‰©æ•£æ¨¡å‹å®ç°å®é™…å†…å­˜å’Œå»¶è¿Ÿä¼˜åŒ–çš„å·¥å…·ï¼Œå¯å®ç°â€œå¾®å°è€Œå¿«é€Ÿâ€çš„å›¾åƒç”Ÿæˆã€‚


![alt text](assets/6367/image-10.png)


![alt text](assets/6367/image-11.png)


è‡´è°¢    
æˆ‘ä»¬çš„ä»£ç æ˜¯åŸºäºQ-Diffusionå’ŒDiffusers Librarayå¼€å‘çš„ã€‚


å¾…åŠäº‹é¡¹    
è¯„ä¼°è„šæœ¬ï¼ˆFIDã€ClipScoreã€ImageRewardï¼‰   
é«˜æ•ˆçš„ INT8 GPU å†…æ ¸å®ç°  



æ™®é€š fp16 æ²¡æœ‰ 

AttributeError: 'StableDiffusionXLPipeline' object has no attribute 'set_cuda_graph'


AttributeError: 'StableDiffusionXLPipeline' object has no attribute 'run_for_test'


## åŸç†

    def quantize_unet(
            self,
            w_bit=None,
            a_bit=None,
            bos=False,
            # cuda_graph_only=True,
            # run_pipeline=True,
            # compile=False,
        ):
        r"""
        This function helps quantize the UNet in the SDXL Pipeline
        Now we only support quantization with the setting W8A8

        Args:
            w_bit: (`str`):
                the bit width of weight
            a_bit: (`str`):
                the bit width of activation
            bos: (`bool`):
                if to use bos technique
            cuda_graph_only: (`bool`):
                if to use cuda_graph
            run_pipeline: (`bool`):
                run the full pipeline or just the unet
        """

path = hf_hub_download(
            repo_id="Stein-Fun/mixdq_test",
            filename="quant_para_wsym_fp16.pt",
            revision="version_0",
        )













## diffusersä½¿ç”¨å®‰è£…ç¯å¢ƒ
    3  pip install mixdq-extension
    4  pip uninstall torchaudio
    5  pip install xformers==0.0.25 å¯¹æ ‡ torch 2.2.1

AttributeError: 'StableDiffusionXLPipeline' object has no attribute 'quantize_unet'

AttributeError: type object 'DiffusionPipeline' has no attribute 'from_single_file'


    class MixDQ_SDXLTurbo_Pipeline_W8A8(
        DiffusionPipeline,
        FromSingleFileMixin,
        StableDiffusionXLLoraLoaderMixin,
        TextualInversionLoaderMixin,
        IPAdapterMixin,
    ):
        r"""
        Pipeline for text-to-image generation using Stable Diffusion XL.

        This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
        library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)

        In addition the pipeline inherits the following loading methods:
            - *LoRA*: [`loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`]
            - *Ckpt*: [`loaders.FromSingleFileMixin.from_single_file`]

å…ˆè½¬diffusersä½¿ç”¨DiffusionPipelineåŠ è½½sdxl


    IndexError                                Traceback (most recent call last)
    Cell In[48], line 2
        1 # quant the UNet
    ----> 2 pipe.quantize_unet(
        3                 w_bit = 8, 
        4                 a_bit = 8, 
        5                 bos=True, 
        6                 )

    File ~/.cache/huggingface/modules/diffusers_modules/local/pipeline.py:2766, in MixDQ_SDXLTurbo_Pipeline_W8A8.quantize_unet(self, w_bit, a_bit, bos)
    2756 ckpt = torch.load(path, map_location='cpu')
    2758 register_qconfig_from_input_files(
    2759     self.unet,
    2760     # args,
    (...)
    2764     bos_dict=bos_dict
    2765 )
    -> 2766 convert_to_quantized(self.unet, ckpt)

    File ~/.cache/huggingface/modules/diffusers_modules/local/pipeline.py:1924, in convert_to_quantized(unet, ckpt)
    1922 def convert_to_quantized(unet, ckpt):
    1923     # from quantize import convert
    -> 1924     convert(unet,
    1925             mapping={nn.Linear: QuantizedLinear,
    ...
    --> 791         _split = _SPLIT[_NUM]
        792         _NUM = _NUM + 1
        793         # num = num + 1

    IndexError: list index out of range

32ç²¾åº¦çš„é—®é¢˜     
è¿˜æ˜¯sdxlä¸æ”¯æŒï¼Ÿï¼Ÿï¼Ÿ       

IndexError: list index out of range

fp16ä¸è¡Œ

RuntimeError: Not all keys in weight yaml map to a module in UNet.

sd1.5ä¸è¡Œ

åŸä»£ç ä½¿ç”¨

python scr
ipts/txt2img.py \ --config ./configs/stable-diffusion/$config_name \ --base_path $BASE_PATH --batch_size 2 --num_imgs 8  --prompt  "a vanilla and chocolate mixing icecream cone, ice background" \ --fp16

usage: txt2img.py [-h] [--prompt [PROMPT]]
                  [--base_path [BASE_PATH]]
                  [--batch_size BATCH_SIZE] [--cfg CFG]
                  [--config CONFIG]
                  [--image_folder IMAGE_FOLDER]
                  [--num_imgs NUM_IMGS] [--seed SEED]
                  [--fp16]
txt2img.py: error: unrecognized arguments:  --config ./configs/stable-diffusion/sdxl.yaml  --base_path ./logs/debug_fp  --fp16

python scripts/txt2img.py  --config ./configs/stable-diffusion/$config_name  --base_path $BASE_PATH --batch_size 2 --num_imgs 8  --prompt  "a vanilla and chocolate mixing icecream cone, ice background"  --fp16

å¯ä»¥äº†



python scripts/txt2img.py  --config ./configs/stable-diffusion/$config_name  --base_path $BASE_PATH --batch_size 1 --num_imgs 2  --prompt  "(8k, RAW photo,masterpiece
),(realistic, photo-realistic:1.37),ID photo,jk,teen
agers,woman,solo,looking at viewer,simple background
,brown eyes,necktie,upper body"  --fp16


å…¬å¸è‡ªå·±æ¨¡å‹     

python scripts/gen_calib_data.py --config ./configs/stable-diffusion/$config_name.yaml --save_image_path ./debug_imgs


FileNotFoundError: [Errno 2] No such file or directory: './scripts/utils/captions_val2014.json'



Traceback (most recent call last):
  newlytest/MixDQ/scripts/gen_calib_data.py", line 131, in <module>
    main()
  newlytest/MixDQ/scripts/gen_calib_data.py", line 51, in main
    prompt_list, image_path = prepare_coco_text_and_image(json_file=json_file)
  newlytest/MixDQ/quant_utils/qdiff/utils.py", line 587, in prepare_coco_text_and_image
    info = json.load(open(json_file, 'r'))
FileNotFoundError: [Errno 2] No such file or directory: './scripts/utils/captions_val2014.json'

ä¼˜å…ˆçº§å¤§äºhidiffusion     

ä¸ªäººä¼˜å…ˆçº§ï¼Ÿ   
patcherå®ç°çš„ç»†è‡´è®¤çŸ¥    
æ™®é€šä»£ç é˜…è¯»    
ä¸­æœŸ   

é˜…è¯»æ— å…³æ™®é€šä»£ç  å¥½åƒå·²ç»æ²¡æœ‰å¿…è¦      
ç¡®å®æ²¡å¿…è¦   
éƒ½æ˜¯java k8sç›¸å…³ 


## å…ˆéªŒçŸ¥è¯†

https://pytorch.org/docs/stable/quantization.html

é‡åŒ–ç®€ä»‹    
é‡åŒ–æ˜¯æŒ‡ä»¥æ¯”æµ®ç‚¹ç²¾åº¦æ›´ä½çš„ä½å®½æ‰§è¡Œè®¡ç®—å’Œå­˜å‚¨å¼ é‡çš„æŠ€æœ¯ã€‚é‡åŒ–æ¨¡å‹å¯¹å¼ é‡æ‰§è¡Œéƒ¨åˆ†æˆ–å…¨éƒ¨æ“ä½œæ—¶ç²¾åº¦ä¼šé™ä½ï¼Œè€Œä¸æ˜¯å…¨ç²¾åº¦ï¼ˆæµ®ç‚¹ï¼‰å€¼ã€‚è¿™å…è®¸æ›´ç´§å‡‘çš„æ¨¡å‹è¡¨ç¤ºå’Œåœ¨è®¸å¤šç¡¬ä»¶å¹³å°ä¸Šä½¿ç”¨é«˜æ€§èƒ½çŸ¢é‡åŒ–æ“ä½œã€‚ä¸å…¸å‹çš„ FP32 æ¨¡å‹ç›¸æ¯”ï¼ŒPyTorch æ”¯æŒ INT8 é‡åŒ–ï¼Œå¯å°†æ¨¡å‹å¤§å°å‡å°‘ 4 å€ï¼Œå¹¶å°†å†…å­˜å¸¦å®½è¦æ±‚å‡å°‘ 4 å€ã€‚ä¸ FP32 è®¡ç®—ç›¸æ¯”ï¼Œç¡¬ä»¶å¯¹ INT8 è®¡ç®—çš„æ”¯æŒé€šå¸¸å¿« 2 åˆ° 4 å€ã€‚é‡åŒ–ä¸»è¦æ˜¯ä¸€ç§åŠ é€Ÿæ¨ç†çš„æŠ€æœ¯ï¼Œé‡åŒ–è¿ç®—ç¬¦ä»…æ”¯æŒå‰å‘ä¼ é€’ã€‚

PyTorch æ”¯æŒå¤šç§æ–¹æ³•æ¥é‡åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨ FP32 ä¸­è®­ç»ƒï¼Œç„¶åå°†æ¨¡å‹è½¬æ¢ä¸º INT8ã€‚æ­¤å¤–ï¼ŒPyTorch è¿˜æ”¯æŒé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œå®ƒä½¿ç”¨å‡é‡åŒ–æ¨¡å—å¯¹å‰å‘å’Œåå‘ä¼ é€’ä¸­çš„é‡åŒ–è¯¯å·®è¿›è¡Œå»ºæ¨¡ã€‚è¯·æ³¨æ„ï¼Œæ•´ä¸ªè®¡ç®—éƒ½æ˜¯ä»¥æµ®ç‚¹è¿›è¡Œçš„ã€‚åœ¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒç»“æŸæ—¶ï¼ŒPyTorch æä¾›è½¬æ¢å‡½æ•°å°†è®­ç»ƒåçš„æ¨¡å‹è½¬æ¢ä¸ºè¾ƒä½çš„ç²¾åº¦ã€‚

åœ¨è¾ƒä½çº§åˆ«ï¼ŒPyTorch æä¾›äº†ä¸€ç§è¡¨ç¤ºé‡åŒ–å¼ é‡å¹¶ä½¿ç”¨å®ƒä»¬æ‰§è¡Œæ“ä½œçš„æ–¹æ³•ã€‚å®ƒä»¬å¯ç”¨äºç›´æ¥æ„å»ºä»¥è¾ƒä½ç²¾åº¦æ‰§è¡Œå…¨éƒ¨æˆ–éƒ¨åˆ†è®¡ç®—çš„æ¨¡å‹ã€‚æä¾›äº†æ›´é«˜çº§åˆ«çš„ APIï¼Œå…¶ä¸­åŒ…å«å°† FP32 æ¨¡å‹è½¬æ¢ä¸ºè¾ƒä½ç²¾åº¦çš„å…¸å‹å·¥ä½œæµç¨‹ï¼ŒåŒæ—¶å°†ç²¾åº¦æŸå¤±é™è‡³æœ€ä½ã€‚

æ„Ÿè°¢æ‚¨å¯¹æˆ‘ä»¬å·¥ä½œçš„å…³æ³¨ï¼æˆ‘ä»¬è¿˜æ²¡æœ‰å°è¯•è¿‡ ONNXRuntimeï¼Œä½†æˆ‘ä»¬è®¤ä¸ºå®ƒæ˜¯é€‚ç”¨çš„ã€‚MixDQ é‡‡ç”¨æ ‡å‡†ä¸”æ˜“äºéƒ¨ç½²çš„é‡åŒ–æ–¹æ¡ˆï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨pytorch_quantizationéƒ¨ç½²å·¥å…·æµ‹è¯•äº† MixDQã€‚



æ”¯æŒä¸‰ç§ç±»å‹çš„é‡åŒ–ï¼š

åŠ¨æ€é‡åŒ–ï¼ˆä½¿ç”¨æµ®ç‚¹è¯»å–/å­˜å‚¨çš„æ¿€æ´»é‡åŒ–çš„æƒé‡å¹¶è¿›è¡Œè®¡ç®—é‡åŒ–ï¼‰

é™æ€é‡åŒ–ï¼ˆæƒé‡é‡åŒ–ã€æ¿€æ´»é‡åŒ–ã€è®­ç»ƒåéœ€è¦æ ¡å‡†ï¼‰

é™æ€é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆè®­ç»ƒæœŸé—´é‡åŒ–çš„æƒé‡ã€é‡åŒ–çš„æ¿€æ´»ã€é‡åŒ–æ•°å€¼å»ºæ¨¡ï¼‰

There are three types of quantization supported:

dynamic quantization (weights quantized with activations read/stored in floating point and quantized for compute)

static quantization (weights quantized, activations quantized, calibration required post training)

static quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training)

è®­ç»ƒåé™æ€é‡åŒ–
è®­ç»ƒåé™æ€é‡åŒ– (PTQ static) é‡åŒ–æ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»ã€‚å®ƒä¼šå°½å¯èƒ½å°†æ¿€æ´»èåˆåˆ°å‰é¢çš„å±‚ä¸­ã€‚å®ƒéœ€è¦ä½¿ç”¨ä»£è¡¨æ€§æ•°æ®é›†è¿›è¡Œæ ¡å‡†ï¼Œä»¥ç¡®å®šæ¿€æ´»çš„æœ€ä½³é‡åŒ–å‚æ•°ã€‚è®­ç»ƒåé™æ€é‡åŒ–é€šå¸¸ç”¨äºå†…å­˜å¸¦å®½å’Œè®¡ç®—èŠ‚çœéƒ½å¾ˆé‡è¦çš„æƒ…å†µï¼ŒCNN å°±æ˜¯ä¸€ä¸ªå…¸å‹ç”¨ä¾‹ã€‚

åœ¨åº”ç”¨è®­ç»ƒåé™æ€é‡åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¿®æ”¹æ¨¡å‹ã€‚è¯·å‚é˜…Eager Mode é™æ€é‡åŒ–çš„æ¨¡å‹å‡†å¤‡ã€‚

é™æ€é‡åŒ–çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ      
é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT) å¯åœ¨è®­ç»ƒæœŸé—´æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœï¼Œä¸å…¶ä»–é‡åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œå¯å®ç°æ›´é«˜çš„å‡†ç¡®åº¦ã€‚æˆ‘ä»¬å¯ä»¥å¯¹é™æ€ã€åŠ¨æ€æˆ–ä»…æƒé‡é‡åŒ–è¿›è¡Œ QATã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œæ‰€æœ‰è®¡ç®—å‡ä»¥æµ®ç‚¹æ•°å®Œæˆï¼Œå…¶ä¸­ fake_quant æ¨¡å—é€šè¿‡é™åˆ¶å’Œèˆå…¥æ¥æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœï¼Œä»¥æ¨¡æ‹Ÿ INT8 çš„æ•ˆæœã€‚åœ¨æ¨¡å‹è½¬æ¢åï¼Œæƒé‡å’Œæ¿€æ´»ä¼šè¢«é‡åŒ–ï¼Œå¹¶ä¸”æ¿€æ´»ä¼šå°½å¯èƒ½èåˆåˆ°å‰ä¸€å±‚ã€‚å®ƒé€šå¸¸ä¸ CNN ä¸€èµ·ä½¿ç”¨ï¼Œä¸é™æ€é‡åŒ–ç›¸æ¯”ï¼Œå¯å®ç°æ›´é«˜çš„å‡†ç¡®åº¦ã€‚

åœ¨åº”ç”¨è®­ç»ƒåé™æ€é‡åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¿®æ”¹æ¨¡å‹ã€‚è¯·å‚é˜…Eager Mode é™æ€é‡åŒ–çš„æ¨¡å‹å‡†å¤‡ã€‚


![alt text](assets/6367/image-19.png)


ç»å¸¸é—®çš„é—®é¢˜
å¦‚ä½•åœ¨ GPU ä¸Šè¿›è¡Œé‡åŒ–æ¨ç†ï¼Ÿï¼š

æˆ‘ä»¬å°šæœªæä¾›å®˜æ–¹çš„ GPU æ”¯æŒï¼Œä½†è¿™æ˜¯ä¸€ä¸ªç§¯æå¼€å‘çš„é¢†åŸŸï¼Œæ‚¨å¯ä»¥ åœ¨æ­¤å¤„æ‰¾åˆ°æ›´å¤šä¿¡æ¯

æˆ‘å¯ä»¥åœ¨å“ªé‡Œè·å¾—å¯¹æˆ‘çš„é‡åŒ–æ¨¡å‹çš„ ONNX æ”¯æŒï¼Ÿ

å¦‚æœåœ¨å¯¼å‡ºæ¨¡å‹æ—¶å‡ºç°é”™è¯¯ï¼ˆä½¿ç”¨ ä¸‹çš„ API torch.onnxï¼‰ï¼Œæ‚¨å¯ä»¥åœ¨ PyTorch å­˜å‚¨åº“ä¸­æ‰“å¼€ä¸€ä¸ªé—®é¢˜ã€‚åœ¨é—®é¢˜æ ‡é¢˜å‰åŠ ä¸Š ï¼Œ[ONNX]å¹¶å°†é—®é¢˜æ ‡è®°ä¸ºã€‚module: onnx

å¦‚æœæ‚¨é‡åˆ° ONNX Runtime çš„é—®é¢˜ï¼Œè¯·åœ¨GitHub - microsoft/onnxruntimeä¸Šæ‰“å¼€ä¸€ä¸ªé—®é¢˜ã€‚


ç®—æ³•çº§é‡åŒ–æ¨¡æ‹Ÿä»£ç ï¼ˆåœ¨githubä¸­ï¼‰æ”¯æŒæ··åˆç²¾åº¦ï¼ˆåŒ…æ‹¬W4A8ï¼‰ï¼Œç³»ç»Ÿçº§é‡åŒ–ä»£ç ï¼ˆåœ¨huggingfaceä¸­ï¼ŒåŒ…æ‹¬cudaå†…æ ¸ï¼‰ç›®å‰ä»…æ”¯æŒW8A8ï¼Œæˆ‘ä»¬ä»åœ¨è‡´åŠ›äºæ··åˆç²¾åº¦CUDAå†…æ ¸çš„å®ç°ã€‚



## ç¯å¢ƒä½ç½®

å››æœº

myconda



## é‡åŒ–è‡ªæœ‰xlæ¨¡å‹

ä¸¥é‡é—®é¢˜æ˜¯é‡åŒ–åå¥½åƒä¸èƒ½gpuæ¨ç†        

ç›®å‰å·²æœ‰çš„w8a8 diffuserså¯ä»¥gpuæ¨ç†     

å†å¾€ä¸‹å¥½åƒä¸è¡Œäº†     

With negligible visual quality degradation and content change, MixDQ could achieve W4A8, with equivalent 3.4x memory compression and 1.5x latency speedup.

 Open-Source Huggingface Pipeline ğŸ¤—: We implement efficient INT8 GPU kernel to achieve actual GPU acceleration (1.45x) and memory savings (2x) for W8A8



The quantization process consists of 3 steps: (1) generating the calibration data. (2) conduct PTQ process. (3) conduct quantized model inference. We also provide the scripts for each of the 3 processes


 (main_calib_data.sh,main_ptq.sh,main_quant_infer.sh)

1.1 Generate Calibration Data    
 generating the calibration data      
main_calib_data   

1.2 Post Training Quantization (PTQ) Process     
conduct PTQ process
main_ptq    

1.3 Inference Quantized Model   
conduct quantized model inference      
main_quant_infer    
1.3.1 Normal   
1.3.2 Mixed Precision     
The "act protect" represents layers that are preserved as FP16. (It's also worth noting that the mixed_precision_scripts/quant_inference_mp.py are used for mixed precision search, for infering the mixed precision quant model, use scripts/quant_txt2img.py)






Data Preparation    
The stable diffusion checkpoints are automatically downloaded with the diffusers pipeline, we also provide manual download scripts in ./scripts/utils/download_huggingface_model.py. For text-to-image generation on COCO annotations, we provide the captions_val2014.json with Google Drive, please put it in the ./scripts/utils.

3. Mixed Precision Search     
Please download the util_files from Google Drive, and unzip it in the repository root directory. Please refer to the ./mixed_precision_scripts/mixed_precision_search.md for detailed process of the mixed precision search process.














## æ–°å¼€æº ViDiT-Q

6.24å¼€æºä»£ç 

https://github.com/A-suozhang/ViDiT-Q

https://arxiv.org/abs/2406.02540

[Submitted on 4 Jun 2024]

æˆ‘ä»¬å¼•å…¥äº† ViDiT-Qï¼Œä¸€ç§ä¸“é—¨ç”¨äºæ‰©æ•£å˜å‹å™¨çš„é‡åŒ–æ–¹æ³•ã€‚å¯¹äºç”¨äºè§†é¢‘å’Œå›¾åƒç”Ÿæˆä»»åŠ¡çš„æµè¡Œå¤§å‹æ¨¡å‹ï¼ˆä¾‹å¦‚ open-soraã€Latteã€Pixart-Î±ã€Pixart-Î£ï¼‰ï¼ŒViDiT-Q å¯ä»¥å®ç° W8A8 é‡åŒ–è€Œä¸ä¼šå¯¼è‡´åº¦é‡ä¸‹é™ï¼Œå¹¶ä¸”å¯ä»¥å®ç° W4A8 é‡åŒ–è€Œä¸ä¼šå¯¼è‡´è§†è§‰è´¨é‡æ˜æ˜¾ä¸‹é™ã€‚


Acknowledgments      
Our code was developed based on opensora v1.0(Apache License), PixArt-alpha(AGPL-3.0 license), PixArt-sigama(AGPL-3.0 license) and q-diffusion(MIT License)









# HelloWorld 7.0 æ›´æ–° - 2024 å¹´ 6 æœˆ 13 æ—¥




HelloWorld 7.0æ˜¯è¿­ä»£ä¼˜åŒ–çš„ç‰ˆæœ¬ï¼Œæ‹¥æœ‰å…¨ç³»åˆ—æœ€ä¼˜çš„æœ¬ä½“è¡¨ç°ï¼Œæ¦‚å¿µèŒƒå›´ä¸ç»†èŠ‚ä¸°å¯Œåº¦è¿›ä¸€æ­¥å¢å¼ºã€‚

æ›´æ–°è¯¦ç»†ä¿¡æ¯ï¼š

é€šè¿‡æ·»åŠ è´Ÿé¢è®­ç»ƒå›¾ç‰‡ã€åŠ å¼ºå§¿åŠ¿è®­ç»ƒã€ä¼˜åŒ–clipæ¨¡å‹ï¼Œæ¨¡å‹è‚¢ä½“å’Œæ‰‹éƒ¨å‡†ç¡®ç‡è¾ƒä¹‹å‰ç‰ˆæœ¬æœ‰æ‰€æå‡ï¼Œæ¨èçš„è´Ÿé¢æç¤ºè¯ä¸ºï¼šâ€œbad handã€bad anatomyã€worst qualityã€ai generated imagesã€low qualityã€average qualityâ€ã€‚

ä»å®˜æ–¹çš„ SPO æ¨¡å‹ä¸­æå–äº†ç»è¿‡å¾®è°ƒçš„ LoRA ï¼Œå¹¶å°†å…¶çº³å…¥ HelloWorld 7.0ã€‚SPO æ˜¯å¯¹ DPO æ–¹æ³•çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä½¿ç”¨ SPO åŸºç¡€æ¨¡å‹æ¯” DPO XL åŸºç¡€æ¨¡å‹å’ŒåŸå§‹ SDXL åŸºç¡€æ¨¡å‹æ€§èƒ½æ›´å¥½ã€‚SPO LoRA å¯ä»¥å¢å¼ºå›¾åƒç»†èŠ‚å’Œå¯¹æ¯”åº¦å¹¶ç¾åŒ–å›¾åƒã€‚æ„Ÿè°¢ SPO èƒŒåçš„æŠ€æœ¯å›¢é˜Ÿã€‚

ç»§ç»­æ‰©å¤§è®­ç»ƒé›†çš„æ¦‚å¿µèŒƒå›´ï¼Œä½†å¯¹è®­ç»ƒé›†è¿›è¡Œäº†ä¼˜åŒ–å’Œç²¾ç®€ï¼ˆå¤§è®­ç»ƒé›†å¾®è°ƒå¤ªè´µï¼Œè€Œä¸”æœ€è¿‘H800ç§Ÿèµ·æ¥ä¹Ÿéš¾ï¼Œè´Ÿæ‹…ä¸èµ·æœ¬åœ°è®­ç»ƒçš„æ—¶é—´ï¼‰ã€‚ç›®å‰æ€»è®­ç»ƒé›†ä¸º20821å¼ å›¾ç‰‡ã€‚è®­ç»ƒé›†åˆ†è¾¨ç‡åˆ†å¸ƒå¦‚ä¸‹ï¼Œå»ºè®®ä½¿ç”¨å›¾ç‰‡æ•°é‡è¾ƒå¤šçš„å‡ ç§åˆ†è¾¨ç‡è¿›è¡Œè¾“å‡ºï¼š

    (832, 1248) - Count: 7128
    (896, 1152) - Count: 6250
    (1248, 832) - Count: 2402
    (1024, 1024) - Count: 1639
    (1360, 768) - Count: 928
    (1152, 896) - Count: 870
    (768, 1360) - Count: 432
    (960, 1088) - Count: 506
    (992, 1056) - Count: 162
    (1088, 960) - Count: 140
    (704, 1472) - Count: 120
    (1056, 992) - Count: 122
    (1472, 704) - Count: 115
    (1632, 640) - Count: 75
    (640, 1632) - Count: 12
ä½¿ç”¨ GPT4O å¯¹æ‰€æœ‰æ•°æ®é›†è¿›è¡Œé‡æ–°æ ‡æ³¨ã€‚æœ¬æ¬¡é‡‡ç”¨äº†ç»“æ„åŒ–çš„æ ‡æ³¨æ–¹æ³•ï¼Œå…·ä½“ç»“æ„ä¸ºï¼šâ€œä¸€å¥è¯æ¦‚æ‹¬æè¿°+å¤šä¸ªå›¾å…ƒæ ‡ç­¾+çµæ„Ÿæ¥è‡ª XXX+ç¾å­¦å“è´¨æè¿°è¯â€ï¼Œå…¶ä¸­ç¾å­¦å“è´¨æè¿°è¯åˆ†ä¸ºäº”ä¸ªç­‰çº§ï¼šæœ€å·®å“è´¨ã€ä½å“è´¨ã€ä¸€èˆ¬å“è´¨ã€æœ€å¥½å“è´¨ã€æ°ä½œã€‚å…¸å‹çš„æ ‡æ³¨ç¤ºä¾‹å¦‚ä¸‹ï¼š

conceptual art featuring a human hand wrapped in red and beige ribbons, isolated against a plain, light background, realistic style, minimalist color scheme, smooth textures, elongated and surreal aesthetic, inspired by salvador dalÃ­'s surrealist works, masterpiece
Inspired by XXX for HelloWorld 7.0ç‰ˆæœ¬æ‰€æ¶‰åŠçš„â€œé«˜é¢‘æ ‡æ³¨è¯è¡¨â€å’Œâ€œé«˜é¢‘è‰ºæœ¯é£æ ¼è¡¨â€ä»…æä¾›ç»™å•†ä¸šæˆæƒç”¨æˆ·ï¼Œä»¥å¾€è´­ä¹°è¿‡Helloworld XLç³»åˆ—æ¨¡å‹æˆæƒçš„ä¼™ä¼´ï¼Œå¦‚æœ‰é—æ¼è¯·è”ç³»æˆ‘å…è´¹è·å–ã€‚

å„ä½ç©å®¶å¯ä»¥å‚è€ƒHelloWorld 6.0é«˜é¢‘æ ‡æ³¨è¯è¡¨ï¼Œå¦å¤–æˆ‘åœ¨å›¾åº“ä¸­ä¹Ÿæä¾›äº†150+å¼ é«˜è´¨é‡çš„HelloWorld 7.0ç¤ºä¾‹å›¾ï¼Œå¯ä»¥ä½œä¸ºå¤§å®¶è¾“å‡ºçš„å‚è€ƒã€‚æ¨¡å‹åˆ¶ä½œä¸æ˜“ï¼Œæ„Ÿè°¢å„ä½ç©å®¶çš„ç†è§£ä¸åŒ…å®¹ï¼


LEOSAM HelloWorld 6.0 Top 250 High-Frequency Tagging Word List 


The main body of the HelloWorld 6.0 training set employs GPT4v tagging. For images that GPT4v cannot tag, cogVQA guided by blip2-opt-6.7b is used for tagging. The tagging language style of these multimodal models differs significantly from the traditional WD1.4 tagger. To facilitate more accurate triggering of different concepts in the training set, I have compiled the top 250 high-frequency tagging words from the HelloWorld 6.0 training set. 

**æ‘„å½±æŠ€æœ¯å’Œç¾å­¦/Photography Techniques and Aesthetics:**

- æ¨¡æ‹Ÿèƒ¶ç‰‡æ‘„å½±ç¾å­¦ (film photography aesthetic)
- æ—¶å°šæ‘„å½± (Fashion photography, Fashion portrait)
- äººåƒæ‘„å½± (Portrait photography, portrait photography, Elegant portrait photography)
- é‡ç”ŸåŠ¨ç‰©æ‘„å½± (wildlife photography)
- ç§æˆ¿æ‘„å½± (Intimate boudoir photography)
- é«˜åˆ†è¾¨ç‡ (high-resolution, high-resolution clarity, high-resolution image, high-resolution portrait)
- æç®€ä¸»ä¹‰é£æ ¼ (minimalist aesthetic, minimalistic aesthetic, minimalistic style, minimalist style, minimalist design)
- å¤å¤ç¾å­¦ (vintage aesthetic)



**æ„å›¾å’ŒèƒŒæ™¯/Composition and Background:**

- ç‰¹å†™ (Close-up, Close-up portrait)
- é¸Ÿç°è§†è§’ (Aerial perspective, Aerial view)
- å¯¹ç§°æ„å›¾ (symmetrical composition)
- æç®€ä¸»ä¹‰æ„å›¾ (minimalist composition)
- æµ…æ™¯æ·± (shallow depth of field)



**è‰²å½©:/Color:**

- é»‘ç™½ (Black and white portrait, Monochrome portrait)
- æš–è‰²è°ƒ (warm color palette, Warm)
- å†·è‰²è°ƒ (cool color palette)
- æŸ”å’Œçš„è‰²å½© (muted color palette, pastel color palette, neutral color palette, earthy tones)
- é²œè‰³çš„è‰²å½© (Vibrant color palette, vibrant colors, rich)

**äººç‰©ç‰¹å¾/Characteristics:**

- å¹´é¾„èŒƒå›´:
    - åå‡ å²åˆ°20å²åˆ (late teens to early twenties, late teens or early twenties)
    - 20å¤šå² (early twenties, early to mid-20s, likely in her 20s, mid-20s, likely in her twenties)
- å¥³æ€§è§’è‰² (young female character,female subject, young adult female, female model)
- å¹´è½»äºšæ´²å¥³æ€§ (young Asian woman, young Asian female, Asian woman, Asian female)
- ç”·æ€§ä¸»ä½“ (young Asian male, male subject)



**æ’å›¾å’Œæ•°å­—è‰ºæœ¯/Illustrations and Digital Art:**

- åŠ¨æ¼«é£æ ¼æ’å›¾ (Anime-style illustration, Vibrant anime-style illustration)
- æ•°å­—æ’å›¾ (Vibrant digital illustration, Digital illustration, Vibrant digital artwork)
- æ‰‹ç»˜æ’å›¾ (Hand-drawn illustration)
- åŠ¨æ¼«é£æ ¼ (anime style)
- æ•°å­—è‰ºæœ¯ (digital art)
- ç®€æ´çš„çº¿æ¡ (clean lines)

**å…¶ä»–/Others:**

- ä¼˜é›… (Elegant, elegance, timeless elegance, casual elegance, sophisticated)
- æ¢¦å¹» (ethereal, ethereal quality, ethereal ambiance, ethereal atmosphere, ethereal aesthetic)
- å®é™ (serene, serene ambiance, serene atmosphere, tranquil ambiance, tranquil atmosphere)



**å‘å‹/Hairstyle:**

- é•¿é»‘å‘ (long dark hair, long black hair)
- çŸ­é»‘å‘ (short black hair)
- é•¿è€Œé£˜é€¸çš„å¤´å‘ (long flowing hair)

**æœè£…/Clothing:**

- ç™½è‰²è¡¬è¡« (white shirt, crisp white shirt)
- ä¼‘é—²è£…æ‰® (casual attire, casual fashion, casual style)
- ç™½è‰²è¿åŠ¨é‹ (white sneakers)
- åŠè†è¢œ (knee-high socks)
- ç™½è‰²è¢œå­ (white socks)
- æ²¡æœ‰æ˜æ˜¾çš„é…é¥° (no visible accessories)


**è¡¨æƒ…/Expression:**

- å¹³é™çš„è¡¨æƒ… (serene expression, serene facial expression)
- æ²‰æ€çš„è¡¨æƒ… (contemplative expression, thoughtful expression, pensive expression, contemplative mood)
- ä¸“æ³¨çš„è¡¨æƒ… (focused expression)
- æ¸©å’Œçš„è¡¨æƒ… (gentle expression)
- å¾®ç¬‘ (slight smile, subtle smile, gentle smile, radiant smile)
- ä¸¥è‚ƒçš„è¡¨æƒ… (solemn expression)
- å¿«ä¹çš„è¡¨æƒ… (joyful expression, playful expression)
- ä¸­æ€§è¡¨æƒ… (neutral expression)
- å†…çœçš„æƒ…ç»ª (introspective mood)

**å§¿åŠ¿å’Œæ€åº¦/Posture and Attitude:**

- ç›´è§† (direct gaze, intense gaze, piercing gaze, focused gaze, soft gaze, contemplative gaze, gentle gaze, introspective gaze)
- é—­çœ¼ (eyes closed, closed eyes)
- ä¾§é¢ (side profile)
- æ”¾æ¾çš„å§¿åŠ¿ (relaxed pose, relaxed posture, relaxed demeanor)
- å¤§æ–¹å¾—ä½“çš„å§¿æ€ (poised stance, poised demeanor, poised expression, confident stance)


**é¢éƒ¨ç‰¹å¾/Facial Features:**

- ç»†è…»çš„é¢éƒ¨ç‰¹å¾ (delicate features, delicate facial features, clear complexion)
- äºšæ´²é¢å­”ç‰¹å¾ (Asian features)
- ç™½çš™è‚Œè‚¤ (fair skin, fair complexion, pale skin, light skin, porcelain skin)
- å…‰æ»‘çš„çš®è‚¤ (smooth skin, flawless skin, clear skin)
- å¾®å¦† (subtle makeup, minimal makeup, natural makeup)
    - å¼ºè°ƒè‡ªç„¶ç¾çš„å¾®å¦† (subtle makeup emphasizing natural beauty, subtle makeup highlighting natural beauty, subtle makeup enhancing natural beauty, subtle makeup highlighting natural features)
- è„¸é¢Šä¸Šæ·¡æ·¡çš„è…®çº¢ (subtle blush on cheeks)






## SPO-Diffusion-Models/SPO-SDXL_4k-p_10ep

æ­¥éª¤æ„ŸçŸ¥åå¥½ä¼˜åŒ–ï¼šåœ¨æ¯ä¸€æ­¥ä¸­å°†åå¥½ä¸å»å™ªæ€§èƒ½ç›¸ç»“åˆ

Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step

æœ€è¿‘ï¼Œç›´æ¥åå¥½ä¼˜åŒ– (DPO) å·²å°†å…¶æˆåŠŸä»å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ‰©å±•åˆ°å¯¹é½æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½ã€‚ä¸å¤§å¤šæ•°ç°æœ‰çš„ DPO æ–¹æ³•å‡è®¾æ‰€æœ‰æ‰©æ•£æ­¥éª¤ä¸æœ€ç»ˆç”Ÿæˆçš„å›¾åƒå…±äº«ä¸€è‡´çš„åå¥½é¡ºåºä¸åŒï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§å‡è®¾å¿½ç•¥äº†ç‰¹å®šäºæ­¥éª¤çš„å»å™ªæ€§èƒ½ï¼Œå¹¶ä¸”åå¥½æ ‡ç­¾åº”è¯¥æ ¹æ®æ¯ä¸ªæ­¥éª¤çš„è´¡çŒ®è¿›è¡Œé‡èº«å®šåˆ¶ã€‚

ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ­¥è¿›æ„ŸçŸ¥åå¥½ä¼˜åŒ– (SPO)ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒæ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ­¥è¿›æ„ŸçŸ¥åå¥½æ¨¡å‹å’Œæ­¥è¿›é‡é‡‡æ ·å™¨æ¥ç¡®ä¿å‡†ç¡®çš„æ­¥è¿›æ„ŸçŸ¥ç›‘ç£ï¼Œä»è€Œç‹¬ç«‹è¯„ä¼°å’Œè°ƒæ•´æ¯ä¸€æ­¥çš„å»å™ªæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬éƒ½ä¼šä»ä¸€ç»„å›¾åƒä¸­æŠ½æ ·ï¼Œæ‰¾åˆ°åˆé€‚çš„èƒœè´Ÿå¯¹ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œä»æ± ä¸­éšæœºé€‰æ‹©ä¸€å¼ å›¾åƒæ¥åˆå§‹åŒ–ä¸‹ä¸€ä¸ªå»å™ªæ­¥éª¤ã€‚è¿™ä¸ªæ­¥è¿›é‡é‡‡æ ·è¿‡ç¨‹å¯ç¡®ä¿ä¸‹ä¸€ä¸ªèƒœè´Ÿå›¾åƒå¯¹æ¥è‡ªåŒä¸€å›¾åƒï¼Œä½¿èƒœè´Ÿæ¯”è¾ƒç‹¬ç«‹äºä¸Šä¸€æ­¥ã€‚ä¸ºäº†è¯„ä¼°æ¯ä¸€æ­¥çš„åå¥½ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå•ç‹¬çš„æ­¥è¿›æ„ŸçŸ¥åå¥½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åº”ç”¨äºå˜ˆæ‚å›¾åƒå’Œå¹²å‡€å›¾åƒã€‚

æˆ‘ä»¬å¯¹ Stable Diffusion v1.5 å’Œ SDXL çš„å®éªŒè¡¨æ˜ï¼ŒSPO åœ¨å°†ç”Ÿæˆçš„å›¾åƒä¸å¤æ‚ã€è¯¦ç»†çš„æç¤ºå¯¹é½ä»¥åŠå¢å¼ºç¾æ„Ÿæ–¹é¢æ˜æ˜¾ä¼˜äºæœ€æ–°çš„ Diffusion-DPOï¼ŒåŒæ—¶è®­ç»ƒæ•ˆç‡ä¹Ÿæé«˜äº† 20 å€ä»¥ä¸Šã€‚ä»£ç å’Œæ¨¡å‹ï¼šhttps://rockeycoss.github.io/spo.github.io/

è¯¥æ¨¡å‹æ˜¯ä»stable-diffusion-xl-base-1.0å¾®è°ƒè€Œæ¥çš„ã€‚å®ƒå·²é’ˆå¯¹ 4,000 ä¸ªæç¤ºè¿›è¡Œäº† 10 ä¸ªæ—¶æœŸçš„è®­ç»ƒã€‚

è¿™æ˜¯ä¸€ä¸ªåˆå¹¶æ£€æŸ¥ç‚¹ï¼Œå°† LoRA æ£€æŸ¥ç‚¹ä¸åŸºç¡€æ¨¡å‹stable-diffusion-xl-base-1.0ç›¸ç»“åˆã€‚å¦‚æœæ‚¨æƒ³è®¿é—® LoRA æ£€æŸ¥ç‚¹ï¼Œè¯·è®¿é—®SPO-SDXL_4k-p_10ep_LoRA ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸stable-diffusion-webuiå…¼å®¹çš„ LoRA æ£€æŸ¥ç‚¹ï¼Œå¯åœ¨æ­¤å¤„è®¿é—®ã€‚



![alt text](assets/6367/image-12.png)


# EasyAnimate

easyphotoå‡ºå“    

å¯ç”¨äºç”ŸæˆAIå›¾ç‰‡ä¸è§†é¢‘ã€è®­ç»ƒDiffusion Transformerçš„åŸºçº¿æ¨¡å‹ä¸Loraæ¨¡å‹ï¼Œæˆ‘ä»¬æ”¯æŒä»å·²ç»è®­ç»ƒå¥½çš„EasyAnimateæ¨¡å‹ç›´æ¥è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆä¸åŒåˆ†è¾¨ç‡ï¼Œ6ç§’å·¦å³ã€fps24çš„è§†é¢‘ï¼ˆ1 ~ 144å¸§, æœªæ¥ä¼šæ”¯æŒæ›´é•¿çš„è§†é¢‘ï¼‰ï¼Œä¹Ÿæ”¯æŒç”¨æˆ·è®­ç»ƒè‡ªå·±çš„åŸºçº¿æ¨¡å‹ä¸Loraæ¨¡å‹ï¼Œè¿›è¡Œä¸€å®šçš„é£æ ¼å˜æ¢ã€‚


æ–°ç‰¹æ€§ï¼š

æ›´æ–°åˆ°v2ç‰ˆæœ¬ï¼Œæœ€å¤§æ”¯æŒ144å¸§(768x768, 6s, 24fps)ç”Ÿæˆã€‚[ 2024.05.26 ]


åŠŸèƒ½æ¦‚è§ˆï¼š

    æ•°æ®é¢„å¤„ç†
    è®­ç»ƒVAE
    è®­ç»ƒDiT
    æ¨¡å‹ç”Ÿæˆ


pixart a åŸºç¡€æ¨¡å‹     
è®­ç»ƒ   


1. å¼•å…¥è¿åŠ¨æ¨¡å—ï¼ˆMotion Moduleï¼‰ï¼Œä»¥å®ç°ä»2Då›¾åƒåˆ°3Dè§†é¢‘çš„æ‰©å±•   
2. å¼•å…¥slice VAEå‹ç¼©æ—¶é—´è½´ï¼Œæœ‰åŠ©äºé•¿è§†é¢‘ç”Ÿæˆã€‚




# MV-VTON


PyTorch implementation of MV-VTON: Multi-View Virtual Try-On with Diffusion Models


https://github.com/hywang2002/MV-VTON

ğŸ”¥The first multi-view virtual try-on dataset MVG is now available.
ğŸ”¥Checkpoints on both frontal-view and multi-view virtual try-on tasks are released.

   [æäº¤äº 2024 å¹´ 4 æœˆ 26 æ—¥ï¼ˆv1ï¼‰ï¼Œæœ€åä¿®è®¢äº 2024 å¹´ 4 æœˆ 29 æ—¥ï¼ˆæ­¤ç‰ˆæœ¬ï¼Œv2ï¼‰]     
MV-VTONï¼šé‡‡ç”¨æ‰©æ•£æ¨¡å‹çš„å¤šè§†å›¾è™šæ‹Ÿè¯•ç©¿   
ç‹æµ©å®‡ã€å¼ å¿—ç¦„ã€é‚¸ä¸œæ—ã€å¼ ä¸–è‰¯ã€å·¦ç‹çŒ›    
åŸºäºå›¾åƒçš„è™šæ‹Ÿè¯•ç©¿çš„ç›®æ ‡æ˜¯ç”Ÿæˆç›®æ ‡äººç‰©è‡ªç„¶ç©¿ç€ç»™å®šæœè£…çš„å›¾åƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…å…³æ³¨ä½¿ç”¨æ­£é¢æœè£…çš„æ­£é¢è¯•ç©¿ã€‚å½“æœè£…å’Œäººç‰©çš„è§†å›¾æ˜æ˜¾ä¸ä¸€è‡´æ—¶ï¼Œç‰¹åˆ«æ˜¯å½“äººç‰©çš„è§†å›¾ä¸æ˜¯æ­£é¢æ—¶ï¼Œç»“æœå¹¶ä¸ä»¤äººæ»¡æ„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šè§†å›¾è™šæ‹Ÿè¯•ç©¿ (MV-VTON)ï¼Œæ—¨åœ¨ä½¿ç”¨ç»™å®šçš„æœè£…ä»å¤šä¸ªè§†å›¾é‡å»ºäººç‰©çš„ç©¿è¡£ç»“æœã€‚ä¸€æ–¹é¢ï¼Œç”±äºå•è§†å›¾æœè£…ä¸º MV-VTON æä¾›çš„ä¿¡æ¯ä¸è¶³ï¼Œæˆ‘ä»¬æ”¹ä¸ºä½¿ç”¨ä¸¤å¹…å›¾åƒï¼Œå³æœè£…çš„æ­£é¢å’ŒèƒŒé¢è§†å›¾ï¼Œä»¥å°½å¯èƒ½åœ°æ¶µç›–å®Œæ•´çš„è§†å›¾ã€‚å¦ä¸€æ–¹é¢ï¼Œé‡‡ç”¨è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹æ¥æ‰§è¡Œæˆ‘ä»¬çš„ MV-VTONã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†å›¾è‡ªé€‚åº”é€‰æ‹©æ–¹æ³•ï¼Œå…¶ä¸­ç¡¬é€‰æ‹©å’Œè½¯é€‰æ‹©åˆ†åˆ«åº”ç”¨äºå…¨å±€å’Œå±€éƒ¨æœè£…ç‰¹å¾æå–ã€‚è¿™ç¡®ä¿æœè£…ç‰¹å¾å¤§è‡´ç¬¦åˆäººçš„è§†è§’ã€‚éšåï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨è”åˆæ³¨æ„åŠ›æ¨¡å—æ¥å¯¹é½å’Œèåˆæœè£…ç‰¹å¾ä¸äººç‰©ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ª MV-VTON æ•°æ®é›†ï¼Œå³å¤šè§†å›¾æœè£… (MVG)ï¼Œå…¶ä¸­æ¯ä¸ªäººéƒ½æœ‰å¤šå¼ å…·æœ‰ä¸åŒè§†è§’å’Œå§¿åŠ¿çš„ç…§ç‰‡ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ä»…åœ¨ä½¿ç”¨æˆ‘ä»¬çš„ MVG æ•°æ®é›†çš„ MV-VTON ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³ç»“æœï¼Œè€Œä¸”åœ¨ä½¿ç”¨ VITON-HD å’Œ DressCode æ•°æ®é›†çš„æ­£é¢è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸Šä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨æ­¤ https URLä¸Šå…¬å¼€å‘å¸ƒã€‚

![alt text](assets/6367/image-13.png)

![alt text](assets/6367/image-14.png)

å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦

ä¼°è®¡æ˜¯å‡†å¤‡ä¸Šç ”ä¸€ å¤§å››åˆšæ¯•ä¸š ç›´æ¥ä¿    

Acknowledgements   
Our code is heavily borrowed from Paint-by-Example and DCI-VTON. We also thank previous work PF-AFN, GP-VTON, LaDI-VTON and StableVITON.

å®ä¹ 

ç†æƒ³æ±½è½¦æ˜¯ä¸­å›½æ–°èƒ½æºæ±½è½¦å¸‚åœºçš„é¢†å¯¼è€…ã€‚æœ¬å…¬å¸è®¾è®¡ã€ç ”å‘ã€åˆ¶é€ å’Œé”€å”®è±ªåæ™ºèƒ½ç”µåŠ¨è½¦ã€‚ç†æƒ³æ±½è½¦çš„ä½¿å‘½æ˜¯åˆ›é€ ç§»åŠ¨çš„å®¶ï¼Œåˆ›é€ å¹¸ç¦çš„å®¶ã€‚é€šè¿‡äº§å“ã€æŠ€æœ¯å’Œä¸šåŠ¡æ¨¡å¼çš„åˆ›æ–°ï¼Œæœ¬å…¬å¸ä¸ºå®¶åº­ç”¨æˆ·æä¾›å®‰å…¨ã€ä¾¿æ·ã€èˆ’é€‚çš„äº§å“ä¸æœåŠ¡ã€‚åœ¨ä¸­å›½ï¼Œç†æƒ³æ±½è½¦æ˜¯æˆåŠŸå°†å¢ç¨‹å¼ç”µåŠ¨è½¦å•†ä¸šåŒ–çš„å…ˆé©±ã€‚ç†æƒ³æ±½è½¦äº2019å¹´11æœˆå¼€å§‹é‡äº§ã€‚



å¯ä»¥å‡†å¤‡å»æ·˜å®äº¬ä¸œäº†    


# LCM-LoRA


æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ (LCM) é€šå¸¸åªéœ€ 2-4 ä¸ªæ­¥éª¤å³å¯ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä»è€Œå¯ä»¥åœ¨å‡ ä¹å®æ—¶çš„è®¾ç½®ä¸­ä½¿ç”¨æ‰©æ•£æ¨¡å‹ã€‚


åªéœ€ 4,000 ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆçº¦ 32 ä¸ª A100 GPU å°æ—¶ï¼‰å³å¯ä»ä»»ä½•é¢„å…ˆè®­ç»ƒçš„ç¨³å®šæ‰©æ•£ (SD) ä¸­æå– LCMï¼Œä»è€Œä»¥ 2~4 ä¸ªæ­¥éª¤ç”šè‡³ä¸€ä¸ªæ­¥éª¤ç”Ÿæˆé«˜è´¨é‡çš„ 768 x 768 åˆ†è¾¨ç‡å›¾åƒï¼Œæ˜¾è‘—åŠ å¿«æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆé€Ÿåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ LCM ä»…ç”¨ 4,000 æ¬¡è®­ç»ƒè¿­ä»£å°±æå–äº† Dreamshaper-V7 ç‰ˆæœ¬çš„ SDã€‚

ä½†æ˜¯ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½éœ€è¦å•ç‹¬è¿›è¡Œè’¸é¦æ‰èƒ½è¿›è¡Œæ½œåœ¨ä¸€è‡´æ€§è’¸é¦ã€‚LCM-LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯åªè®­ç»ƒå‡ ä¸ªé€‚é…å™¨å±‚ï¼Œåœ¨æœ¬ä¾‹ä¸­é€‚é…å™¨å°±æ˜¯ LoRAã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±ä¸å¿…è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥æ§åˆ¶å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚ç„¶åå¯ä»¥å°†ç”Ÿæˆçš„ LoRA åº”ç”¨äºæ¨¡å‹çš„ä»»ä½•å¾®è°ƒç‰ˆæœ¬ï¼Œè€Œæ— éœ€å•ç‹¬è’¸é¦å®ƒä»¬ã€‚æ­¤å¤–ï¼ŒLoRA å¯ä»¥åº”ç”¨äºå›¾åƒåˆ°å›¾åƒã€ControlNet/T2I-Adapterã€ä¿®å¤ã€AnimateDiff ç­‰ã€‚LCM-LoRA è¿˜å¯ä»¥ä¸å…¶ä»– LoRA ç»“åˆä½¿ç”¨ï¼Œä»¥æå°‘çš„æ­¥éª¤ï¼ˆ4-8ï¼‰ç”Ÿæˆé£æ ¼åŒ–å›¾åƒã€‚

LCM-LoRA é€‚ç”¨äºstable-diffusion-v1-5ã€stable-diffusion-xl-base-1.0å’ŒSSD-1Bæ¨¡å‹ã€‚æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½å¯ä»¥åœ¨æ­¤é›†åˆä¸­æ‰¾åˆ°ã€‚

    åŠ è½½ç‰¹å®šä»»åŠ¡çš„ç®¡é“å’Œæ¨¡å‹ã€‚
    å°†è°ƒåº¦ç¨‹åºè®¾ç½®ä¸ºLCMSchedulerã€‚
    ä¸ºæ¨¡å‹åŠ è½½ LCM-LoRA æƒé‡ã€‚
    å‡å°‘guidance_scaleé—´éš”[1.0, 2.0]å¹¶å°†num_inference_stepsé—´éš”è®¾ç½®ä¸º[4, 8]ã€‚
    ä½¿ç”¨å¸¸ç”¨å‚æ•°é€šè¿‡ç®¡é“æ‰§è¡Œæ¨ç†ã€‚


é¦–å…ˆï¼Œç¡®ä¿æ‚¨å·²å®‰è£…peftï¼Œä»¥è·å¾—æ›´å¥½çš„ LoRA æ”¯æŒã€‚

å·²å¤åˆ¶
pip å®‰è£…-U peft


æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°æˆ‘ä»¬è®¾ç½®äº†guidance_scale=1.0ï¼Œè¿™å°†ç¦ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼ã€‚è¿™æ˜¯å› ä¸º LCM-LoRA æ˜¯åœ¨æŒ‡å¯¼ä¸‹è®­ç»ƒçš„ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹æ‰¹å¤„ç†å¤§å°ä¸å¿…åŠ å€ã€‚è¿™å¯ä»¥ç¼©çŸ­æ¨ç†æ—¶é—´ï¼Œä½†ç¼ºç‚¹æ˜¯è´Ÿé¢æç¤ºå¯¹å»å™ªè¿‡ç¨‹æ²¡æœ‰ä»»ä½•å½±å“ã€‚

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ LCM-LoRA è¿›è¡ŒæŒ‡å¯¼ï¼Œä½†ç”±äºè®­ç»ƒçš„æ€§è´¨ï¼Œæ¨¡å‹å¯¹å€¼éå¸¸æ•æ„Ÿguidance_scaleï¼Œé«˜å€¼å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„å›¾åƒä¸­å‡ºç°ä¼ªå½±ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°æœ€ä½³å€¼åœ¨ [1.0, 2.0] èŒƒå›´å†…ã€‚

This is because the LCM-LoRA is trained with guidance, so the batch size does not have to be doubled in this case. 

ä¸é£æ ¼åŒ–çš„ LoRA ç›¸ç»“åˆ     
LCM-LoRA å¯ä»¥ä¸å…¶ä»– LoRA ç»“åˆä½¿ç”¨ï¼Œåªéœ€å‡ ä¸ªæ­¥éª¤ï¼ˆ4-8ï¼‰å³å¯ç”Ÿæˆæ ·å¼å›¾åƒã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LCM-LoRA å’Œå‰ªçº¸ LoRA


loraæ›´åƒæ˜¯ä¸€ç§è¿‡æ‹Ÿåˆçš„ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒ     
åŒ…æ‹¬åŠ é€Ÿlora     
å…¶æœ¬è´¨å°±æ˜¯ä¸‹æ¬¡è¾“å…¥çš„æç¤ºè¯å¾ˆå°‘å°±æœ‰åŸæ¥çš„é£æ ¼     
åŒ…æ‹¬å¾®è°ƒæ¨¡å‹ï¼Œç°åœ¨åŸºæœ¬ä¸Šå°†å…ˆéªŒè®¾ç½®æˆäº†ç¾ä¸½å’Œå¥½çœ‹


åŠ¨ç”»å·®å¼‚   
AnimateDiffå…è®¸æ‚¨ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸ºå›¾åƒåˆ¶ä½œåŠ¨ç”»ã€‚ä¸ºäº†è·å¾—è‰¯å¥½çš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆå¤šä¸ªå¸§ï¼ˆ16-24ï¼‰ï¼Œè€Œä½¿ç”¨æ ‡å‡† SD æ¨¡å‹æ‰§è¡Œæ­¤æ“ä½œå¯èƒ½ä¼šéå¸¸æ…¢ã€‚LCM-LoRA å¯ç”¨äºæ˜¾è‘—åŠ å¿«è¯¥è¿‡ç¨‹ï¼Œå› ä¸ºæ‚¨åªéœ€ä¸ºæ¯ä¸ªå¸§æ‰§è¡Œ 4-8 ä¸ªæ­¥éª¤ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ LCM-LoRA å’Œ AnimateDiff æ‰§è¡ŒåŠ¨ç”»ã€‚


    import torch
    from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler
    from diffusers.utils import export_to_gif

    adapter = MotionAdapter.from_pretrained("diffusers/animatediff-motion-adapter-v1-5")
    pipe = AnimateDiffPipeline.from_pretrained(
        "frankjoshua/toonyou_beta6",
        motion_adapter=adapter,
    ).to("cuda")

    # set scheduler
    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

    # load LCM-LoRA
    pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5", adapter_name="lcm")
    pipe.load_lora_weights("guoyww/animatediff-motion-lora-zoom-in", weight_name="diffusion_pytorch_model.safetensors", adapter_name="motion-lora")

    pipe.set_adapters(["lcm", "motion-lora"], adapter_weights=[0.55, 1.2])

    prompt = "best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress"
    generator = torch.manual_seed(0)
    frames = pipe(
        prompt=prompt,
        num_inference_steps=5,
        guidance_scale=1.25,
        cross_attention_kwargs={"scale": 1},
        num_frames=24,
        generator=generator
    ).frames[0]
    export_to_gif(frames, "animation.gif")

## åŸç†
æŠ½è±¡çš„     
æ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDM) åœ¨åˆæˆé«˜åˆ†è¾¨ç‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œè¿­ä»£é‡‡æ ·è¿‡ç¨‹è®¡ç®—é‡å·¨å¤§ï¼Œå¯¼è‡´ç”Ÿæˆé€Ÿåº¦ç¼“æ…¢ã€‚å—ä¸€è‡´æ€§æ¨¡å‹ (song ç­‰äºº) çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ (LCM)ï¼Œèƒ½å¤Ÿåœ¨ä»»ä½•é¢„å…ˆè®­ç»ƒçš„ LDMï¼ˆåŒ…æ‹¬ç¨³å®šæ‰©æ•£ (rombach ç­‰äºº)ï¼‰ä¸Šä»¥æœ€å°‘çš„æ­¥éª¤è¿›è¡Œå¿«é€Ÿæ¨ç†ã€‚å°†å¼•å¯¼çš„é€†æ‰©æ•£è¿‡ç¨‹è§†ä¸ºæ±‚è§£å¢å¼ºæ¦‚ç‡æµ ODE (PF-ODE)ï¼ŒLCM æ—¨åœ¨ç›´æ¥é¢„æµ‹æ­¤ç±» ODE åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è§£ï¼Œä»è€Œæ— éœ€å¤šæ¬¡è¿­ä»£å¹¶å…è®¸å¿«é€Ÿã€é«˜ä¿çœŸé‡‡æ ·ã€‚ä»é¢„å…ˆè®­ç»ƒçš„æ— åˆ†ç±»å™¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¸­æœ‰æ•ˆæç‚¼è€Œæ¥ï¼Œé«˜è´¨é‡çš„ 768 x 768 2~4 æ­¥ LCM ä»…éœ€ 32 ä¸ª A100 GPU å°æ—¶è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨ä¸€è‡´æ€§å¾®è°ƒ (LCF)ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºåœ¨å®šåˆ¶å›¾åƒæ•°æ®é›†ä¸Šå¾®è°ƒ LCMã€‚åœ¨ LAION-5B-Aesthetics æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLCM ä»…éœ€å‡ æ­¥æ¨ç†å³å¯å®ç°æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://latent-consistency-models.github.io/


## ä½¿ç”¨æ•ˆæœ
æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œä»…ä½¿ç”¨1æ­¥å³å¯ç”Ÿæˆè¿‘ä¼¼å½¢çŠ¶ï¼Œæ²¡æœ‰å¯è¾¨åˆ«çš„ç‰¹å¾ï¼Œä¹Ÿç¼ºä¹çº¹ç†ã€‚ä½†æ˜¯ï¼Œç»“æœä¼šè¿…é€Ÿæ”¹å–„ï¼Œé€šå¸¸åªéœ€ 4 åˆ° 6 æ­¥å³å¯è·å¾—ä»¤äººæ»¡æ„çš„ç»“æœã€‚å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘å‘ç°ä¸Šä¸€ä¸ªæµ‹è¯•ä¸­çš„ 8 æ­¥å›¾åƒæœ‰ç‚¹å¤ªé¥±å’Œå’Œâ€œå¡é€šåŒ–â€ï¼Œä¸ç¬¦åˆæˆ‘çš„å£å‘³ï¼Œå› æ­¤æˆ‘å¯èƒ½ä¼šåœ¨æ­¤ç¤ºä¾‹ä¸­é€‰æ‹© 5 æ­¥å’Œ 6 æ­¥çš„å›¾åƒã€‚ç”Ÿæˆé€Ÿåº¦éå¸¸å¿«ï¼Œæ‚¨åªéœ€ 4 æ­¥å°±å¯ä»¥åˆ›å»ºå¤§é‡ä¸åŒçš„å˜ä½“ï¼Œç„¶åé€‰æ‹©æ‚¨å–œæ¬¢çš„å˜ä½“ï¼Œå¹¶æ ¹æ®éœ€è¦ä½¿ç”¨æ›´å¤šæ­¥éª¤å’Œç»†åŒ–æç¤ºè¿›è¡Œè¿­ä»£ã€‚


![alt text](assets/6367/image-15.png)


![alt text](assets/6367/image-16.png)


SDXL ç®¡é“ç»“æœï¼ˆç›¸åŒçš„æç¤ºå’Œéšæœºç§å­ï¼‰ï¼Œä½¿ç”¨ 1ã€4ã€8ã€15ã€20ã€25ã€30 å’Œ 50 ä¸ªæ­¥éª¤ã€‚



## peft

PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware

PEFTï¼šåœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿çº§æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

åŸºäº Transformer æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ï¼Œå¦‚ GPTã€T5 å’Œ BERTï¼Œå·²åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æˆæœã€‚å®ƒä»¬è¿˜å¼€å§‹æ¶‰è¶³å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è®¡ç®—æœºè§†è§‰ (CV)ï¼ˆVITã€Stable Diffusionã€LayoutLMï¼‰å’ŒéŸ³é¢‘ï¼ˆWhisperã€XLS-Rï¼‰ã€‚ä¼ ç»ŸèŒƒå¼æ˜¯åœ¨é€šç”¨ç½‘ç»œè§„æ¨¡æ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç„¶åå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚ä¸å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒ LLMï¼ˆä¾‹å¦‚é›¶æ ·æœ¬æ¨ç†ï¼‰ç›¸æ¯”ï¼Œåœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šå¯¹è¿™äº›é¢„è®­ç»ƒçš„ LLM è¿›è¡Œå¾®è°ƒå¯å¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ã€‚


PEFT æ–¹æ³•ä»…å¾®è°ƒå°‘é‡ï¼ˆé¢å¤–ï¼‰æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶å†»ç»“é¢„è®­ç»ƒ LLM çš„å¤§å¤šæ•°å‚æ•°ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚è¿™ä¹Ÿå…‹æœäº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œè¿™æ˜¯åœ¨ LLM å®Œå…¨å¾®è°ƒæœŸé—´è§‚å¯Ÿåˆ°çš„è¡Œä¸ºã€‚PEFT æ–¹æ³•è¿˜æ˜¾ç¤ºå‡ºæ¯”ä½æ•°æ®èŒƒå›´å†…çš„å¾®è°ƒæ›´å¥½ï¼Œå¹¶ä¸”æ›´å¥½åœ°æ¨å¹¿åˆ°åŸŸå¤–åœºæ™¯ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»å’Œç¨³å®šæ‰©æ•£ dreamboothã€‚

    LoRAï¼šLORAï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç§©è‡ªé€‚åº”
    å‰ç¼€è°ƒä¼˜ï¼šP-Tuning v2ï¼šå³æ—¶è°ƒä¼˜å¯ä¸è·¨å°ºåº¦å’Œä»»åŠ¡çš„æ™®éå¾®è°ƒç›¸åª²ç¾
    å¿«é€Ÿè°ƒä¼˜ï¼šå‚æ•°é«˜æ•ˆå¿«é€Ÿè°ƒä¼˜çš„è§„æ¨¡åŒ–åŠ›é‡
    P-Tuningï¼šGPT ä¹Ÿèƒ½ç†è§£


    LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
    Prefix Tuning: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
    Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning
    P-Tuning: GPT Understands, Too






# sd3 canny
6.16


# ç»“å°¾
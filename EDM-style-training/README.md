# Elucidating the Design Space of Diffusion-Based Generative Models

## é¡¹ç›®èƒŒæ™¯
PlaygroundAI ä¸‰æœˆåˆæ¨å‡º Playground v2.5 ï¼Œå…¶ä»¿ä½›åŸºäºedmå…¬å¼è®­ç»ƒã€‚è´¨é‡å®£ç§°ä¼˜äºç°æœ‰å„ç§æ¨¡å‹        
æˆ‘ä»¬è§£å†³äº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šå¢å¼ºè‰²å½©å’Œå¯¹æ¯”åº¦ã€æ”¹è¿›å¤šå®½é«˜æ¯”ç”Ÿæˆä»¥åŠæ”¹è¿›ä»¥äººä¸ºä¸­å¿ƒçš„ç²¾ç»†ç»†èŠ‚ã€‚    
è‹±æ–‡ç‰ˆåšå®¢ https://playground.com/blog/playground-v2-5     
æŠ€æœ¯æŠ¥å‘Š https://marketing-cdn.playground.com/research/pgv2.5_compressed.pdf    

diffuseråº“åœ¨2024ä¸‰æœˆåå·ä¸Šæ–°ï¼Œæ”¯æŒ Playground v2.5 æ¨ç†å’ŒåŸºäºå…¶çš„ dreambooth_lora å¾®è°ƒ (å¯ä»¥å¸¦ä¸Šedm)      

diffuser
v0.27.0: Stable Cascade, Playground v2.5, EDM-style training, IP-Adapter image embeds, and more   
éœ€è¦å®æµ‹æ¨¡å‹æ•ˆæœ    
å¤§è‡´çœ‹æ¥å¯ä»¥è¾¾åˆ°åŠ é€Ÿï¼Œä»¥åŠè´¨é‡ä¸é™ä½çš„ç‰¹æ•ˆ   
ç±»ä¼¼LCM   

æ›´æ–°æ¦‚è§ˆï¼š    

Stable Cascade    
Stable Cascade ç³»åˆ—ç®¡é“ä¸ Stable Diffusion çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒä»¬å»ºç«‹åœ¨ä¸‰ä¸ªä¸åŒçš„æ¨¡å‹ä¹‹ä¸Šï¼Œå¹¶å…è®¸å¯¹æ‚£è€…å›¾åƒè¿›è¡Œåˆ†å±‚å‹ç¼©ï¼Œä»è€Œå®ç°å“è¶Šçš„è¾“å‡ºã€‚   
allow for hierarchical compression of image patients, achieving remarkable outputs.


Playground v2.5   
PlaygroundAI å‘å¸ƒäº†æ–°çš„ v2.5 æ¨¡å‹ï¼ˆplaygroundai/playground-v2.5-1024px-aestheticï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ç¾è§‚æ–¹é¢å°¤å…¶å‡ºè‰²ã€‚é™¤äº†ä¸€äº›è°ƒæ•´ä¹‹å¤–ï¼Œè¯¥æ¨¡å‹ç´§å¯†éµå¾ª Stable Diffusion XL çš„æ¶æ„ã€‚   

EDM-style training   
EDM æ˜¯æŒ‡ä»¥ä¸‹è®ºæ–‡ä¸­ä»‹ç»çš„è®­ç»ƒå’Œé‡‡æ ·æŠ€æœ¯ï¼šElucidating the Design Space of Diffusion-Based Generative Modelsã€‚æˆ‘ä»¬åœ¨è„šæœ¬ä¸­å¼•å…¥äº†å¯¹ä½¿ç”¨ EDM å…¬å¼è¿›è¡Œè®­ç»ƒçš„æ”¯æŒtrain_dreambooth_lora_sdxl.pyã€‚  
è¦stabilityai/stable-diffusion-xl-base-1.0ä½¿ç”¨ EDM å…¬å¼è¿›è¡Œè®­ç»ƒï¼Œæ‚¨åªéœ€--do_edm_style_trainingåœ¨è®­ç»ƒå‘½ä»¤ä¸­æŒ‡å®šæ ‡å¿—å³å¯   
é‡‡ç”¨ EDM å…¬å¼çš„æ–°è°ƒåº¦ç¨‹åº   
ä¸ºäº†æ›´å¥½åœ°æ”¯æŒ Playground v2.5 æ¨¡å‹å’Œ EDM å¼åŸ¹è®­ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹EDMDPMSolverMultistepSchedulerå’Œ çš„æ”¯æŒEDMEulerSchedulerã€‚DPMSolverMultistepSchedulerå®ƒä»¬åˆ†åˆ«æ”¯æŒå’Œ çš„EDM å…¬å¼EulerDiscreteSchedulerã€‚    

Trajectory Consistency Distillation   
è½¨è¿¹ä¸€è‡´æ€§è’¸é¦ (TCD) ä½¿æ¨¡å‹èƒ½å¤Ÿä»¥æ›´å°‘çš„æ­¥éª¤ç”Ÿæˆæ›´é«˜è´¨é‡å’Œæ›´è¯¦ç»†çš„å›¾åƒã€‚æ­¤å¤–ï¼Œç”±äºè’¸é¦è¿‡ç¨‹ä¸­æœ‰æ•ˆçš„è¯¯å·®ç¼“è§£ï¼Œå³ä½¿åœ¨æ¨ç†æ­¥éª¤è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼ŒTCD ä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚å®ƒæ˜¯åœ¨è½¨è¿¹ä¸€è‡´æ€§è’¸é¦ä¸­æå‡ºçš„ã€‚   
æ­¤ç‰ˆæœ¬æä¾›äº†æ”¯æŒTCDSchedulerè¿™ç§å¿«é€Ÿé‡‡æ ·çš„åŠŸèƒ½ã€‚ä¸ LCM-LoRA éå¸¸ç›¸ä¼¼ï¼ŒTCD éœ€è¦é¢å¤–çš„é€‚é…å™¨æ¥åŠ é€Ÿã€‚   

IP-Adapter å›¾åƒåµŒå…¥å’Œå±è”½ embeddings and masking     
æ‰€æœ‰æ”¯æŒ IP é€‚é…å™¨çš„ç®¡é“éƒ½æ¥å— ip_adapter_image_embeds å‚æ•°ã€‚    
æˆ‘ä»¬è¿˜å¼•å…¥äº†å¯¹æä¾›äºŒè¿›åˆ¶æ©ç çš„æ”¯æŒï¼Œä»¥æŒ‡å®šåº”å°†è¾“å‡ºå›¾åƒçš„å“ªä¸€éƒ¨åˆ†åˆ†é…ç»™ IP é€‚é…å™¨ã€‚å¯¹äºæ¯ä¸ªè¾“å…¥ IP é€‚é…å™¨å›¾åƒï¼Œå¿…é¡»æä¾›äºŒè¿›åˆ¶æ©ç å’Œ IP é€‚é…å™¨ã€‚   


åˆå¹¶ LoRA æŒ‡å—    
åˆå¹¶ LoRA æ˜¯ä¸€ç§æœ‰è¶£ä¸”å¯Œæœ‰åˆ›æ„çš„æ–¹å¼æ¥åˆ›å»ºæ–°çš„ã€ç‹¬ç‰¹çš„å›¾åƒã€‚set_adaptersDiffusers é€šè¿‡è¿æ¥ LoRA æƒé‡è¿›è¡Œåˆå¹¶çš„æ–¹æ³•æä¾›åˆå¹¶æ”¯æŒã€‚   
ç°åœ¨ï¼ŒDiffusers è¿˜æ”¯æŒadd_weighted_adapterPEFT åº“ä¸­çš„æ–¹æ³•ï¼Œè§£é”æ›´é«˜æ•ˆçš„åˆå¹¶æ–¹æ³•ï¼Œå¦‚ TIESã€DAREã€çº¿æ€§ï¼Œç”šè‡³è¿™äº›åˆå¹¶æ–¹æ³•çš„ç»„åˆï¼Œå¦‚dare_ties.    

LEDITS++   
æˆ‘ä»¬æ­£åœ¨æ·»åŠ å¯¹åä¸º LEDITS++ çš„çœŸå®å›¾åƒç¼–è¾‘æŠ€æœ¯çš„æ”¯æŒï¼šä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œæ— é™å›¾åƒç¼–è¾‘ Limitless Image Editing using Text-to-Image Models ï¼Œè¿™æ˜¯ä¸€ç§æ— å‚æ•°æ–¹æ³•ï¼Œä¸éœ€è¦å¾®è°ƒæˆ–ä»»ä½•ä¼˜åŒ–ã€‚   
ä¸ºäº†ç¼–è¾‘çœŸå®å›¾åƒï¼ŒLEDITS++ ç®¡é“é¦–å…ˆåè½¬å›¾åƒ DPM-solver++ è°ƒåº¦ç¨‹åºï¼Œè¯¥è°ƒåº¦ç¨‹åºæœ‰åŠ©äºé€šè¿‡ åªéœ€è¦ 20 ä¸ªæ‰©æ•£æ­¥éª¤è¿›è¡Œç¼–è¾‘ï¼Œä»¥å®ç°åè½¬å’Œæ¨ç†ç›¸ç»“åˆã€‚LEDITS++ æŒ‡å¯¼çš„å®šä¹‰ä½¿å…¶æ—¢åæ˜ äº†ç¼–è¾‘çš„æ–¹å‘ï¼ˆå¦‚æœæˆ‘ä»¬æƒ³è¦è¿œç¦»/é è¿‘ç¼–è¾‘æ¦‚å¿µï¼‰åˆåæ˜ äº†æ•ˆæœçš„å¼ºåº¦ã€‚è¯¥æŒ‡å—è¿˜åŒ…æ‹¬ä¸€ä¸ªä¸“æ³¨äºç›¸å…³å›¾åƒåŒºåŸŸçš„å±è”½æœ¯è¯­ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤šæ¬¡ç¼–è¾‘ï¼Œå¯ç¡®ä¿æ¯ä¸ªæ¦‚å¿µçš„ç›¸åº”æŒ‡å—æœ¯è¯­å¤§éƒ¨åˆ†ä¿æŒéš”ç¦»ï¼Œä»è€Œé™åˆ¶å¹²æ‰°ã€‚    




## åŸç†
NeurIPS 2022 å¹´ 
æ‘˜è¦   
æˆ‘ä»¬è®¤ä¸ºï¼ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„ç†è®ºå’Œå®è·µç›®å‰ä¸å¿…è¦åœ°å¤æ‚åŒ–ï¼Œå¹¶è¯•å›¾é€šè¿‡æå‡ºä¸€ä¸ªæ˜ç¡®åŒºåˆ†å…·ä½“è®¾è®¡é€‰æ‹©çš„è®¾è®¡ç©ºé—´æ¥å¼¥è¡¥è¿™ç§æƒ…å†µã€‚è¿™è®©æˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«é‡‡æ ·å’Œè®­ç»ƒè¿‡ç¨‹ä»¥åŠè¯„åˆ†ç½‘ç»œçš„é¢„å¤„ç†çš„ä¸€äº›å˜åŒ–ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„æ”¹è¿›ä½¿ CIFAR-10 åœ¨ç±»æ¡ä»¶è®¾ç½®ä¸‹çš„ FID è¾¾åˆ° 1.79ï¼Œåœ¨æ— æ¡ä»¶è®¾ç½®ä¸‹è¾¾åˆ° 1.97ï¼Œé‡‡æ ·é€Ÿåº¦æ¯”ä¹‹å‰çš„è®¾è®¡å¿«å¾—å¤šï¼ˆæ¯ä¸ªå›¾åƒ 35 ä¸ªç½‘ç»œè¯„ä¼°ï¼‰ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯æ˜å…¶æ¨¡å—åŒ–æ€§è´¨ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„è®¾è®¡æ›´æ”¹æå¤§åœ°æé«˜äº†ä¹‹å‰å·¥ä½œä¸­é¢„è®­ç»ƒè¯„åˆ†ç½‘ç»œçš„æ•ˆç‡å’Œè´¨é‡ï¼ŒåŒ…æ‹¬å°†ä¹‹å‰è®­ç»ƒçš„ ImageNet-64 æ¨¡å‹çš„ FID ä» 2.07 æé«˜åˆ°æ¥è¿‘ SOTA 1.55 ï¼Œå¹¶åœ¨ä½¿ç”¨æˆ‘ä»¬å»ºè®®çš„æ”¹è¿›è¿›è¡Œé‡æ–°è®­ç»ƒåè¾¾åˆ°æ–°çš„ SOTA 1.36ã€‚   




Tero Karras ç­‰ç ”ç©¶è€…åœ¨è®ºæ–‡ã€ŠElucidating the design space of diffusionbased generative modelsã€‹ä¸­å¯¹æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ç©ºé—´è¿›è¡Œäº†åˆ†æï¼Œå¹¶ç¡®å®šäº† 3 ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«ä¸º   
i) é€‰æ‹©å™ªå£°æ°´å¹³çš„è°ƒåº¦ï¼Œ  
ii) é€‰æ‹©ç½‘ç»œå‚æ•°åŒ–ï¼ˆæ¯ä¸ªå‚æ•°åŒ–ç”Ÿæˆä¸€ä¸ªä¸åŒçš„æŸå¤±å‡½æ•°ï¼‰ï¼Œ  
iii) è®¾è®¡é‡‡æ ·ç®—æ³•ã€‚     

æœ¬æ¥æ˜¯æƒ³ç›´æ¥consistency modelçš„ï¼Œä½†æ˜¯å‘ç°consistency modelåŸºæœ¬è¢«Karras methodå…¨æ–‡è´¯ç©¿äº†ï¼Œæ‰€ä»¥ç´¢æ€§å°±ç›´æ¥ä»Karras methodå¼€å§‹äº†  

Karras method æ¥æºäº

'' Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[C]. NIPS, 2022. '' https://arxiv.org/pdf/2206.00364.pdf

è¿™ç¯‡æ–‡ç« ä¸»è¦å¹²äº†3ä»¶äº‹æƒ…ï¼š

1. ç»™å‡ºäº†å‡ ç§diffusionæ¨¡å‹çš„é€šç”¨æ¡†æ¶ï¼Œå¹¶ä¸”å°†è¿™äº›æ¨¡å‹åˆ†æˆäº†å‡ ä¸ªéƒ¨åˆ†ï¼Œç„¶ååŠ ä»¥åˆ†æï¼Œçœ‹ä¸€çœ‹ä»€ä¹ˆæ˜¯å¯¹æ¨¡å‹å½±å“æœ€å¤§çš„ï¼Œå“ªäº›éœ€è¦è°ƒæ•´ä¹‹ç±»çš„äº‹æƒ…ã€‚

2. ç€çœ¼äºsamplingè¿‡ç¨‹ï¼Œå³å›¾åƒç”Ÿæˆéƒ¨åˆ†ï¼Œç¢ç£¨æ€ä¹ˆæœ‰æ•ˆå‡å°‘ç”Ÿæˆå›¾ç‰‡çš„æ­¥æ•°ã€‚

3. åæ€äº†ç¥ç»ç½‘ç»œç»“æ„éƒ¨åˆ†ï¼Œå¹¶ä¸”å°è¯•ä¿®æ”¹è¿™ä¸ªéƒ¨åˆ†ï¼Œä»¥ä¾¿ä»–è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚


æ€»ä½“æ¥è¯´Karrasæ–‡ç« å‘Šè¯‰æˆ‘ä»¬çš„äº‹æƒ…æ˜¯ï¼Œå¦‚æœæŠŠæ•´ä¸ªdiffusionæ¨¡å‹çš„æ¶æ„ç»Ÿä¸€èµ·æ¥ï¼Œé‚£ä¹ˆå°±å¯ä»¥å‘ç°ä¸å°‘å¯ä»¥åŠ é€Ÿæ¨¡å‹ï¼Œæå‡æ•ˆæœçš„ç‚¹ã€‚

### Consistency Model
ä¸€è‡´æ€§æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹çš„è”ç³»å¤ªç´§å¯†äº†ï¼Œè®¨è®ºä»æ‰©æ•£æ¨¡å‹ä½œä¸ºåŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹çš„æƒ³æ³•å¼€å§‹ï¼Œå› æ­¤åœ¨æˆ‘ä»¬èµ¶ä¸Šæ•…äº‹ä¹‹å‰æœ‰å¾ˆå¤šèƒŒæ™¯ã€‚

ç‰¹åˆ«æ˜¯ä»¥ä¸‹ä¸¤ç¯‡è®ºæ–‡ä¸ä¸€è‡´æ€§æ¨¡å‹å¯†åˆ‡ç›¸å…³

1. åŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹çš„åŸºäºåˆ†æ•°çš„ç”Ÿæˆå»ºæ¨¡


2. é˜æ˜åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„è®¾è®¡ç©ºé—´


å¦å¤–ï¼Œæˆ‘è®¤ä¸ºä»¥æŸç§æ–¹å¼é˜…è¯»å®ƒå¯èƒ½æ˜¯å¯ä»¥çš„ã€‚



''Consistency Models Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever In the 40th International Conference on Machine Learning, 2023.'' https://arxiv.org/pdf/2303.01469.pdf

Consistency Modelçš„ç›®æ ‡æ˜¯è®©ç›®æ ‡ç”Ÿæˆçš„é€Ÿåº¦å°½å¯èƒ½å¿«ï¼Œæ¯”å¦‚ä¸€æ­¥åˆ°ä½ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æ”¯æŒåŸºäºzero-shotçš„å›¾åƒç”Ÿæˆé—®é¢˜ï¼Œæ¯”å¦‚å›¾åƒå¡«å……ä¹‹ç±»çš„é—®é¢˜ã€‚è¿™æ˜¯å› ä¸ºConsistency Modelä¸å…‰å¯ä»¥ä»0å¼€å§‹é‡æ–°è®­ç»ƒï¼Œä¹Ÿå¯ä»¥å…ˆè·å¾—ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨è¿™ä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ­£ï¼Œä»¥æ­¤è¾¾åˆ°å®Œæˆæ—¢å®šçš„ä»»åŠ¡ã€‚

å‚è€ƒé“¾æ¥ https://zhuanlan.zhihu.com/p/630353542



é¦–å…ˆï¼Œå·²çŸ¥å¦‚æœå°†æ‰©æ•£æ¨¡å‹çš„æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥é•¿å¢åŠ åˆ°æ— ç©·å¤§ï¼Œåˆ™é€†æ‰©æ•£è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºéšæœºå¾®åˆ†æ–¹ç¨‹çš„è§£ï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºä¸ºç§°ä¸ºæ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹çš„å¸¸å¾®åˆ†æ–¹ç¨‹çš„è§£ï¼ˆSong et ç­‰ï¼Œ2021 å¹´ï¼‰ã€‚

ç„¶ååœ¨æŸä¸ªæ—¶é—´
ï¿½
t
æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF ODEï¼‰çš„æ±‚è§£è½¨è¿¹å¦‚ä¸‹ã€‚









### è¿›ä¸€æ­¥å­¦ä¹ 
æ–‡ç« ç”¨ä¸€ç§æ–°çš„è®¾è®¡æ¡†æ¶ç»Ÿä¸€diffusion-based modelï¼Œå¹¶ä½¿ç”¨æ¨¡å—åŒ–ï¼ˆmodularï¼‰çš„æ€æƒ³ï¼Œåˆ†åˆ«ä»é‡‡æ ·ã€è®­ç»ƒã€score networkè®¾è®¡ä¸‰ä¸ªæ–¹é¢åˆ†æå’Œæ”¹è¿›diffusion-based modelã€‚    
ä¹‹å‰çš„å·¥ä½œ1å·²ç»æŠŠdiffusion-based modelç»Ÿä¸€åˆ°SDEæˆ–è€…ODEæ¡†æ¶ä¸‹äº†ï¼Œè¿™ç¯‡æ–‡ç« çš„ä½œè€…åŒæ ·ä¹Ÿä»SDEå’ŒODEçš„è§’åº¦å‡ºå‘ï¼Œä¸è¿‡æ¢äº†ä¸€ç§SDEå’ŒODEçš„è¡¨ç¤ºå½¢å¼ã€‚  

åŸè®ºæ–‡å…¬å¼è¿‡å¤š   













## æµ‹è¯•
æµ‹è¯• playgroundai/playground-v2.5-1024px-aesthetic   
æ¨ç†fp16   
æ˜¾å­˜å ç”¨   
åŠ è½½æ¨¡å‹æ¯”è¾ƒä¹…ï¼Œå°†è¿‘å››åˆ†é’Ÿ    
æ¨¡å‹åŠ è½½8gå·¦å³   
50æ­¥ç”¨æ—¶   

è¿‡ç¨‹é—®é¢˜   
åœ¨åº•å±‚è®¡ç®—å·ç§¯æ—¶æŠ¥é”™è¿”å›   
æŠ¥é”™invalid argument   
torchå’Œtorchvisionä¸åŒ¹é…   
æ²¡æœ‰æŒ‰ç…§torchå®˜ç½‘å®‰è£…  

æŒ‰ç…§å®˜ç½‘è£…torchåç»ˆäºè¿è¡ŒæˆåŠŸ     
å…¶å®è¿˜æ˜¯æŒºå¿«   


    from diffusers import DiffusionPipeline
    import torch

    model_path = "/models/playground-v2.5-1024px-aesthetic/"
    #model_path = "/home/WujieAITeam/private/dj/models/playground-v2.5-1024px-aesthetic/playground-v2.5-1024px-aesthetic.fp16.safetensors"

    pipe = DiffusionPipeline.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        variant="fp16",
    ).to("cuda")

    # # Optional: Use DPM++ 2M Karras scheduler for crisper fine details
    # from diffusers import EDMDPMSolverMultistepScheduler
    # pipe.scheduler = EDMDPMSolverMultistepScheduler()

    prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
    image = pipe(prompt=prompt, num_inference_steps=50, guidance_scale=3).images[0]

    image.save("/home/WujieAITeam/private/lujunda/infer-pics/playground-v2.5/" + prompt[:10] + ".png")

1024*1024  1.4mb   
![alt text](assets/README/i.png)
![alt text](assets/README/WeChatbb6ba74337cd6941194bd0c10d955424.jpg)

prompt = "A Girl with brown hair with a ponytail, With a light brown shirt from the 80s, with short pants with Brown suspenders with colorful buttons, With black tights with light brown sneakers in a classic tone,"   
negative_prompt = "watermark, low quality, cloned face, ugly, poorly drawn hands, extra limbs, missing legs, (bad body), (signature), (watermark), (username), blurry, cropped, (text), too many fingers, long neck,lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, bad feet,{{poorly drawn hands}}, {{poorly drawn face}}, {{{mutation}}}, {{{deformed}}}, {{ugly}}, blurry, {{bad anatomy}}, {{{bad proportions}}}, {{extra limbs}}, cloned face, {{{disfigured}}}, {{{more than 2 nipples}}}, {{{adult}}}, out of frame, ugly, extra limbs, {bad anatomy}, gross proportions, {malformed limbs}, {{missing arms}}, {{missing legs}}, {{{extra arms}}}, {{{extra legs}}}, mutated hands, {fused fingers}, {too many fingers}, (((long neck:1.3)), missing fingers, extra digit, fewer digits, bad feet, sideways, side view portrait, no photo frame, ((long length neck:1.5))"   

æ˜¾å­˜14g    
è€—æ—¶ä¸¤åˆ†é’Ÿ   
1024*1024   
1.18mb   
![alt text](<assets/README/A Girl wit.png>)   
![alt text](assets/README/WeChat026f416f5b1158f6b047312b66e991fd.jpg)    


é‡‡ç”¨â€œwebui æ•ˆæœæŠ–åŠ¨â€ prompt      
åŸå›¾   
![alt text](assets/README/image.png)     
![alt text](assets/README/image-1.png)    







## è®­ç»ƒ
è®­ç»ƒ dreambooth_lora      
å…¶åŸç†å’Œç‰¹ç‚¹åœ¨äºdreamboothå¾®è°ƒunet,åŒæ—¶è®­ç»ƒloraå±‚     
è¿˜ç”¨loraå¾®è°ƒtext encoders   
Training with text encoder(s)   
Alongside the UNet, LoRA fine-tuning of the text encoders is also supported.    

ä½¿ç”¨edm    
è®¡åˆ’é‡‡ç”¨ /diffusers-main/examples/advanced_diffusion_training    
æˆ–è€… diffusers-main/examples/dreambooth/train_dreambooth_lora_sdxl.py     

### å•çº¯advanced
ä¼šä½¿ç”¨   
__main__ - list of token identifiers: ['TOK']  
validation prompt: a \<s0>\<s1> icon of an astronaut riding a horse, in the style of \<s0>\<s1>   
æ‰€è°“text_inverseçš„æ–¹æ³•åŠ å…¥åˆ°dreambooth_loraä¸­è®­ç»ƒ     
æ¢è½´å¾®è°ƒ (Pivotal Tuning) ,å…³é”®è°ƒæ•´, å°†æ–‡æœ¬åè½¬ä¸å¸¸è§„æ‰©æ•£å¾®è°ƒç›¸ç»“åˆ - æˆ‘ä»¬å°†æ–°æ ‡è®°æ’å…¥æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼Œè€Œä¸æ˜¯é‡ç”¨ç°æœ‰æ ‡è®°ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼˜åŒ–æ–°æ’å…¥çš„ä»¤ç‰ŒåµŒå…¥æ¥è¡¨ç¤ºæ–°æ¦‚å¿µã€‚    

é™¤äº† UNet ä¹‹å¤–ï¼Œè¿˜æ”¯æŒ LoRA æ–‡æœ¬ç¼–ç å™¨å¾®è°ƒã€‚   

optimizer: for this example, we'll use prodigy - an adaptive optimizer   
pivotal tuning   
min SNR gamma   
æˆ‘ä»¬æŠŠ Replicate åœ¨ SDXL Cog è®­ç»ƒå™¨ä¸­ä½¿ç”¨çš„æ¢è½´å¾®è°ƒ (Pivotal Tuning) æŠ€æœ¯ä¸ Kohya è®­ç»ƒå™¨ä¸­ä½¿ç”¨çš„ Prodigy ä¼˜åŒ–å™¨ç›¸ç»“åˆï¼Œå†åŠ ä¸Šä¸€å †å…¶ä»–ä¼˜åŒ–ï¼Œä¸€èµ·å¯¹ SDXL è¿›è¡Œ Dreambooth LoRA å¾®è°ƒï¼Œå–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚    

ä½¿ç”¨ Dreambooth LoRA å¾®è°ƒåçš„ Stable Diffusion XL(SDXL) æ¨¡å‹ä»…éœ€å€ŸåŠ©å°‘é‡å›¾åƒå³å¯æ•è·æ–°æ¦‚å¿µï¼ŒåŒæ—¶ä¿ç•™äº† SDXL å‡ºå›¾ç¾è§‚é«˜è´¨çš„ä¼˜åŠ¿ã€‚   








#### ç¬¬ä¸€æ¬¡å¯åŠ¨è®­ç»ƒ    
æ²¡æœ‰ä½¿ç”¨dora    

ç”¨äº†è¿ä¸ä¸Šç½‘  --push_to_hub   
éœ€è¦å‡çº§peftä½¿ç”¨dora å®‰è£…prodigyopt     


å ç”¨22gæ˜¾å­˜   
è®­ç»ƒæ—¶é—´åŠä¸ªå°æ—¶  

åŸå§‹   
![alt text](assets/README/image-2.png)    
```
è®­ç»ƒå‘½ä»¤
accelerate launch train_dreambooth_lora_sdxl_advanced.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --pretrained_vae_model_name_or_path=$VAE_PATH \
  --dataset_name=$DATASET_NAME \
  --instance_prompt="3d icon in the style of TOK" \
  --validation_prompt="a TOK icon of an astronaut riding a horse, in the style of TOK" \
  --output_dir=$OUTPUT_DIR \
  --caption_column="prompt" \
  --mixed_precision="bf16" \
  --resolution=1024 \
  --train_batch_size=3 \
  --repeats=1 \
  --report_to="wandb"\
  --gradient_accumulation_steps=1 \
  --gradient_checkpointing \
  --learning_rate=1.0 \
  --text_encoder_lr=1.0 \
  --optimizer="prodigy"\
  --train_text_encoder_ti\
  --train_text_encoder_ti_frac=0.5\
  --snr_gamma=5.0 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --rank=8 \
  --max_train_steps=1000 \
  --checkpointing_steps=2000 \
  --seed="0" \


Loaded scheduler as EulerDiscreteScheduler 
Num examples = 22
03/27/2024 07:07:33 - INFO - __main__ -   Num batches each epoch = 8
03/27/2024 07:07:33 - INFO - __main__ -   Num Epochs = 125
03/27/2024 07:07:33 - INFO - __main__ -   Instantaneous batch size per device = 3
03/27/2024 07:07:33 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 3
03/27/2024 07:07:33 - INFO - __main__ -   Gradient Accumulation steps = 1
03/27/2024 07:07:33 - INFO - __main__ -   Total optimization steps = 1000

Loaded scheduler as EulerDiscreteScheduler è¿›è¡Œ validation æ¨ç†

```

The weights were trained using DreamBooth.

LoRA for the text encoder was enabled: False.

Pivotal tuning was enabled: True.

Special VAE used for training: madebyollin/sdxl-vae-fp16-fix.  

Trigger words    
To trigger image generation of trained concept(or concepts) replace each concept identifier in you prompt with the new inserted tokens:    

to trigger concept TOK-> use <s0><s1> in your prompt



####   ç¬¬äºŒæ¬¡å¯åŠ¨è®­ç»ƒ
ä½¿ç”¨ dora    
åŠ å¤§è®­ç»ƒè½®æ¬¡3000    
ä¾æ—§æ˜¯22gæ˜¾å­˜    
ä¸¤ä¸ªå°æ—¶   






#### æ¨ç†    
ä½¿ç”¨ä¸Šè¿°æŠ€æœ¯è®­ç»ƒçš„æ¨¡å‹çš„æ¨ç†åº”è¯¥ä¸ä»»ä½•å…¶ä»–è®­ç»ƒå™¨è®­å¾—çš„æ¨¡å‹çš„æ¨ç†æ–¹å¼ç›¸åŒï¼Œ      
ä¸åŒä¹‹å¤„åœ¨äºï¼Œå½“æˆ‘ä»¬è¿›è¡Œæ¢è½´å¾®è°ƒæ—¶ï¼Œé™¤äº† LoRA çš„ *.safetensors æƒé‡ä¹‹å¤–ï¼Œè¿˜æœ‰ç»è¿‡è®­ç»ƒçš„æ–°è¯å…ƒåŠå…¶æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„ *.safetensors ã€‚there is also the *.safetensors text embeddings trained with the model for the new tokens. .ä¸ºäº†å¯¹è¿™äº›è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬åœ¨åŠ è½½ LoRA æ¨¡å‹çš„æ–¹å¼ä¸ŠåŠ äº† 2 ä¸ªæ­¥éª¤:       

![alt text](assets/README/image-3.png)
![alt text](assets/README/llama2.png)

å°†åµŒå…¥æ¨¡å‹åŠ è½½åˆ°æ–‡æœ¬ç¼–ç å™¨ä¸­

    # load embeddings to the text encoders
    state_dict = load_file(embedding_path)
    è¿™é‡Œæ˜¯17KB

    # notice we load the tokens <s0><s1>, as "TOK" as only a place-holder and training was performed using the new initialized   tokens - <s0><s1>
    # load embeddings of text_encoder 1 (CLIP ViT-L/14)
    pipe.load_textual_inversion(state_dict["clip_l"], token=["<s0>", "<s1>"], text_encoder=pipe.text_encoder, tokenizer=pipe.  tokenizer)
    # load embeddings of text_encoder 2 (CLIP ViT-G/14)
    pipe.load_textual_inversion(state_dict["clip_g"], token=["<s0>", "<s1>"], text_encoder=pipe.text_encoder_2, tokenizer=pipe.  tokenizer_2)

åŠ è½½ä½ çš„ LoRA     
è¿™é‡Œæ˜¯24MB     

    # normal LoRA loading
    pipe.load_lora_weights("LinoyTsaban/web_y2k_lora", weight_name="pytorch_lora_weights.safetensors")
    prompt="a <s0><s1> webpage about an astronaut riding a horse"
    images = pipe(
        prompt,
        cross_attention_kwargs={"scale": 0.8},
    ).images
    # your output image
    images[0]


cross_attention_kwargs={"scale": 1.0}?????
loraæƒé‡å¦‚ä½•å½±å“ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ  



#### æ€»ç»“
2024.1.2æ–‡ç« LoRA training scripts of the world, unite!å‘è¡¨  
åæ¥diffuseræ”¯æŒè¯¥advancedè®­ç»ƒ  

advancedè®­ç»ƒåŸç†ï¼š    
ä¼˜åŒ–å™¨prodigyï¼Œæ ‡é…å­¦ä¹ ç‡1ï¼Œä¸”æ–‡æœ¬ç¼–ç å™¨å’Œunetå­¦ä¹ ç‡å¿…é¡»ç›¸åŒ    
è®­ç»ƒunetçš„loraï¼Œä¸ç¡®å®šæ–‡æœ¬ç¼–ç å™¨æ˜¯å¦è®­ç»ƒlora  
dreamboothæ–¹å¼è®­ç»ƒï¼Œç»™æ–‡æœ¬ï¼ˆç½•è§è¯ï¼‰å’Œå›¾ç‰‡ä¸€èµ·å»è®­ç»ƒ  
ç½•è§è¯æ›¿æ¢è®¾ç½®ï¼Œæ‰€è°“pivotal finetune   
å¯ä»¥è®¾ç½®doraè®­ç»ƒ    
ç§©é»˜è®¤8     




### advanced + edm è®­ç»ƒ
EDM å¼è®­ç»ƒå°šä¸æ”¯æŒ Min-SNR gammaã€‚    





# advance åŸºç¡€çŸ¥è¯†
all of them have been incorporated into the new diffusers training script.

åŒ…æ‹¬: Nataniel Ruiz çš„ Dreamboothã€ Rinon Gal çš„ æ–‡æœ¬é€†åŒ– (textual inversion) ã€  
Ron Mokady çš„ æ¢è½´å¾®è°ƒã€Simo Ryu çš„ cog-sdxlã€  
Kohya çš„ sd-scripts   
ä»¥åŠ The Last Ben çš„ fast-stable-diffusionã€‚    

Replicate åœ¨ SDXL Cog è®­ç»ƒå™¨ä¸­ä½¿ç”¨çš„æ¢è½´å¾®è°ƒ (Pivotal Tuning) æŠ€æœ¯Pivoting halfway    
ä¸ Kohya è®­ç»ƒå™¨ä¸­ä½¿ç”¨çš„ Prodigy ä¼˜åŒ–å™¨    

æŒ‰æˆ‘ç†è§£ textual inversion å’Œ dreambooth éƒ½æ˜¯ç‰¹å®šipçš„å­¦ä¹      
textual inversionè®­ç»ƒæ–°æ–‡æœ¬.just optimizing the inserted embeddings         
loraä½¿ç”¨loraå±‚é£æ ¼è¿ç§»    
dreamboothä½¿ç”¨ç½•è§è¯å¾®è°ƒunet    

## dreambooth

DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation      
å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹--ä¸»é¢˜é©±åŠ¨ç”Ÿæˆ
We include a file dataset/prompts_and_classes.txt which contains all of the prompts used in the paper for live subjects and objects, as well as the class name used for the subjects.

è¯¥æ•°æ®é›†åŒ…æ‹¬ 15 ä¸ªä¸åŒç±»åˆ«çš„ 30 ä¸ªç§‘ç›®ã€‚å…¶ä¸­ 9 ä¸ªæ˜¯æ´»ä½“ä¸»ä½“ï¼ˆç‹—å’ŒçŒ«ï¼‰ï¼Œ21 ä¸ªæ˜¯ç‰©ä½“ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¯ä¸ªä¸»é¢˜çš„å¯å˜æ•°é‡çš„å›¾åƒ (4-6)ã€‚æ‹æ‘„å¯¹è±¡çš„å›¾åƒé€šå¸¸æ˜¯åœ¨ä¸åŒæ¡ä»¶ã€ç¯å¢ƒå’Œä¸åŒè§’åº¦ä¸‹æ‹æ‘„çš„ã€‚

![alt text](assets/README/image-8.png)    
![alt text](assets/README/image-9.png)      
![alt text](assets/README/image-10.png)     




## æœ€æ—©çš„loraå¾®è°ƒ    
ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ      

è¿™ä¸ªå¥½åƒæ˜¯DreamBooth LoRAå¾®è°ƒ   
https://aistudio.baidu.com/projectdetail/5704542?contributionType=1   
sd1.5   
prompt: "a photo of sks dog in a bucket"  
![alt text](assets/README/image-4.png)  

dogsæ–‡ä»¶å¤¹  
![alt text](assets/README/image-5.png)  
æ–‡æœ¬ä¿¡æ¯ç»Ÿä¸€ä¸ºinstance_prompt="a photo of sks dog"  
å•å¼ å›¾ç‰‡å¯¹åº”å¤šå›¾  






## Pivotal tuning
Dreamboothå¾®è°ƒ å‡çº§ç‰ˆ   

ä»¥ Dreambooth ä¸ºä¾‹ï¼Œè¿›è¡Œå¸¸è§„ Dreambooth å¾®è°ƒæ—¶ï¼Œä½ éœ€è¦é€‰æ‹©ä¸€ä¸ªç¨€æœ‰è¯å…ƒä½œä¸ºè§¦å‘è¯ï¼Œä¾‹å¦‚â€œä¸€åª sks ç‹—â€ ä¸­çš„ sks ã€‚ä½†æ˜¯ï¼Œå› ä¸ºè¿™äº›è¯å…ƒåŸæœ¬å°±æ¥è‡ªäºè¯è¡¨ï¼Œæ‰€ä»¥å®ƒä»¬é€šå¸¸æœ‰è‡ªå·±çš„åŸä¹‰ï¼Œè¿™å°±æœ‰å¯èƒ½ä¼šå½±å“ä½ çš„ç»“æœã€‚ä¸¾ä¸ªä¾‹å­ï¼Œç¤¾åŒºä¹‹å‰ç»å¸¸ä½¿ç”¨ sks ä½œä¸ºè§¦å‘è¯ï¼Œä½†å®é™…ä¸Šå…¶åŸä¹‰æ˜¯ä¸€ä¸ªæ­¦å™¨å“ç‰Œã€‚   

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬`æ’å…¥ä¸€ä¸ªæ–°è¯å…ƒ\<s0>\<s1>åˆ°æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼Œè€Œéé‡ç”¨è¯è¡¨ä¸­ç°æœ‰çš„è¯å…ƒ`ã€‚ç„¶åï¼Œæˆ‘ä»¬`ä¼˜åŒ–æ–°æ’å…¥è¯å…ƒçš„åµŒå…¥å‘é‡æ¥è¡¨ç¤ºæ–°æ¦‚å¿µï¼Œè¿™ç§æƒ³æ³•å°±æ˜¯æ–‡æœ¬é€†åŒ–`ï¼Œå³æˆ‘ä»¬å¯¹åµŒå…¥ç©ºé—´ä¸­çš„æ–°è¯å…ƒè¿›è¡Œå­¦ä¹ æ¥è¾¾åˆ°å­¦ä¹ æ–°æ¦‚å¿µçš„ç›®çš„ã€‚ä¸€æ—¦æˆ‘ä»¬è·å¾—äº†æ–°è¯å…ƒåŠå…¶å¯¹åº”çš„åµŒå…¥å‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¿™äº›è¯å…ƒåµŒå…¥å‘é‡æ¥è®­ç»ƒæˆ‘ä»¬çš„ Dreambooth LoRAï¼Œä»¥è·å¾—ä¸¤å…¨å…¶ç¾çš„æ•ˆæœã€‚    
We then optimize the newly-inserted token embeddings to represent the new concept: that is Textual Inversion â€“ we learn to represent the concept through new "words" in the embedding space. Once we obtain the new token and its embeddings to represent it, we can train our Dreambooth LoRA with those token embeddings to get the best of both worlds.    

    --train_text_encoder_ti
    --train_text_encoder_ti_frac=0.5
    --token_abstraction="TOK"
    --num_new_tokens_per_abstraction=2
    --adam_weight_decay_text_encoder

train_text_encoder_ti å¼€å¯æ–‡æœ¬é€†åŒ–è®­ç»ƒï¼Œç”¨äºè®­ç»ƒæ–°æ¦‚å¿µçš„åµŒå…¥å‘é‡ã€‚   
train_text_encoder_ti_frac æŒ‡å®šä½•æ—¶åœæ­¢æ–‡æœ¬é€†åŒ– (å³åœæ­¢æ–‡æœ¬åµŒå…¥å‘é‡çš„æ›´æ–°ï¼Œä»…ç»§ç»­æ›´æ–° UNet.i.e. stop optimization of the textual embeddings and continue optimizing the UNet only)ã€‚ä¸­é€”å®šè½´Pivoting halfway  (å³ä»…åœ¨è®­ç»ƒå‰åŠéƒ¨åˆ†æ‰§è¡Œæ–‡æœ¬é€†åŒ–) æ˜¯ cog sdxl ä½¿ç”¨çš„é»˜è®¤è®¾ç½®ï¼Œæˆ‘ä»¬ç›®å‰çš„å®éªŒä¹ŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é¼“åŠ±å¤§å®¶å¯¹æ­¤å‚æ•°è¿›è¡Œæ›´å¤šå®éªŒã€‚     
token_abstraction å³æ¦‚å¿µæ ‡è¯†ç¬¦concept identifierï¼Œæˆ‘ä»¬åœ¨æç¤ºæ–‡æœ¬ä¸­ç”¨è¯¥è¯æè¿°æˆ‘ä»¬å¸Œæœ›è®­ç»ƒçš„æ¦‚å¿µã€‚è¯¥æ ‡è¯†ç¬¦è¯å…ƒä¼šç”¨åœ¨ instance prompt, validation prompt or custom captions.ä¸­ã€‚è¿™é‡Œæˆ‘ä»¬é€‰æ‹© TOK ä½œä¸ºæ¦‚å¿µæ ‡è¯†ç¬¦ï¼Œå¦‚ â€œTOK çš„ç…§ç‰‡â€å³ä¸ºä¸€ä¸ªå«æœ‰æ¦‚å¿µæ ‡è¯†ç¬¦çš„å®ä¾‹æç¤º "a photo of a TOK" can be the instance promptã€‚æ³¨æ„ï¼Œ--token_abstraction åªæ˜¯ä¸€ä¸ªå ä½ç¬¦place-holderï¼Œå› æ­¤ï¼Œåœ¨è®­ç»ƒä¹‹å‰æˆ‘ä»¬éœ€è¦ç”¨ä¸€ä¸ªæ–°è¯å…ƒnew tokens æ¥ä»£æ›¿ TOK å¹¶å¯¹å…¶è¿›è¡Œè®­ç»ƒ (ä¸¾ä¸ªä¾‹å­ï¼Œè®­ç»ƒæ—¶â€œä¸€å¼  TOK çš„ç…§ç‰‡â€ä¼šå˜æˆâ€œä¸€å¼  <s0><s1> çš„ç…§ç‰‡â€ï¼Œå…¶ä¸­ <s0><s1> å°±æ˜¯æ–°è¯å…ƒ)ã€‚åŒæ ·åœ°ï¼Œéœ€è¦ç¡®ä¿è¿™é‡Œçš„ token_abstraction ä¸å®ä¾‹æç¤ºã€éªŒè¯æç¤ºå’Œè‡ªå®šä¹‰æç¤º (å¦‚æœ‰) ä¸­çš„æ ‡è¯†ç¬¦ç›¸ä¸€è‡´ã€‚    
num_new_tokens_per_abstraction è¡¨ç¤ºæ¯ä¸ª token_abstraction å¯¹åº”å¤šå°‘ä¸ªæ–°è¯å…ƒ the number of new tokens to initialize for each token_abstraction- - å³éœ€è¦å‘æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨æ’å…¥å¤šå°‘ä¸ªæ–°è¯å…ƒå¹¶å¯¹å…¶è¿›è¡Œè®­ç»ƒhow many new tokens to insert and train for each text encoder of the modelã€‚é»˜è®¤è®¾ç½®ä¸º 2ï¼Œæˆ‘ä»¬é¼“åŠ±å¤§å®¶å¯¹ä¸åŒå–å€¼è¿›è¡Œå®éªŒå¹¶åˆ†äº«ä½ ä»¬çš„å‘ç°ï¼   
adam_weight_decay_text_encoder ç”¨äºä¸ºæ–‡æœ¬ç¼–ç å™¨è®¾ç½®ä¸ UNet ä¸åŒçš„æƒé‡è¡°å‡ã€‚   







## Min-SNR Gamma weighting
Efficient Diffusion Training via Min-SNR Weighting Strategy   
2023.03   

è®­ç»ƒæ‰©æ•£æ¨¡å‹é€šå¸¸ä¼šé‡åˆ°æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ï¼Œéƒ¨åˆ†æ˜¯ç”±äºå„æ—¶é—´æ­¥ä¹‹é—´çš„ä¼˜åŒ–æ–¹å‘ç›¸äº’å†²çªã€‚Hang ç­‰äºº é€šè¿‡å¼•å…¥ç®€å•çš„æœ€å°ä¿¡å™ªæ¯” Gamma æ³• simple Min-SNR-gamma approach æ¥ç¼“è§£æ­¤é—®é¢˜ã€‚     
--snr_gamma=5.0  \    

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†æ‰©æ•£è®­ç»ƒè§†ä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼Œå¹¶å¼•å…¥ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸º Min-SNR-Î³ã€‚    

æ–¹æ³•æ ¹æ®é’³ä½ä¿¡å™ªæ¯”è°ƒæ•´æ—¶é—´æ­¥é•¿çš„æŸå¤±æƒé‡ï¼Œæœ‰æ•ˆå¹³è¡¡æ—¶é—´æ­¥é•¿ä¹‹é—´çš„å†²çªã€‚
This method adapts loss weights of timesteps based on clamped signal-to-noise ratios, which effectively balances the conflicts among timesteps.

3.4Ã— faster than previous weighting strategies.

For small datasets, the effects of Min-SNR weighting strategy might not appear to be pronounced, but for larger datasets, the effects will likely be more pronounced.     

![alt text](assets/README/image-7.png)

é»˜è®¤æƒ…å†µä¸‹ --snr_gamma=None ï¼Œå³ä¸å¯ç”¨ã€‚å¯ç”¨ --snr_gamma æ—¶ï¼Œå»ºè®®å–å€¼ä¸º 5.0ã€‚








## prodigy å¿«é€Ÿè‡ªé€‚åº”æ— å‚æ•°å­¦ä¹ å™¨

An Expeditiously Adaptive Parameter-Free Learner    


Adaptive Optimizers   
![alt text](assets/README/image-6.png)    
æ”¶æ•›æ„å‘³ç€æˆ‘ä»¬é€‰æ‹©çš„æŸå¤±å‡½æ•°è¾¾åˆ°äº†æœ€å°å€¼ï¼Œæˆ‘ä»¬è®¤ä¸ºæŸå¤±å‡½æ•°è¾¾åˆ°æœ€å°å€¼å³è¯´æ˜æ¨¡å‹å·²ä¹ å¾—æˆ‘ä»¬æƒ³è¦æ•™ç»™å®ƒçš„å†…å®¹ã€‚å½“å‰ï¼Œæ·±åº¦å­¦ä¹ ä»»åŠ¡çš„æ ‡å‡† (ä¹Ÿæ˜¯æœ€å…ˆè¿›çš„) ä¼˜åŒ–å™¨å½“å± Adam å’Œ AdamW ä¼˜åŒ–å™¨ã€‚     
ç„¶è€Œï¼Œè¿™ä¸¤ä¸ªä¼˜åŒ–å™¨è¦æ±‚ç”¨æˆ·è®¾ç½®å¤§é‡çš„è¶…å‚ (å¦‚å­¦ä¹ ç‡ã€æƒé‡è¡°å‡ç­‰)ï¼Œä»¥æ­¤ä¸ºæ”¶æ•›é“ºå¹³é“è·¯ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬éœ€è¦ä¸æ–­è¯•éªŒå„ç§è¶…å‚ï¼Œæœ€åå¸¸å¸¸å› ä¸ºè€—æ—¶è¿‡é•¿è€Œä¸å¾—ä¸é‡‡ç”¨æ¬¡ä¼˜è¶…å‚ï¼Œä»è€Œå¯¼è‡´æ¬¡ä¼˜ç»“æœã€‚   

å³ä½¿ä½ æœ€åè¯•åˆ°äº†ç†æƒ³çš„å­¦ä¹ ç‡ï¼Œä½†å¦‚æœå­¦ä¹ ç‡åœ¨è®­ç»ƒæœŸé—´ä¿æŒä¸ºå¸¸æ•°ï¼Œä»ç„¶å¯èƒ½å¯¼è‡´æ”¶æ•›é—®é¢˜ã€‚      
ä¸€äº›è¶…å‚å¯èƒ½éœ€è¦é¢‘ç¹çš„æ›´æ–°ä»¥åŠ é€Ÿæ”¶æ•›ï¼Œè€Œå¦ä¸€äº›è¶…å‚çš„è°ƒæ•´åˆä¸èƒ½å¤ªå¤§ä»¥é˜²æ­¢æŒ¯è¡ã€‚    

æˆ‘ä»¬å¼•å…¥äº†æœ‰è‡ªé€‚åº”å­¦ä¹ ç‡çš„ç®—æ³•ï¼Œä¾‹å¦‚ Adafactor å’Œ Prodigyã€‚è¿™äº›æ–¹æ³•æ ¹æ®æ¯ä¸ªå‚æ•°è¿‡å»çš„æ¢¯åº¦æ¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œå€Ÿæ­¤æ¥ä¼˜åŒ–ç®—æ³•å¯¹æœç´¢ç©ºé—´çš„éå†è½¨è¿¹ã€‚

å¯¹æ‰©æ•£æ¨¡å‹ç‰¹åˆ«æ˜¯å…¶ LoRA è®­ç»ƒæœ‰ç”¨çš„è®¾ç½®è¿˜æœ‰:

    --prodigy_safeguard_warmup=True
    --prodigy_use_bias_correction=True
    --adam_beta1=0.9
    # æ³¨æ„ï¼Œä»¥ä¸‹è¶…å‚çš„å–å€¼ä¸é»˜è®¤å€¼ä¸åŒ:
    --adam_beta2=0.99
    --adam_weight_decay=0.01

åœ¨ä½¿ç”¨ Prodigy è¿›è¡Œè®­ç»ƒæ—¶ï¼Œä½ è¿˜å¯ä»¥å¯¹å…¶ä»–è¶…å‚è¿›è¡Œè°ƒæ•´ (å¦‚: --prodigy_beta3 ã€prodigy_de Couple ã€prodigy_safeguard_warmup )


é»˜è®¤æƒ…å†µä¸‹ï¼ŒProdigy ä½¿ç”¨ AdamW ä¸­çš„æƒé‡è¡°å‡ã€‚å¦‚æœä½ æƒ³è®©å®ƒä½¿ç”¨æ ‡å‡†l2æ­£åˆ™åŒ–ï¼ˆå¦‚ Adamæ‰€ä½¿ç”¨ï¼‰ç”¨é€‰é¡¹decouple=False      








## å½±å“ LoRA æ¨¡å‹è´¨é‡çš„å…¶ä»–æŠ€å·§
### æ–‡æœ¬ç¼–ç å™¨lr
The importance of different unet and text encoder learning rates is evident when performing pivotal tuning as well- in this case, setting a higher learning rate for the text encoder is perceived to be better.    
åœ¨ä½¿ç”¨Pivotal Tuningæ—¶ï¼Œä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡æ˜¯å·²è¢«è¯æ˜äº†çš„ - æ­¤æ—¶ï¼Œä¸ºæ–‡æœ¬ç¼–ç å™¨è®¾ç½®æ›´é«˜çš„å­¦ä¹ ç‡æ›´å¥½ã€‚

å½“ä½¿ç”¨ Prodigy (æˆ–ä¸€èˆ¬çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨) æ—¶ï¼Œæˆ‘ä»¬ä¸€å¼€å§‹å¯ä»¥è®©æ‰€æœ‰è®­ç»ƒå‚æ•°çš„åˆå§‹å­¦ä¹ ç‡ç›¸åŒï¼Œè®©ä¼˜åŒ–å™¨è‡ªè¡Œè°ƒèŠ‚    

åœ¨ä¼˜åŒ–æ–‡æœ¬ç¼–ç å™¨æ—¶ï¼Œä¸ºå…¶è®¾ç½®ä¸“æœ‰çš„å­¦ä¹ ç‡ (ä¸ UNet å­¦ä¹ ç‡ç‹¬ç«‹) æ‰€å¾—æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ä¼šæ›´å¥½ - ç‰¹åˆ«åœ°ï¼Œæ–‡æœ¬ç¼–ç å™¨éœ€è¦ è¾ƒä½ çš„å­¦ä¹ ç‡ï¼Œå› ä¸ºå®ƒä¸€èˆ¬è¿‡æ‹Ÿåˆ æ›´å¿« ã€‚   

    --train_text_encoder
    --learning_rate=1e-4 #unet
    --text_encoder_lr=5e-5
--train_text_encoder å¯ç”¨æ–‡æœ¬ç¼–ç å™¨å…¨æ¨¡å‹è®­ç»ƒ (å³æ–‡æœ¬ç¼–ç å™¨çš„æƒé‡éƒ½å‚ä¸æ›´æ–°ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¼˜åŒ–æ–‡æœ¬é€†åŒ–æ–°è¯å…ƒçš„åµŒå…¥ as opposed to just optimizing the inserted embeddings we saw in textual inversion ( --train_text_encoder_ti ))ã€‚å¦‚æœä½ å¸Œæœ›æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡å§‹ç»ˆä¸ --learning_rate ä¸€è‡´ï¼Œå¯è®¾ --text_encoder_lr=None ã€‚



### Custom Captioning è‡ªå®šä¹‰æè¿°æ–‡æœ¬
è¿™å±äºå‡çº§æŠ€å·§    
ä¸åœ¨ä¼ ç»Ÿloraæˆ–è€…dreamboothè®­ç»ƒä¸­ä½¿ç”¨   


While it is possible to achieve good results by training on a set of images all captioned with the same instance prompt, e.g. "photo of a person" or "in the style of " etc, using the same caption may lead to suboptimal results, depending on the complexity of the learned concept, how "familiar" the model is with the concept, and how well the training set captures it.
å…·ä½“æ•ˆæœå–å†³äºå¾ˆå¤šå› ç´ ï¼ŒåŒ…æ‹¬å¾…å­¦ä¹ æ¦‚å¿µçš„å¤æ‚æ€§ã€æ¨¡å‹åŸæœ¬å¯¹è¯¥æ¦‚å¿µçš„â€œç†Ÿæ‚‰ç¨‹åº¦â€ï¼Œä»¥åŠè®­ç»ƒé›†æ•è·æ–°æ¦‚å¿µçš„æ•ˆæœå¦‚ä½•ã€‚      

è¦åŠ è½½è‡ªå®šä¹‰å›¾åƒæè¿°æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒé›†çš„ç›®å½•ç»“æ„éµå¾ª datasets åº“çš„ ImageFolder ç»“æ„ï¼Œå…¶åŒ…å«å›¾åƒåŠæ¯å¼ å›¾åƒå¯¹åº”çš„æè¿°æ–‡æœ¬ã€‚   
æŒ‡çš„æ˜¯metadata.json   
è€Œä¸æ˜¯torchvisionçš„ImageFolderï¼Œä¸€ä¸ªæ–‡ä»¶å¤¹ä¸€ä¸ªç±»    


æ–¹å¼ 1:   
ä» hub ä¸­é€‰æ‹©ä¸€ä¸ªå·²åŒ…å«å›¾åƒåŠå…¶å¯¹åº”æç¤ºçš„æ•°æ®é›† - å¦‚ LinoyTsaban/3d_iconã€‚ç°åœ¨è¦åšçš„å°±æ˜¯åœ¨è®­ç»ƒå‚æ•°ä¸­æŒ‡å®šæ•°æ®é›†çš„åç§°åŠæ–‡æœ¬åˆ—çš„åç§° (åœ¨æœ¬ä¾‹ä¸­åˆ—åä¸º "prompt"):

    --dataset_name=LinoyTsaban/3d_icon
    --caption_column=prompt

æ–¹å¼ 2:   
ä½ è¿˜å¯ä»¥ä½¿ç”¨è‡ªå·±çš„å›¾åƒå¹¶ä¸ºå…¶æ·»åŠ æè¿°æ–‡æœ¬ã€‚æ­¤æ—¶ï¼Œä½ å¯ä»¥å€ŸåŠ© è¿™ä¸ª Colab Notebook æ¥ç”¨ BLIP è‡ªåŠ¨ä¸ºå›¾åƒç”Ÿæˆæè¿°æ–‡æœ¬ï¼Œæˆ–è€…ä½ å¯ä»¥åœ¨å…ƒæ•°æ®æ–‡ä»¶ä¸­æ‰‹åŠ¨åˆ›å»ºæè¿°æ–‡æœ¬ã€‚åé¢çš„äº‹æƒ…å°±å’Œæ–¹å¼ 1 ä¸€æ ·äº†ï¼Œå°† --dataset_name è®¾ä¸ºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œ --caption_column è®¾ä¸ºæè¿°æ–‡æœ¬æ‰€å¯¹åº”çš„åˆ—åã€‚



### Repeats å›¾åƒæ ·æœ¬é‡å¤æ¬¡æ•°
This argument refers to the number of times an image from your dataset is repeated in the training set. This differs from epochs in that first the images are repeated, and only then shuffled.     
å…¶ä¸ epoch çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œå›¾åƒé¦–å…ˆè¢«é‡å¤ï¼ˆå¤åˆ¶ï¼‰ï¼Œç„¶åæ‰è¢«æ‰“ä¹±ã€‚ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿæœ‰ä»€ä¹ˆç”¨å—ï¼Ÿï¼Ÿï¼Ÿ    



### æ•°æ®é›†é€‰æ‹©
For example, if my concept is this red backpack:    
![alt text](assets/README/image-11.png)   
I would likely want to prompt it worn by people as well, so having examples like this:    
æˆ‘è§‰å¾—åé¢ç”¨æˆ·æœ‰å¯èƒ½ç»™ä¸€ä¸ªæŸäººèƒŒç€åŒ…çš„æç¤ºï¼Œå› æ­¤è®­ç»ƒé›†ä¸­æœ€å¥½æœ‰ä¸‹é¢è¿™æ ·çš„æ ·æœ¬:   
![alt text](assets/README/image-12.png)   

è¿™æ ·å°±èƒ½åœ¨è®­ç»ƒæ—¶åŒ¹é…æ¨ç†æ—¶çš„åº”ç”¨åœºæ™¯ï¼Œå› æ­¤æ¨¡å‹æ¨ç†æ—¶å°±æ›´å®¹æ˜“å¤–æ¨è‡³è¯¥ç±»åœºæ™¯æˆ–è€…è®¾å®šã€‚      

ä¸¾ä¸ªä¾‹å­_ï¼Œ åœ¨ äººè„¸ æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œåœ¨å‡†å¤‡æ•°æ®é›†æ—¶éœ€è¦ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹å‡ ç‚¹:   
åº”å°½å¯èƒ½é€‰æ‹© é«˜åˆ†è¾¨ç‡ã€é«˜ç”»è´¨ çš„å›¾åƒã€‚æ¨¡ç³Šæˆ–ä½åˆ†è¾¨ç‡çš„å›¾åƒä¸åˆ©äºå¾®è°ƒç»“æœã€‚

åœ¨è®­ç»ƒç‰¹å®šäººç‰©çš„äººè„¸æ—¶ï¼Œå»ºè®®è®­ç»ƒé›†ä¸­ä¸è¦å‡ºç°å…¶ä»–äººçš„è„¸ï¼Œå› ä¸ºæˆ‘ä»¬ä¸æƒ³å¯¹ç›®æ ‡äººè„¸äº§ç”Ÿæ¨¡ç³Šçš„æ¦‚å¿µã€‚

ç‰¹å†™ç…§ç‰‡ å¯¹äºæœ€ç»ˆäº§ç”ŸçœŸå®æ„Ÿçš„æ•ˆæœå¾ˆé‡è¦ï¼Œä½†æ˜¯åŒæ—¶ä¹Ÿåº”è¯¥åŒ…å«å¥½çš„å…¨èº«ç…§ç‰‡ï¼Œä»¥æé«˜å¯¹ä¸åŒå§¿åŠ¿/æ„å›¾çš„æ³›åŒ–èƒ½åŠ›ã€‚

æˆ‘ä»¬å»ºè®® é¿å…çº³å…¥ç¦»æ‹æ‘„ä¸»ä½“è¾ƒè¿œçš„ç…§ç‰‡ï¼Œå› ä¸ºæ­¤ç±»å›¾åƒä¸­çš„å¤§å¤šæ•°åƒç´ ä¸æˆ‘ä»¬å¸Œæœ›ä¼˜åŒ–çš„æ¦‚å¿µæ— å…³ï¼Œæ¨¡å‹å¯ä»¥ä»ä¸­å­¦ä¹ çš„ä¸œè¥¿ä¸å¤šã€‚

é¿å…é‡å¤çš„èƒŒæ™¯/æœè£…/å§¿åŠ¿ â€”â€” åœ¨ç¯å…‰ã€å§¿åŠ¿ã€èƒŒæ™¯å’Œé¢éƒ¨è¡¨æƒ…æ–¹é¢åŠ›æ±‚ å¤šæ ·æ€§ã€‚å¤šæ ·æ€§è¶Šå¤§ï¼ŒLoRA å°±è¶Šçµæ´»ã€è¶Šé€šç”¨ã€‚

å…ˆéªŒä¿ç•™æŸå¤± `å…ˆéªŒä¿ç•™æŸå¤±æ˜¯ä¸€ç§ä½¿ç”¨æ¨¡å‹è‡ªå·±ç”Ÿæˆçš„æ ·æœ¬æ¥å¸®åŠ©å…¶å­¦ä¹ å¦‚ä½•ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›¾åƒçš„æ–¹æ³•`ã€‚ç”±äºè¿™äº›å›¾åƒæ ·æœ¬ä¸ä½ æä¾›çš„å›¾åƒå±äºåŒä¸€ç±»ï¼Œå› æ­¤å®ƒä»¬å¯ä»¥`å¸®åŠ©æ¨¡å‹ä¿ç•™å·²ä¹ å¾—çš„æœ‰å…³è¯¥ç±»çš„ä¿¡æ¯`ï¼Œå¹¶ä¹ å¾—å¦‚ä½•ä½¿ç”¨å·²çŸ¥çš„è¯¥ç±»çš„ä¿¡æ¯æ¥å½¢æˆæ–°çš„ç»„åˆã€‚

ç”¨äºæ­£åˆ™åŒ–çš„çœŸå®å›¾åƒ VS æ¨¡å‹ç”Ÿæˆçš„å›¾åƒ real images for regularization VS model generated ones     
é€‰æ‹©ç±»å›¾åƒæ—¶ï¼Œä½ å¯ä»¥åœ¨åˆæˆå›¾åƒ (å³ç”±æ‰©æ•£æ¨¡å‹ç”Ÿæˆ) å’ŒçœŸå®å›¾åƒä¹‹é—´è¿›è¡Œé€‰æ‹© When choosing class images, you can decide between synthetic ones (i.e. generated by the diffusion model) and real ones. ã€‚æ”¯æŒä½¿ç”¨çœŸå®å›¾åƒçš„ç†ç”±æ˜¯å®ƒä»¬æé«˜äº†å¾®è°ƒæ¨¡å‹çš„çœŸå®æ„Ÿã€‚å¦ä¸€æ–¹é¢ï¼Œæœ‰äº›äººä¹Ÿä¼šè®¤ä¸ºä½¿ç”¨æ¨¡å‹ç”Ÿæˆçš„å›¾åƒå¯ä»¥æ›´å¥½åœ°ä¿ç•™æ¨¡å‹ä¹ å¾—çš„ çŸ¥è¯† åŠå®¡ç¾ã€‚preserving the models knowledge of the class and general aesthetics.

åäººç›¸ - è¿™ä¸»è¦ä¸ç”¨äºè®­ç»ƒçš„æè¿°æ–‡æœ¬æˆ–å®ä¾‹æç¤ºæœ‰å½“å…³ã€‚ä½¿ç”¨â€œä»¤ç‰Œæ ‡è¯†ç¬¦ + åŸºç¡€æ¨¡å‹æ‰€çŸ¥é“çš„ä¸å¾…è®­ç»ƒä»»åŠ¡ç›¸ä¼¼çš„å…¬ä¼—äººç‰©â€è¿›è¡Œæç¤ºæ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¸€äº›æ¡ˆä¾‹çš„å¾®è°ƒæ•ˆæœå¾—åˆ°äº†æ”¹å–„ã€‚      
Celebrity lookalike - this is more a comment on the captioning/instance prompt used to train. Some fine tuners experienced improvements in their results when prompting with a token identifier + a public person that the base model knows about that resembles the person they trained on.      


#### ä½¿ç”¨å…ˆéªŒä¿ç•™æŸå¤±è¿›è¡Œè®­ç»ƒ:

    --with_prior_preservation
    --class_data_dir
    --num_class_images
    --class_prompt

--with_prior_preservation - å¯ç”¨å…ˆéªŒä¿ç•™è®­ç»ƒ    
--class_data_dir - åŒ…å«ç±»å›¾åƒçš„æ–‡ä»¶å¤¹çš„è·¯å¾„   
--num_class_images - å…ˆéªŒä¿ç•™æŸå¤±æ‰€éœ€çš„æœ€å°ç±»å›¾åƒæ•°Minimal class images for prior preservation lossã€‚å¦‚æœ --class_data_dir ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å›¾åƒï¼Œåˆ™ç”¨ --class_prompt é‡‡æ ·å‡ºæ›´å¤šçš„å›¾åƒã€‚





### å®è·µ

#### æ¯”è¾ƒsnr, pivotal tuning
ä¸ºäº†å‡å°è¶…å‚æœç´¢ç©ºé—´ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€äº›æµè¡Œé…ç½®ä½œä¸ºèµ·ç‚¹ï¼Œå¹¶åŸºäºæ­¤è¿›è¡Œè°ƒæ•´ä»¥è¾¾æˆæ•ˆæœã€‚

    --train_batch_size = 1, 2,3, 4
    -repeats = 1,2
    -learning_rate = 1.0 (Prodigy), 1e-4 (AdamW)
    -text_encoder_lr = 1.0 (Prodigy), 3e-4, 5e-5 (AdamW)
    -snr_gamma = None, 5.0
    -max_train_steps = 1000, 1500, 1800
    -text_encoder_training = regular finetuning, pivotal tuning (textual inversion)

æ–‡æœ¬ç¼–ç å™¨å…¨æ¨¡å‹å¾®è°ƒ VS æ¢è½´å¾®è°ƒ - æˆ‘ä»¬æ³¨æ„åˆ°æ¢è½´å¾®è°ƒå–å¾—äº†æ¯”æ–‡æœ¬ç¼–ç å™¨å…¨æ¨¡å‹å¾®è°ƒæ›´å¥½çš„ç»“æœï¼Œä¸”æ— éœ€å¾®è°ƒæ–‡æœ¬ç¼–ç å™¨çš„æƒé‡ã€‚   

é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³ä¸º Huggy å¾®è°ƒä¸€ä¸ª LoRA æ¨¡å‹ï¼Œè¿™æ„å‘³ç€æ—¢è¦æ•™ä¼šæ¨¡å‹ç›¸åº”çš„è‰ºæœ¯é£æ ¼ï¼ŒåŒæ—¶è¿˜è¦æ•™ä¼šå®ƒç‰¹å®šçš„è§’è‰²ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åˆ¶ä½œäº†ä¸€ä¸ªé«˜è´¨é‡çš„ Huggy å‰ç¥¥ç‰©æ•°æ®é›† (æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ Chunte-Lee çš„è‰ºæœ¯ä½œå“)ï¼Œè¯¥æ•°æ®é›†åŒ…å« 31 å¼ å›¾åƒåŠå…¶å¯¹åº”çš„æè¿°æ–‡æœ¬ã€‚   

    --learning_rate=1e-4 \
    --text_encoder_lr=3e-4 \
    --optimizer="adamw"\
    --train_text_encoder_ti\
    --lr_scheduler="constant" \
    --lr_warmup_steps=0 \
    --rank=32 \
    --max_train_steps=1000 \    
![alt text](assets/README/image-14.png)   
è¿™ä¸¤ä¸ªç‰ˆæœ¬éƒ½ä½¿ç”¨äº†ä»¥ä¸‹å‚æ•° (ä½†ç‰ˆæœ¬ 2 å¤šäº†ä¸€ä¸ª snr_gamma )   
![alt text](assets/README/image-13.png)     





#### æ¯”è¾ƒ AdamW åŠ Prodigy ä¼˜åŒ–å™¨
ä¸¤ä¸ªç‰ˆæœ¬éƒ½ä½¿ç”¨æ¢è½´å¾®è°ƒè¿›è¡Œè®­ç»ƒã€‚      
ä½¿ç”¨ optimizer=prodigy è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å°†åˆå§‹å­¦ä¹ ç‡è®¾ç½®ä¸º 1ã€‚è€Œå¯¹ adamWï¼Œæˆ‘ä»¬ä½¿ç”¨äº† cog-sdxl ä¸­ç”¨äºæ¢è½´å¾®è°ƒçš„é»˜è®¤å­¦ä¹ ç‡ ( learning_rate ä¸º 1e-4 ï¼Œ text_encoder_lr ä¸º 3e-4 )      
![alt text](assets/README/image-15.png)




#### ç½‘é¡µé£æ ¼
    â€“rank = 4,16,32
    -optimizer = prodigy, adamW
    -repeats = 1,2,3
    -learning_rate = 1.0 (Prodigy), 1e-4 (AdamW)
    -text_encoder_lr = 1.0 (Prodigy), 3e-4, 5e-5 (AdamW)
    -snr_gamma = None, 5.0
    -train_batch_size = 1, 2, 3, 4
    -max_train_steps = 500, 1000, 1500
    -text_encoder_training = regular finetuning, pivotal tuning

ä»äº’è”ç½‘ä¸ŠæŠ“å–çš„ 27 ä¸ª 20 ä¸–çºª 90 å¹´ä»£å’Œ 21 ä¸–çºªåˆçš„ç½‘é¡µæˆªå›¾ (ç›¸å½“å¤å¤ğŸ¥²):
![alt text](assets/README/image-16.png)     

è™½ç„¶æˆ‘ä»¬ä½¿ç”¨çš„è®­ç»ƒå›¾åƒå¤§è‡´ç›¸åŒ (~30 å¼ )ï¼Œä½†æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå¯¹äºè¿™ç§é£æ ¼çš„ LoRAï¼Œåœ¨ Huggy LoRA æ•ˆæœå¾ˆå¥½çš„è®¾ç½®å¯¹äºç½‘é¡µé£æ ¼æ¥è¯´è¡¨ç°å‡ºäº†è¿‡æ‹Ÿåˆã€‚    
![alt text](assets/README/image-17.png)     

å¯¹äº v1ï¼Œæˆ‘ä»¬é€‰æ‹©äº†è®­ç»ƒ Huggy LoRA æ—¶çš„æœ€ä½³é…ç½®ä½œä¸ºèµ·ç‚¹ - æ˜¾ç„¶è¿‡æ‹Ÿåˆäº†ã€‚å› æ­¤æˆ‘ä»¬å°è¯•åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­é€šè¿‡è°ƒæ•´ --max_train_steps ã€ --repeats ã€ --train_batch_size ä»¥åŠ --snr_gamma æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬åŸºäºè¿™å››ä¸ªé…ç½®è¿­ä»£äº† 8 ä¸ªç‰ˆæœ¬ï¼Œå¦‚ä¸‹ (æ‰€æœ‰å…¶ä½™é…ç½®ä¿æŒä¸å˜):    
![alt text](assets/README/image-18.png)     

æˆ‘ä»¬å‘ç° v4ã€v5 åŠ v6 å¯ä»¥è¾¾åˆ°æœ€ä½³çš„æŠ˜è¡·æ•ˆæœ:ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ   
å®¡ç¾ä¸ä¸€è‡´     
![alt text](assets/README/image-19.png)     


#### äººè„¸lora
é—®é¢˜åœ¨äºï¼šå³ä½¿åªéœ€è¦å°‘é‡å›¾ç‰‡ï¼Œè®­ç»ƒè¿‡ç¨‹ä¹Ÿä¼šåŠå°æ—¶åˆ°ä¸‰å°æ—¶ï¼Œè¿˜æœ‰è¿‡æ‹Ÿåˆé£é™©ï¼Œå®šåˆ¶ä¸ªäººsdxlæˆæœ¬è¿˜æ˜¯å¤ªé«˜      



åœ¨äººè„¸å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®© LoRA ç”Ÿæˆå°½å¯èƒ½çœŸå®ä¸”ä¸ç›®æ ‡äººç‰©ç›¸ä¼¼çš„å›¾åƒï¼ŒåŒæ—¶åˆèƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–è‡³è®­ç»ƒé›†ä¸­æœªè§è¿‡çš„èƒŒæ™¯å’Œæ„å›¾ã€‚      
æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬èšåˆäº†å¤šä¸ª Linoy è„¸éƒ¨æ•°æ®é›† (æ¯ä¸ªæ•°æ®é›†å« 6-10 å¼ å›¾åƒ)ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ç»„åŒæ—¶æ‹æ‘„çš„ç‰¹å†™ç…§ç‰‡ã€ä¸åŒåœºåˆçš„ç…§ç‰‡é›† (ä¸åŒçš„èƒŒæ™¯ã€ç¯å…‰å’Œæœè£…) ä»¥åŠå…¨èº«ç…§ã€‚      
we used different datasets of Linoy's face composed of 6-10 images, including a set of close-up photos taken all at the same time and a dataset of shots taken at different occasions (changing backgrounds, lighting and outfits) as well as full body shots.      
æˆ‘ä»¬æ·±çŸ¥ï¼Œå¦‚æœç”±äºç…§æ˜/åˆ†è¾¨ç‡/èšç„¦ä¸Šçš„é—®é¢˜å¯¼è‡´å›¾åƒçš„è´¨é‡åä½ï¼Œæ­¤æ—¶è¾ƒå°‘çš„é«˜è´¨å›¾åƒæ¯”è¾ƒå¤šçš„ä½è´¨å›¾åƒçš„å¾®è°ƒæ•ˆæœæ›´å¥½


    rank = 4,16,32, 64
    optimizer = prodigy, adamW
    repeats = 1,2,3,4
    learning_rate = 1.0 , 1e-4
    text_encoder_lr = 1.0, 3e-4
    snr_gamma = None, 5.0
    num_class_images = 100, 150
    max_train_steps = 75 * num_images, 100 * num_images, 120 * num_images
    text_encoder_training = regular finetuning, pivotal tuning

å…ˆéªŒä¿ç•™æŸå¤±

ä¸é€šå¸¸çš„åšæ³•ç›¸åï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨ç”Ÿæˆçš„ç±»å›¾åƒä¼šé™ä½ä¸ç›®æ ‡äººè„¸çš„ç›¸ä¼¼æ€§åŠç”Ÿæˆå›¾åƒçš„çœŸå®æ€§ã€‚    
æˆ‘ä»¬åˆ©ç”¨ä» unsplash ä¸‹è½½çš„å¼€æºå›¾åƒåˆ›å»ºäº†[çœŸå®è‚–åƒçš„ æ•°æ®é›†ã€‚](https://huggingface.co/datasets/multimodalart/faces-prior-preservation)

æ€»ç»“ï¼šå¯¹äºçœŸå®æ€§è¦æ±‚é«˜çš„ï¼Œé‡‡ç”¨çœŸå®ç±»æ•°æ®ä¿ç•™å…ˆéªŒ    

å½“ä½¿ç”¨çœŸå®å›¾åƒæ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°è¯­è¨€æ¼‚ç§»æ›´å°‘ (å³æ¨¡å‹ä¸ä¼šå°†â€œå¥³äºº/ç”·äººâ€ä¸€è¯ä»…ä¸ç»è¿‡è®­ç»ƒçš„é¢å­”ç›¸å…³è”ï¼Œè€Œæ˜¯å¯ä»¥ç”Ÿæˆä¸åŒçš„äºº i.e. the model doesn't associate the term woman/man with trained faces only and can generate different people as well) ; åŒæ—¶åœ¨è¾“å…¥å«ç»è®­ç»ƒçš„äººè„¸ç›¸å…³è¯å…ƒçš„æç¤ºè¯æƒ…å†µä¸‹ï¼Œå…¶ç”Ÿæˆåˆèƒ½ä¿è¯çœŸå®æ„ŸåŠæ•´ä½“è´¨é‡ã€‚while at the same time maintaining realism and overall quality when prompted for the trained faces.    

å³é€šè¿‡çœŸå®äººè„¸å…ˆéªŒæ•°æ®é›†ï¼Œå¯ä»¥åšåˆ°è¾“å…¥prompt:manä¾æ—§èƒ½ç”Ÿæˆä¸æ˜¯ è®­ç»ƒäººè„¸ çš„å›¾      

ç§©

æˆ‘ä»¬æ¯”è¾ƒäº†ç§©è®¾ä¸º 4ã€16ã€32 å’Œ 64 ç­‰ä¸åŒå€¼æ—¶çš„ LoRAã€‚åœ¨è¿™äº›å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°ç§©ä¸º 64 çš„ LoRA ç”Ÿæˆçš„å›¾åƒç£¨çš®æ•ˆæœæ›´å¼ºtend to have a more air-brushed appearance,ï¼Œå¹¶ä¸”çš®è‚¤çº¹ç†çœ‹ä¸Šå»ä¸å¤ªçœŸå®ã€‚      
å› æ­¤ï¼Œåœ¨åé¢çš„å®éªŒä»¥åŠ LoRA ease ç©ºé—´ ä¸Šï¼Œæˆ‘ä»¬éƒ½æŠŠç§©é»˜è®¤è®¾ä¸º 32ã€‚

6å¼ å›¾å®ç°idè¿ç§»     
è®­ç»ƒæ­¥æ•°      

å°½ç®¡ä»…éœ€å¾ˆå°‘é«˜è´¨å›¾åƒ (åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸º 6) å°±èƒ½è¾¾åˆ°å¥½çš„è®­ç»ƒæ•ˆæœï¼Œä½†æˆ‘ä»¬ä»éœ€è¦ç¡®å®šæ¨¡å‹è®­ç»ƒçš„ç†æƒ³æ­¥æ•°ã€‚      
åŸºäºè®­ç»ƒå›¾åƒæ•°ï¼Œæˆ‘ä»¬å°è¯•äº†å‡ ç§ä¸åŒçš„ä¹˜æ•°: 6 x75 = 450 æ­¥ / 6 x100 = 600 æ­¥ / 6 x120 = 720 æ­¥ã€‚      
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåˆæ­¥ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ 120 å€ä¹˜æ•°æ•ˆæœè¾ƒå¥½ (`å¦‚æœæ•°æ®é›†è¶³å¤Ÿå¤šæ ·è€Œä¸ä¼šè¿‡æ‹Ÿåˆï¼Œè®­ç»ƒé›†ä¸­æœ€å¥½ä¸è¦æœ‰ç›¸ä¼¼çš„ç…§ç‰‡`)        

![alt text](assets/README/image-20.png)     


    rank = 32
    optimizer = prodigy
    repeats = 1
    learning_rate = 1.0
    text_encoder_lr = 1.0
    max_train_steps = 75 * num_images, 100 * num_images, 120 * num_images
    train_text_encoder_ti
    with_prior_preservation_loss
    num_class_images = 150







## dora
DoRAï¼šæƒé‡åˆ†è§£ä½é˜¶è‡ªé€‚åº”ä¸­æå‡ºï¼Œ DoRAä¸ LoRA éå¸¸ç›¸ä¼¼ï¼Œä¸åŒä¹‹å¤„åœ¨äºå®ƒå°†é¢„è®­ç»ƒçš„æƒé‡åˆ†è§£ä¸ºå¤§å°å’Œæ–¹å‘ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå¹¶é‡‡ç”¨ LoRA è¿›è¡Œå®šå‘æ›´æ–°ï¼Œä»¥æœ‰æ•ˆåœ°æœ€å°åŒ–å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚ä½œè€…å‘ç°ï¼Œé€šè¿‡ä½¿ç”¨ DoRAï¼ŒLoRA çš„å­¦ä¹ èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§éƒ½å¾—åˆ°äº†å¢å¼ºï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ²¡æœ‰ä»»ä½•é¢å¤–çš„å¼€é”€ã€‚    

LoRA ä¼¼ä¹æ¯” DoRA æ”¶æ•›å¾—æ›´å¿«ï¼ˆå› æ­¤è®­ç»ƒ LoRA æ—¶å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆçš„ä¸€ç»„å‚æ•°å¯èƒ½å¯¹ DoRA æ•ˆæœå¾ˆå¥½ï¼‰
DoRA è´¨é‡ä¼˜äº LoRAï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒä½ç­‰çº§ä¸­ï¼Œç­‰çº§ 8 çš„ DoRA å’Œç­‰çº§ 8 çš„ LoRA çš„è´¨é‡å·®å¼‚ä¼¼ä¹æ¯”è®­ç»ƒç­‰çº§ä¸º 32 æˆ– 64 æ—¶æ›´æ˜¾ç€ã€‚
è¿™ä¹Ÿä¸è®ºæ–‡ä¸­æ˜¾ç¤ºçš„ä¸€äº›å®šé‡åˆ†æç›¸ä¸€è‡´ã€‚
ç”¨æ³•



## ZipLoRA-pytorch
![alt text](assets/README/image-21.png)   
![alt text](assets/README/image-22.png)   
![alt text](assets/README/image-23.png)     
![alt text](assets/README/image-24.png)   
åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬å‡è®¾ä¸€ç§ç±»ä¼¼äºæ‹‰é“¾çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘ç›¸ä¼¼æ–¹å‘å’Œçš„æ•°é‡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ LoRA çš„å†…å®¹å’Œé£æ ¼ç”Ÿæˆå±æ€§ï¼Œå°†äº§ç”Ÿæ›´ç¨³å¥ã€æ›´é«˜è´¨é‡çš„åˆå¹¶ã€‚å°±åƒæ‹‰é“¾æ— ç¼è¿æ¥ç»‡ç‰©çš„ä¸¤ä¾§ä¸€æ ·ï¼Œæˆ‘ä»¬æå‡ºçš„åŸºäºä¼˜åŒ–çš„æ–¹æ³•æ‰¾åˆ°äº†ä¸€ç»„ä¸ç›¸äº¤çš„åˆå¹¶ç³»æ•°æ¥æ··åˆä¸¤ä¸ª LoRAã€‚è¿™ç¡®ä¿äº†åˆå¹¶åçš„ LoRA èƒ½å¤Ÿç†Ÿç»ƒåœ°æ•æ‰ä¸»é¢˜å’Œé£æ ¼ã€‚












# ç»“å°¾
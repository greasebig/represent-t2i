# Inpaint anything 
https://github.com/geekyutao/Inpaint-Anything

æœ€è¿‘ä¿®æ”¹å…«ä¸ªæœˆå‰

Inpaint anything using Segment Anything and inpainting models.

æ„Ÿè§‰å·²ç»è¢«webui æ’ä»¶å¹³æ›¿


Inpaint Anything å¯ä»¥ä¿®å¤å›¾åƒã€è§†é¢‘å’Œ3D åœºæ™¯ä¸­çš„ä»»ä½•å†…å®¹ï¼


[2023/9/15] Remove Anything 3Dä»£ç å¯ç”¨ï¼
[2023/4/30] Remove Anything è§†é¢‘å¯ç”¨ï¼æ‚¨å¯ä»¥ä»Žè§†é¢‘ä¸­åˆ é™¤ä»»ä½•å¯¹è±¡ï¼
[2023/4/24]æ”¯æŒæœ¬åœ° Web UIï¼æ‚¨å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæ¼”ç¤ºç½‘ç«™ï¼
[2023/4/22]ç½‘ç«™å¯ç”¨ï¼æ‚¨å¯ä»¥é€šè¿‡ç•Œé¢ä½“éªŒ Inpaint Anythingï¼
[2023/4/22] Remove Anything 3Då¯ç”¨ï¼æ‚¨å¯ä»¥ä»Ž 3D åœºæ™¯ä¸­åˆ é™¤ä»»ä½• 3D å¯¹è±¡ï¼
[2023/4/13] arXiv ä¸Šçš„æŠ€æœ¯æŠ¥å‘Šå¯ç”¨ï¼


 ç§»é™¤ä»»ä½•å†…å®¹
 å¡«å……ä»»æ„å†…å®¹
 æ›¿æ¢ä»»ä½•å†…å®¹
 åˆ é™¤ä»»ä½•3Då†…å®¹ï¼ˆðŸ”¥æ–°åŠŸèƒ½ï¼‰

 åˆ é™¤ä»»ä½•è§†é¢‘ï¼ˆðŸ”¥æ–°åŠŸèƒ½ï¼‰

æœªå®žçŽ°
 å¡«å……ä»»æ„3Då†…å®¹
æ›¿æ¢ä»»ä½•3Då†…å®¹
å¡«å……ä»»ä½•è§†é¢‘
æ›¿æ¢ä»»ä½•è§†é¢‘

ç‚¹å‡»ä¸€ä¸ªå¯¹è±¡ï¼›
åˆ†å‰²ä»»ä½•æ¨¡åž‹ï¼ˆSAMï¼‰å°†å¯¹è±¡åˆ†å‰²å‡ºæ¥ï¼›
ä¿®å¤æ¨¡åž‹ï¼ˆä¾‹å¦‚ï¼ŒLaMaï¼‰å¡«è¡¥äº†â€œç©ºæ´žâ€ã€‚

Acknowledgments    
Segment Anything
LaMa
Stable Diffusion
OSTrack
STTN
Other Interesting Repositories
Awesome Anything
Composable AI
Grounded SAM

## LaMa


https://advimman.github.io/lama-project/

WACV 2022


https://github.com/advimman/lama

æœ€è¿‘ä¿®æ”¹ä¸Šä¸ªæœˆ

ðŸ¦™ LaMa Image Inpainting, Resolution-robust Large Mask Inpainting with Fourier Convolutions, WACV 2022


LaMa çš„æŽ¨å¹¿æ•ˆæžœå‡ºå¥‡åœ°å¥½ï¼Œå¯ä»¥è¾¾åˆ°æ¯”è®­ç»ƒæœŸé—´ï¼ˆ256x256ï¼‰æ›´é«˜çš„åˆ†è¾¨çŽ‡ï¼ˆ~2kâ—ï¸ï¼‰ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ä¹Ÿèƒ½å®žçŽ°å‡ºè‰²çš„æ€§èƒ½ï¼Œä¾‹å¦‚å®Œæˆå‘¨æœŸæ€§ç»“æž„ã€‚



è‡´è°¢
å¦‚æžœå½¢æˆCSAILVision ï¼Œåˆ™åˆ†å‰²ä»£ç å’Œæ¨¡åž‹ã€‚
LPIPS æŒ‡æ ‡æ¥è‡ªrichzhang
SSIM æ¥è‡ªPo-Hsun-Su
FID æ¥è‡ªmseitzer


![alt text](assets/610614/image.png)


å°½ç®¡çŽ°ä»£å›¾åƒä¿®å¤ç³»ç»Ÿå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†é€šå¸¸ä»éš¾ä»¥å¤„ç†å¤§é¢ç§¯ç¼ºå¤±ã€å¤æ‚çš„å‡ ä½•ç»“æž„å’Œé«˜åˆ†è¾¨çŽ‡å›¾åƒã€‚æˆ‘ä»¬å‘çŽ°ï¼Œå…¶ä¸­ä¸€ä¸ªä¸»è¦åŽŸå› æ˜¯ä¿®å¤ç½‘ç»œå’ŒæŸå¤±å‡½æ•°ä¸­éƒ½ç¼ºä¹æœ‰æ•ˆçš„æ„Ÿå—é‡Žã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºå¤§æŽ©æ¨¡ä¿®å¤ï¼ˆLaMaï¼‰çš„æ–°æ–¹æ³•ã€‚LaM åŸºäºŽï¼š
ä¸€ç§æ–°çš„ä¿®å¤ç½‘ç»œæž¶æž„ï¼Œä½¿ç”¨å¿«é€Ÿå‚…é‡Œå¶å·ç§¯ï¼Œå…·æœ‰å›¾åƒèŒƒå›´çš„æ„Ÿå—é‡Ž
é«˜åº¦æ„Ÿå—é‡ŽçŸ¥è§‰æŸå¤±ï¼›
å¤§åž‹è®­ç»ƒé¢ç½©ï¼Œé‡Šæ”¾å‰ä¸¤ä¸ªç»„ä»¶çš„æ½œåŠ›ã€‚
æˆ‘ä»¬çš„ä¿®å¤ç½‘ç»œåœ¨ä¸€ç³»åˆ—æ•°æ®é›†ä¸Šæå‡äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ä¹Ÿèƒ½å®žçŽ°å‡ºè‰²çš„æ€§èƒ½ï¼Œä¾‹å¦‚å®Œæˆå‘¨æœŸæ€§ç»“æž„ã€‚æˆ‘ä»¬çš„æ¨¡åž‹å¯¹é«˜äºŽè®­ç»ƒæ—¶çš„åˆ†è¾¨çŽ‡çš„æ³›åŒ–æ•ˆæžœä»¤äººæƒŠè®¶ï¼Œå¹¶ä¸”ä»¥æ¯”ç«žäº‰åŸºçº¿æ›´ä½Žçš„å‚æ•°å’Œè®¡ç®—æˆæœ¬å®žçŽ°äº†è¿™ä¸€ç‚¹ã€‚



# DemoFusion
[Submitted on 24 Nov 2023 (v1), last revised 15 Dec 2023 (this version, v2)]    
DemoFusion: Democratising High-Resolution Image Generation With No $$$


https://github.com/ttulttul/ComfyUI-Iterative-Mixer


ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) ç”Ÿæˆé«˜åˆ†è¾¨çŽ‡å›¾åƒå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç”±äºŽåŸ¹è®­éœ€è¦å·¨é¢èµ„æœ¬æŠ•å…¥ï¼Œå®ƒè¶Šæ¥è¶Šé›†ä¸­äºŽå°‘æ•°å‡ å®¶å¤§å…¬å¸ï¼Œå¹¶ä¸”éšè—åœ¨ä»˜è´¹å¢™åŽé¢ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æŽ¨è¿›é«˜åˆ†è¾¨çŽ‡ç”Ÿæˆçš„å‰æ²¿ï¼ŒåŒæ—¶ä¿æŒå¹¿å¤§å—ä¼—çš„å¯è®¿é—®æ€§ï¼Œä½¿é«˜åˆ†è¾¨çŽ‡ GenAI æ°‘ä¸»åŒ–ã€‚æˆ‘ä»¬è¯æ˜ŽçŽ°æœ‰çš„æ½œåœ¨æ‰©æ•£æ¨¡åž‹ (LDM) åœ¨é«˜åˆ†è¾¨çŽ‡å›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰å°šæœªå¼€å‘çš„æ½œåŠ›ã€‚æˆ‘ä»¬æ–°é¢–çš„ DemoFusion æ¡†æž¶æ— ç¼æ‰©å±•äº†å¼€æº GenAI æ¨¡åž‹ï¼Œé‡‡ç”¨æ¸è¿›å¼å‡çº§ã€è·³è¿‡æ®‹å·®å’Œæ‰©å¼ é‡‡æ ·æœºåˆ¶æ¥å®žçŽ°æ›´é«˜åˆ†è¾¨çŽ‡çš„å›¾åƒç”Ÿæˆã€‚DemoFusion çš„æ¸è¿›æ€§éœ€è¦æ›´å¤šæ¬¡ä¼ é€’ï¼Œä½†ä¸­é—´ç»“æžœå¯ä»¥ä½œä¸ºâ€œé¢„è§ˆâ€ï¼Œä¿ƒè¿›å¿«é€ŸåŠæ—¶è¿­ä»£ã€‚



# Stable Diffusion Infinity Grid Generator

https://github.com/mcmonkeyprojects/sd-infinity-grid-generator-script

Infinite-Axis Grid Generator for Stable Diffusion!



ç›¸å½“äºŽä¸€ä¸ªwebui   

æœ€è¿‘æ›´æ–°ä¸¤ä¸ªæœˆå‰

è‡´è°¢
è¿™ä¸ªè®¾è®¡éƒ¨åˆ†å—åˆ°â€œxrypgameâ€çš„â€œXYZ Plotâ€è„šæœ¬çš„å¯å‘ï¼ˆä¸è¦ä¸Ž Auto WebUI ä¸­çš„â€œXYZ Plotâ€è„šæœ¬æ··æ·†ï¼ŒåŽè€…å®žé™…ä¸Šåªæ˜¯â€œX/Y Plotâ€è„šæœ¬ï¼Œä½†ä»–ä»¬æ·»åŠ äº†ä¸€ä¸ªâ€œZâ€å“ˆå“ˆï¼‰
ä»£ç éƒ¨åˆ†å¼•ç”¨è‡ª WebUI æœ¬èº«åŠå…¶é»˜è®¤çš„â€œX/Y Plotâ€è„šæœ¬ï¼ˆåŽæ¥é‡å‘½åä¸ºâ€œXYZ Plotâ€ï¼‰ã€‚
ä¸€äº›ä»£ç æ®µå¼•ç”¨è‡ªå…¶ä»–å„ç§ç›¸å…³æ¥æºï¼Œä¾‹å¦‚ï¼Œd8ahazard çš„ Dreambooth æ‰©å±•è¢«å¼•ç”¨ç”¨äºŽ JavaScript ä»£ç æŠ€å·§ï¼ˆæ ‡é¢˜è¦†ç›–ï¼‰ã€‚
æœ‰äº›ä»£ç éƒ¨åˆ†å¼•ç”¨è‡ª StackOverflow ä¸Šçš„éšæœºç­”æ¡ˆä»¥åŠå„ç§å…¶ä»–è°·æ­Œæ–‡æ¡£å’Œç­”æ¡ˆç½‘ç«™ã€‚æˆ‘æ²¡æœ‰è·Ÿè¸ªå®ƒä»¬ï¼Œä½†æˆ‘å¾ˆé«˜å…´ç”Ÿæ´»åœ¨è¿™æ ·ä¸€ä¸ªä¸–ç•Œé‡Œï¼Œæœ‰è¿™ä¹ˆå¤šå¼€å‘äººå‘˜éƒ½å¾ˆé«˜å…´å¹¶æ¸´æœ›å¸®åŠ©å½¼æ­¤å­¦ä¹ å’Œæˆé•¿ã€‚æ‰€ä»¥ï¼Œæ„Ÿè°¢ FOSS ç¤¾åŒºçš„æ‰€æœ‰æˆå‘˜ï¼
æ„Ÿè°¢æ‰€æœ‰åˆå¹¶ PRçš„ä½œè€…ã€‚
æ„Ÿè°¢æ‰€æœ‰æ ‡è®°ä¸ºâ€œå·²å®Œæˆâ€çš„é—®é¢˜çš„ä½œè€…ã€‚
æ„Ÿè°¢ StabilityAIã€RunwayMLã€CompVis for Stable Diffusion ä»¥åŠå…¶å·¥ä½œè¢«çº³å…¥å…¶ä¸­çš„ç ”ç©¶äººå‘˜ã€‚
æ„Ÿè°¢ AUTOMATIC1111 å’Œ WebUI çš„ä¼—å¤šè´¡çŒ®è€…ã€‚




# ELLA

Enhanced Semantic Alignment is a Second Life for SD 1.5 Models, Allows Complex Compositions with SDXL-like Prompting



ä¸¤ä¸‰ä¸ªæœˆå‰

ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment

https://github.com/TencentQQGYLab/ELLA

ELLA
Paper: ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment

Project Website: ELLA

EMMA
Paper: EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts

Project Website: EMMA


[2024.6.14] ðŸ”¥ðŸ”¥ EMMA: Technical Report, Project Website
[2024.5.13] EMMA is coming soon. Let's first preview the results of EMMA: ä¸­æ–‡ç‰ˆ, English Version
[2024.4.19] We provide ELLAâ€™s ComfyUI plugin: TencentQQGYLab/ComfyUI-ELLA
[2024.4.11] Add some results of EMMA(Efficient Multi-Modal Adapter)
[2024.4.9] ðŸ”¥ðŸ”¥ðŸ”¥ Release ELLA-SD1.5 Checkpoint! Welcome to try!
[2024.3.11] ðŸ”¥ Release DPG-Bench! Welcome to try!
[2024.3.7] Initial update

ELLA ä»å¤„äºŽæ—©æœŸç ”ç©¶é˜¶æ®µï¼Œæˆ‘ä»¬å°šæœªå¯¹ ELLA çš„æ‰€æœ‰æ½œåœ¨åº”ç”¨è¿›è¡Œå…¨é¢æµ‹è¯•ã€‚æˆ‘ä»¬æ¬¢è¿Žç¤¾åŒºæå‡ºå»ºè®¾æ€§å’Œå‹å¥½çš„å»ºè®®ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ†äº«è¿„ä»Šä¸ºæ­¢å‘çŽ°çš„ä¸€äº›å…³äºŽå¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨ ELLA çš„æŠ€å·§ï¼š

1. æ ‡é¢˜é«˜æ¡£
ELLA æ˜¯ä½¿ç”¨ MLLM æ³¨é‡Šçš„åˆæˆå­—å¹•è¿›è¡Œè®­ç»ƒçš„ã€‚å¦‚ä½¿ç”¨æ›´å¥½çš„å­—å¹•æ”¹è¿›å›¾åƒç”Ÿæˆä¸­æ‰€è¿°ï¼Œåœ¨ä½¿ç”¨ ELLA ä¹‹å‰å¯¹è¾“å…¥å­—å¹•è¿›è¡Œâ€œä¸Šé‡‡æ ·â€å¯ä»¥å‘æŒ¥å…¶æœ€å¤§æ½œåŠ›ã€‚

æˆ‘ä»¬å‘çŽ°ï¼Œåˆ©ç”¨ LLM çš„ä¸Šä¸‹æ–‡å­¦ä¹  (ICL) åŠŸèƒ½å¯ä»¥å®žçŽ°ç®€å•çš„å­—å¹•ä¸Šé‡‡æ ·ï¼š

Please generate the long prompt version of the short one according to the given examples. Long prompt version should consist of 3 to 5 sentences. Long prompt version must sepcify the color, shape, texture or spatial relation of the included objects. DO NOT generate sentences that describe any atmosphere!!!

Short: A calico cat with eyes closed is perched upon a Mercedes.
Long: a multicolored cat perched atop a shiny black car. the car is parked in front of a building with wooden walls and a green fence. the reflection of the car and the surrounding environment can be seen on the car's glossy surface.

Short: A boys sitting on a chair holding a video game remote.
Long: a young boy sitting on a chair, wearing a blue shirt and a baseball cap with the letter 'm'. he has a red medal around his neck and is holding a white game controller. behind him, there are two other individuals, one of whom is wearing a backpack. to the right of the boy, there's a blue trash bin with a sign that reads 'automatic party'.

Short: A man is on the bank of the water fishing.
Long: a serene waterscape where a person, dressed in a blue jacket and a red beanie, stands in shallow waters, fishing with a long rod. the calm waters are dotted with several sailboats anchored at a distance, and a mountain range can be seen in the background under a cloudy sky.

Short: A kitchen with a cluttered counter and wooden cabinets.
Long: a well-lit kitchen with wooden cabinets, a black and white checkered floor, and a refrigerator adorned with a floral decal on its side. the kitchen countertop holds various items, including a coffee maker, jars, and fruits.

Short: a racoon holding a shiny red apple over its head

using: https://huggingface.co/spaces/Qwen/Qwen-72B-Chat-Demo

we got:

a mischievous raccoon standing on its hind legs, holding a bright red apple aloft in its furry paws. the apple shines brightly against the backdrop of a dense forest, with leaves rustling in the gentle breeze. a few scattered rocks can be seen on the ground beneath the raccoon's feet, while a gnarled tree trunk stands nearby.


çµæ´»çš„ token é•¿åº¦
åœ¨ ELLA çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†è¾ƒé•¿çš„åˆæˆå­—å¹•ï¼Œæœ€å¤§æ ‡è®°æ•°è®¾ç½®ä¸º 128ã€‚åœ¨ä½¿ç”¨çŸ­å­—å¹•æµ‹è¯• ELLA æ—¶ï¼Œé™¤äº†å‰é¢æåˆ°çš„å­—å¹•ä¸Šé‡‡æ ·æŠ€æœ¯å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨â€œflexible_token_lengthâ€æŠ€å·§ã€‚è¿™æ¶‰åŠå°†æ ‡è®°å™¨è®¾ç½®max_lengthä¸ºNoneï¼Œä»Žè€Œæ¶ˆé™¤ä»»ä½•æ–‡æœ¬æ ‡è®°å¡«å……æˆ–æˆªæ–­ã€‚æˆ‘ä»¬å·²ç»è§‚å¯Ÿåˆ°ï¼Œæ­¤æŠ€å·§å¯ä»¥å¸®åŠ©æé«˜ä¸ŽçŸ­å­—å¹•ç›¸å¯¹åº”çš„ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚

3. ELLA+CLIP ç¤¾åŒºæ¨¡åž‹
æˆ‘ä»¬çš„æµ‹è¯•è¡¨æ˜Žï¼Œä¸€äº›ä¸¥é‡ä¾èµ–è§¦å‘è¯çš„ç¤¾åŒºæ¨¡åž‹åœ¨ä½¿ç”¨ ELLA æ—¶å¯èƒ½ä¼šé­é‡ä¸¥é‡çš„é£Žæ ¼æŸå¤±ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºåœ¨ ELLA æŽ¨ç†è¿‡ç¨‹ä¸­æ ¹æœ¬æ²¡æœ‰ä½¿ç”¨ CLIPã€‚

å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªä½¿ç”¨ CLIPï¼Œä½†æˆ‘ä»¬å‘çŽ°ä»ç„¶å¯ä»¥åœ¨æŽ¨ç†è¿‡ç¨‹ä¸­å°† ELLA çš„è¾“å…¥ä¸Ž CLIP çš„è¾“å‡ºè¿žæŽ¥èµ·æ¥ï¼ˆBx77x768 + Bx64x768 -> Bx141x768ï¼‰ï¼Œä½œä¸º UNet çš„æ¡ä»¶ã€‚æˆ‘ä»¬é¢„è®¡ï¼Œå°† ELLA ä¸Ž CLIP ç»“åˆä½¿ç”¨å°†æ›´å¥½åœ°èžå…¥çŽ°æœ‰çš„ç¤¾åŒºç”Ÿæ€ç³»ç»Ÿï¼Œå°¤å…¶æ˜¯ä¸Ž CLIP ç‰¹å®šçš„æŠ€æœ¯ï¼ˆä¾‹å¦‚æ–‡æœ¬åè½¬å’Œè§¦å‘è¯ï¼‰ç»“åˆä½¿ç”¨ã€‚

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç¡®ä¿ä¸Žæ›´å¹¿æ³›çš„ç¤¾åŒºæ¨¡åž‹æ›´å¥½åœ°å…¼å®¹ï¼›ä½†æ˜¯ï¼Œæˆ‘ä»¬ç›®å‰è¿˜æ²¡æœ‰ä¸€å¥—å®Œæ•´çš„ç»éªŒå¯ä»¥åˆ†äº«ã€‚å¦‚æžœæ‚¨æœ‰ä»»ä½•å»ºè®®ï¼Œæˆ‘ä»¬å°†éžå¸¸æ„Ÿæ¿€æ‚¨èƒ½åœ¨é—®é¢˜ä¸­åˆ†äº«å®ƒä»¬ã€‚

4.FlanT5å¿…é¡»åœ¨fp16æ¨¡å¼ä¸‹è¿è¡Œã€‚
å¦‚issue#23ä¸­æ‰€è¿°ï¼Œæˆ‘ä»¬ç»å¤§å¤šæ•°å®žéªŒæ˜¯åœ¨V100ä¸Šè¿›è¡Œçš„ï¼Œä¸æ”¯æŒbf16ï¼Œæ‰€ä»¥åªèƒ½ä½¿ç”¨fp16 T5è¿›è¡Œè®­ç»ƒã€‚ç»è¿‡æµ‹è¯•å‘çŽ°ï¼Œfp16 T5å’Œbf16 T5çš„è¾“å‡ºå·®å¼‚ä¸å¯å¿½ç•¥ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒå­˜åœ¨æ˜Žæ˜¾å·®å¼‚ã€‚å› æ­¤ï¼Œå»ºè®®ä½¿ç”¨fp16 T5è¿›è¡ŒæŽ¨ç†ã€‚



ðŸš§ EMMA - é«˜æ•ˆçš„å¤šæ¨¡å¼é€‚é…å™¨ï¼ˆæ­£åœ¨è¿›è¡Œä¸­ï¼‰



ELLAï¼ˆé«˜æ•ˆå¤§åž‹è¯­è¨€æ¨¡åž‹é€‚é…å™¨ï¼‰ä¸ºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡åž‹é…å¤‡äº†å¼ºå¤§çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ï¼Œä»Žè€Œæ— éœ€è®­ç»ƒ U-Net æˆ– LLM å³å¯å¢žå¼ºæ–‡æœ¬å¯¹é½ã€‚



æ¨¡åž‹æè¿°
æ‰©æ•£æ¨¡åž‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨çŽ°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡åž‹ä¸­çš„å¤§å¤šæ•°ä»ç„¶ä½¿ç”¨ CLIP ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ç†è§£å¯†é›†æç¤ºçš„èƒ½åŠ›ï¼Œè¿™äº›æç¤ºåŒ…å«å¤šä¸ªå¯¹è±¡ã€è¯¦ç»†å±žæ€§ã€å¤æ‚å…³ç³»ã€é•¿æ–‡æœ¬å¯¹é½ç­‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„å¤§åž‹è¯­è¨€æ¨¡åž‹é€‚é…å™¨ï¼Œç§°ä¸º ELLAï¼Œå®ƒä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹é…å¤‡äº†å¼ºå¤§çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM)ï¼Œä»¥å¢žå¼ºæ–‡æœ¬å¯¹é½ï¼Œè€Œæ— éœ€è®­ç»ƒ U-Net æˆ– LLMã€‚ä¸ºäº†æ— ç¼è¿žæŽ¥ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡åž‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç³»åˆ—è¯­ä¹‰å¯¹é½è¿žæŽ¥å™¨è®¾è®¡ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°æ¨¡å—ï¼Œå³æ—¶é—´æ­¥æ„ŸçŸ¥è¯­ä¹‰è¿žæŽ¥å™¨ (TSC)ï¼Œå®ƒå¯ä»¥ä»Ž LLM ä¸­åŠ¨æ€æå–ä¸Žæ—¶é—´æ­¥ç›¸å…³çš„æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŽ»å™ªè¿‡ç¨‹çš„ä¸åŒé˜¶æ®µè°ƒæ•´è¯­ä¹‰ç‰¹å¾ï¼Œå¸®åŠ©æ‰©æ•£æ¨¡åž‹åœ¨é‡‡æ ·æ—¶é—´æ­¥ä¸Šè§£é‡Šå†—é•¿è€Œå¤æ‚çš„æç¤ºã€‚æ­¤å¤–ï¼ŒELLA å¯ä»¥å¾ˆå®¹æ˜“åœ°ä¸Žç¤¾åŒºæ¨¡åž‹å’Œå·¥å…·ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜å®ƒä»¬çš„æç¤ºè·Ÿè¸ªèƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°æ–‡æœ¬è½¬å›¾åƒæ¨¡åž‹åœ¨å¯†é›†æç¤ºè·Ÿè¸ªä¸­çš„è¡¨çŽ°ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯†é›†æç¤ºå›¾åŸºå‡† (DPG-Bench)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”± 1K å¯†é›†æç¤ºç»„æˆçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚å¤§é‡å®žéªŒè¡¨æ˜Žï¼ŒELLA åœ¨å¯†é›†æç¤ºè·Ÿè¸ªæ–¹é¢ä¼˜äºŽæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šç§å±žæ€§å’Œå…³ç³»çš„å¤šå¯¹è±¡ç»„åˆä¸­ã€‚


RESEARCH
ELLA: Leveraging LLMs for Enhanced Semantic Alignment in SD 1.5
DANIEL SANDNER APRIL 14, 2024















å…¶ä»–
æˆ‘ä»¬è¿˜å‘çŽ°äº†å¦ä¸€é¡¹ç‹¬ç«‹ä½†ç›¸ä¼¼çš„å·¥ä½œLaVi-Bridgeï¼Œè¿™é¡¹å·¥ä½œå‡ ä¹ŽåŒæ—¶å®Œæˆï¼Œæä¾›äº† ELLA æœªæ¶µç›–çš„é¢å¤–è§è§£ã€‚ELLA å’Œ LaVi-Bridge ä¹‹é—´çš„å·®å¼‚å¯ä»¥åœ¨ç¬¬ 13 æœŸä¸­æ‰¾åˆ°ã€‚æˆ‘ä»¬å¾ˆé«˜å…´æ¬¢è¿Žå…¶ä»–ç ”ç©¶äººå‘˜å’Œç¤¾åŒºç”¨æˆ·æŽ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚

[æäº¤æ—¥æœŸï¼š2024 å¹´ 3 æœˆ 12 æ—¥]
è¿žæŽ¥ä¸åŒçš„è¯­è¨€æ¨¡åž‹å’Œç”Ÿæˆè§†è§‰æ¨¡åž‹ä»¥å®žçŽ°æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆ
èµµä¸–è±ªã€éƒå°‘å“²ã€å­ä¼¯å˜‰ã€å¾æ€€å“²ã€é»„å›ä»ª
éšç€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹çš„å¼•å…¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå–å¾—äº†é‡å¤§è¿›å±•ã€‚è¿™äº›æ¨¡åž‹é€šå¸¸ç”±è§£é‡Šç”¨æˆ·æç¤ºçš„è¯­è¨€æ¨¡åž‹å’Œç”Ÿæˆç›¸åº”å›¾åƒçš„è§†è§‰æ¨¡åž‹ç»„æˆã€‚éšç€è¯­è¨€å’Œè§†è§‰æ¨¡åž‹åœ¨å„è‡ªé¢†åŸŸçš„ä¸æ–­è¿›æ­¥ï¼ŒæŽ¢ç´¢ç”¨æ›´å…ˆè¿›çš„æ¨¡åž‹æ›¿æ¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡åž‹ä¸­çš„ç»„ä»¶å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œæ›´å¹¿æ³›çš„ç ”ç©¶ç›®æ ‡æ˜¯ç ”ç©¶å°†ä»»ä½•ä¸¤ä¸ªä¸ç›¸å…³çš„è¯­è¨€å’Œç”Ÿæˆè§†è§‰æ¨¡åž‹é›†æˆåˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŽ¢ç´¢äº†è¿™ä¸€ç›®æ ‡å¹¶æå‡ºäº† LaVi-Bridgeï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿå°†å„ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹å’Œç”Ÿæˆè§†è§‰æ¨¡åž‹é›†æˆåˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ã€‚é€šè¿‡åˆ©ç”¨ LoRA å’Œé€‚é…å™¨ï¼ŒLaVi-Bridge æä¾›äº†ä¸€ç§çµæ´»çš„å³æ’å³ç”¨æ–¹æ³•ï¼Œè€Œæ— éœ€ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡åž‹çš„åŽŸå§‹æƒé‡ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸Žå„ç§è¯­è¨€æ¨¡åž‹å’Œç”Ÿæˆè§†è§‰æ¨¡åž‹å…¼å®¹ï¼Œå¯é€‚åº”ä¸åŒçš„ç»“æž„ã€‚åœ¨è¿™ä¸ªæ¡†æž¶ä¸­ï¼Œæˆ‘ä»¬è¯æ˜Žäº†ç»“åˆæ›´é«˜çº§çš„æ¨¡å—ï¼ˆä¾‹å¦‚æ›´é«˜çº§çš„è¯­è¨€æ¨¡åž‹æˆ–ç”Ÿæˆè§†è§‰æ¨¡åž‹ï¼‰å¯ä»¥æ˜¾è‘—æé«˜æ–‡æœ¬å¯¹é½æˆ–å›¾åƒè´¨é‡ç­‰åŠŸèƒ½ã€‚å·²ç»è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ä»¥éªŒè¯ LaVi-Bridge çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨æ­¤ https URLä¸Šæ‰¾åˆ°ã€‚

# HDR
é«˜åŠ¨æ€èŒƒå›´ (HDR)çš„ç›®æ ‡æ˜¯é‡çŽ°åœºæ™¯ä¸­æ˜Žæš—åŒºåŸŸç»†èŠ‚æ›´ä¸°å¯Œçš„åœºæ™¯ï¼Œä»Žè€Œå‘ˆçŽ°æ›´çœŸå®žã€æ›´é†’ç›®çš„è§†è§‰æ•ˆæžœã€‚å®ƒæ—¨åœ¨ä»¥æŸç§æ–¹å¼æ¨¡ä»¿æˆ‘ä»¬çœ¼ç›çš„è‰²è°ƒèŒƒå›´ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é¿å…å› æ›å…‰ä¸è¶³æˆ–è¿‡åº¦æ›å…‰è€Œå¯¼è‡´å›¾åƒå‡ºçŽ°å¤§é‡æžæš—æˆ–æžäº®åŒºåŸŸã€‚HDRå¯ä½¿å›¾åƒçœ‹èµ·æ¥æ›´è‡ªç„¶ï¼Œå¹¶åœ¨åŽæœŸå¤„ç†è¿‡ç¨‹ä¸­ä¸ºé¢œè‰²æˆ–è‰²è°ƒé£Žæ ¼åŒ–æä¾›æ›´å¤šè‡ªç”±ã€‚è¿™ç§æŠ€æœ¯é€šå¸¸ç”¨äºŽä¸“ä¸šæ‘„å½±ã€‚



ä½¿ç”¨è„šæœ¬æ‰©å±•åŠ¨æ€èŒƒå›´
æˆ‘ä»¬å°†ä½¿ç”¨ VectorscopeCC æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ­¤æ‰©å±•é€šè¿‡è°ƒæ•´é¢œè‰²ã€äº®åº¦å’Œå¯¹æ¯”åº¦ï¼ˆå¦‚åç§»å™ªå£°ï¼‰æ¥ä¿®æ”¹æ‰©æ•£ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«ä¸€ä¸ªå…·æœ‰æ¬ºéª—æ€§åç§°â€œé«˜åŠ¨æ€èŒƒå›´â€çš„è„šæœ¬ï¼Œè¯¥è„šæœ¬å®žé™…ä¸Šè¾“å‡ºçš„å°±æ˜¯é«˜åŠ¨æ€èŒƒå›´ã€‚

æ‚¨å¯ä»¥ä»Ž Extensions/Avalable/Load ä¸­å®‰è£… VectorscopeCCæ‰©å±•ã€‚åœ¨åˆ—è¡¨ä¸­æ‰¾åˆ°å®ƒå¹¶å®‰è£…ã€‚é‡æ–°å¯åŠ¨ A1111ã€‚æ‚¨å°†åœ¨ txt2img å’Œ img2img æŠ˜å åº•éƒ¨çš„è„šæœ¬åˆ—è¡¨ä¸‹æ‰¾åˆ°è¯¥è„šæœ¬ã€‚

![alt text](assets/610614/image-1.png)


![alt text](assets/610614/image-2.png)


é‡è¦æç¤º
ä¸€æ¬¡ä»…ç”Ÿæˆ 1 ä¸ªæ‰¹æ¬¡æ•°é‡/å°ºå¯¸
å¦‚ä¸Šæ‰€è¿°ï¼Œåœ¨å‡çº§å›¾åƒ/åŒ…å›´æ—¶ä¿æŒä½ŽåŽ»å™ª
æç¤ºä¸­å®šä¹‰çš„é˜´å½±éžå¸¸é”åˆ©çš„å›¾åƒä¸ä¼šä»Žè„šæœ¬ä¸­å—ç›Š
æ‚¨å¯ä»¥åœ¨img2imgä¸­æœ‰æ•ˆåœ°ä½¿ç”¨è¯¥è„šæœ¬ï¼Œåªéœ€æ’å…¥å›¾åƒï¼Œè®¾ç½®å¤§å°å’Œä½ŽåŽ»å™ªå¼ºåº¦å¹¶è®¾ç½®è„šæœ¬
VAE èƒ½è§£å†³åŠ¨æ€èŒƒå›´å—ï¼Ÿ
æœ‰å¯èƒ½ã€‚æˆ‘ä¸çŸ¥é“æœ‰å“ªä¸ª VAE è§£å†³æ–¹æ¡ˆå¯ä»¥åœ¨ä¸å¹³å¦èŒƒå›´çš„æƒ…å†µä¸‹å·¥ä½œï¼Œåªç»™å‡º RAW å¤–è§‚çš„è¾“å‡ºï¼Œä½†æ²¡æœ‰æ‰€éœ€çš„å‡½æ•°ã€‚

å…¶ä»–æ–¹æ³•
æ‚¨å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç§å­å’Œä¸åŒçš„äº®åº¦æ‰‹åŠ¨ç”Ÿæˆ HDRâ€œåŒ…å›´â€å›¾åƒï¼Œç„¶åŽä½¿ç”¨ä¸“ç”¨çš„ HDR åˆå¹¶å·¥å…·ï¼ˆé€šå¸¸æ˜¯å•†ä¸šæˆ–å²å‰å·¥å…·ï¼‰å°†å®ƒä»¬åˆå¹¶ã€‚Darktable ä¸ä¼šåˆå¹¶éž RAWï¼Œä½†æ‚¨å¯ä»¥åœ¨ GIMP ä¸­è¿›è¡Œæ›å…‰æ··åˆï¼š

åœ¨ GIMP ä¸­æ‰‹åŠ¨æ··åˆæ›å…‰



# Photorealism

Basic Scene Setup for Photorealism
You may use photographic terms or descriptions of visual features. The most photorealistic models should react to general or specific tokens like:

    matte beige sphere, dramatic lighting, camera f1.6 lens ,rich colors ,hyper realistic ,lifelike texture
    matte beige sphere, (hard sharp spotlight light:1.4), camera f1.6 lens ,rich colors ,hyper realistic ,lifelike texture
    matte beige sphere, (three point studio light:1.4), camera f1.6 lens ,rich colors ,hyper realistic ,lifelike texture



æ’å›¾ï¼šæ— è´Ÿé¢æç¤ºï¼Œ  Dangerhawkï¼ŒFastNegativeV2

å°†åµŒå…¥ä¸‹è½½åˆ°stable-diffusion-webui\embeddingsæ–‡ä»¶å¤¹ä¸­ï¼Œç„¶åŽé€šè¿‡ç”ŸæˆæŒ‰é’®ä¸‹æ–¹çš„æ˜¾ç¤º/éšè—é¢å¤–ç½‘ç»œæŒ‰é’®å°†å…¶æ’å…¥åˆ°è´Ÿé¢æç¤ºä¸­ï¼ˆé¦–å…ˆåˆ·æ–°åˆ—è¡¨ï¼‰ã€‚å°è¯•å…¶ä¸­å‡ ä¸ªï¼ˆCivitai ä¸Šçš„ NE åˆ—è¡¨ï¼‰ï¼Œå¹¶å°è¯•ç»„åˆã€‚æ‚¨ä¹Ÿå¯ä»¥ä»Žå®žé™…çš„å·¥ä½œæµç¨‹å¼€å§‹ï¼Œä¸»è¦åŸºäºŽè´Ÿé¢æç¤ºï¼Œè¿™æ˜¯ä¸€ä¸ªèµ·ç‚¹ï¼š

è´Ÿé¢æç¤ºï¼š(semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4) 


åŒºåŸŸæè¯å‘˜
æ‚¨å¯ä»¥ä½¿ç”¨ Regional Prompter å¿«é€Ÿå°†ä½œå“åˆ’åˆ†ä¸ºåŒºåŸŸã€‚æ‚¨å°†ä½¿ç”¨ BREAK æ¥åˆ’åˆ†æç¤ºä¸­çš„åŒºåŸŸï¼ˆæ‚¨éœ€è¦åœ¨å¼€å¤´å†ä½¿ç”¨ä¸€ä¸ª BREAK æ¥åˆ†éš”åŸºæœ¬æˆ–é€šç”¨æç¤ºï¼‰ã€‚æç¤ºæ¨¡å¼ä¼šå°è¯•æ ¹æ®æç¤ºæ£€æµ‹åŒºåŸŸæ®µï¼Œè€ŒçŸ©é˜µæ¨¡å¼å°†ä»¥å‡ ä½•æ–¹å¼åˆ’åˆ†åŒºåŸŸï¼š


æœ¬ä¾‹çš„ç»“æž„ä¸ºï¼šåŸºæœ¬æç¤º BREAK å­—ç¬¦ BREAK æž„å»ºã€‚

(background inside dark, moody:1.3) , POV, nikon d850, film stock photograph ,4 kodak portra 400 ,camera f1.6 lens ,rich colors ,hyper realistic ,lifelike texture, dramatic lighting , cinestill 800 rimlight (editorial photograph) BREAK
sfw (1woman:1.1),, (winter clothes:1.2), scarf, winter coat, view from back, (highly detailed face:1.4) (smile:0.7) BREAK
snowy house, cracked wall, decrepit


![alt text](assets/610614/image-3.png)


ä½¿ç”¨ CD Tuner å’Œ VectorscopeCC è¿›è¡Œè‰²å½©å¤„ç†
CDï¼ˆé¢œè‰²/ç»†èŠ‚ï¼‰è°ƒè°å™¨å’Œ VectorscopeCCæ˜¯ A1111 æ‰©å±•ï¼Œå¯åœ¨æ½œåœ¨ç©ºé—´ï¼ˆåç§»å™ªå£°æ–¹å¼ï¼‰ä¸­ä¿®æ”¹é¢œè‰²å’Œè‰²è°ƒã€‚å€ŸåŠ©è¿™äº›å·¥å…·ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹é¢œè‰²ç»„åˆæˆ–è°ƒæ•´å›¾åƒï¼Œåˆ›å»ºåŒ…å›´æ•ˆæžœå’Œé«˜åŠ¨æ€èŒƒå›´ HDR è¿‘ä¼¼å€¼ã€‚


Upscaling
Upscaling is a technique needed in all worklows. Explore upscaler models on upscale wiki https://openmodeldb.info/. Explote the classic ones or new DAT upscalers. See a list of interesting classic upscaler models here.

Stylistic Workflows
These workflows use extensions or LoRA-type models to modify diffusion and influence the results. We have already discussed some extensions, so here are some tips:

Dynamic Thresholding: Mimics CFG scale. We can use it for some effects too. Use higher Sampling steps (40+) to retain some color and shape information.
Latent Mirroring: Tends to create balanced or symmetrical compositions.
Anti-Burn: Can help with overtrained LoRAs or models.


æ½œåœ¨ç©ºé—´å’Œå˜åŒ–ä¸­çš„å™ªå£°æŠ€å·§
é€šè¿‡è°ƒæ•´ sigma å™ªå£°ï¼Œæ‚¨å¯ä»¥å®žçŽ°æœ‰è¶£çš„ç›¸æœºæ•ˆæžœã€‚æœ‰å…³æ­¤å†…å®¹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æœ‰å…³Sigma å’Œ Etaå‚æ•°çš„å•ç‹¬æ–‡ç« ã€‚æˆ‘åœ¨æ­¤é¢œè‰²åˆ†çº§æ•™ç¨‹ ä¸­æåˆ°äº†é¢œè‰²å’Œäº®åº¦è°ƒæ•´ã€‚ä¸€äº› LoRA å’Œæ‰©å±•ä¹Ÿä¼šå½±å“æ‰©æ•£è§£ç ã€‚ 

æ·»åŠ é›‡ä½£ä¿®å¤çš„è¯¦ç»†ä¿¡æ¯
ä¸Ž ADetailer å’Œç±»ä¼¼æ‰©å±•ç±»ä¼¼ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­æ·»åŠ å…¶ä»–æç¤ºï¼Œæ‚¨å¯ä»¥åœ¨Hires ä¿®å¤é˜¶æ®µæ·»åŠ ç…§ç‰‡ç»†èŠ‚ï¼ˆä¹Ÿå¯ä»¥åœ¨æç¤ºä¸­ä½¿ç”¨ LoRAï¼‰ã€‚å°è¯• SwinIR_4xã€4x-UltraSharp æˆ– 4x_RealisticRescaler å‡çº§å™¨ã€‚



æç¤ºå·¥ç¨‹å®žéªŒå’Œé€šé…ç¬¦
é€šé…ç¬¦æ˜¯å‘åœºæ™¯ä¸­æ·»åŠ è§†è§‰å™ªéŸ³å’Œé“å…·çš„å¥½æ–¹æ³•ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨éšæœºé€šé…ç¬¦æµ‹è¯•æ¨¡åž‹ã€‚æ‚¨éœ€è¦å®‰è£…Dynamic Prompts æ‰©å±•æ‰èƒ½å°†é€šé…ç¬¦ç®¡ç†å™¨æ”¾å…¥é€‰é¡¹å¡ä¸­ã€‚ 

æ‚¨å¯ä»¥å°†é€šé…ç¬¦æ”¾å…¥æ–‡æœ¬æ–‡ä»¶ï¼ˆä»…å°†é€šé…ç¬¦åˆ—è¡¨æ”¾åœ¨å•ç‹¬çš„è¡Œä¸­ï¼‰ï¼Œç„¶åŽå°†æ–‡ä»¶å¤åˆ¶åˆ°\stable-diffusion-webui\extensions\sd-dynamic-prompts\wildcardsæ–‡ä»¶å¤¹ä¸­ã€‚æ‚¨å¯ä»¥åˆ›å»ºæ–‡ä»¶å¤¹ç»“æž„å¹¶åµŒå…¥å…¶ä»–é€šé…ç¬¦æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨æç¤ºä¸­ä½¿ç”¨ä¸¤ä¸ªä¸‹åˆ’çº¿è¯­æ³•å¤„ç†é€šé…ç¬¦__wildcard__ï¼Œæ–‡ä»¶åä¸å¸¦ .txtã€‚ 




# æ•°æ®

https://aigc.latentcat.com/resources/image-datasets

https://aigc.latentcat.com/resources/awesome-aigc

æ•°æ®çˆ¬å–
äºŒæ¬¡å…ƒï¼šhttps://deepghs.github.io/waifuc/main/tutorials-CN/installation/index.html

æ•°æ®é›†ä¸‹è½½
ä¸‹è½½å™¨
img2datasetï¼šhttps://github.com/rom1504/img2dataset
HF Datasets
HF é•œåƒï¼šhttps://hf-mirror.com

ä¸€ç¯‡æ•™ç¨‹ï¼šhttps://zhuanlan.zhihu.com/p/663712983

ä½¿ç”¨ hf api ä¸‹è½½ï¼ˆä½¿ç”¨ cli åŒç†ï¼Œè¿™å°±æ˜¯ cli çš„å†…éƒ¨å®žçŽ°ï¼‰



æ•°æ®æ ‡æ³¨
å¤šæ¨¡æ€æ¨¡åž‹æ ‡æ³¨

LLaVAï¼šhttps://llava-vl.github.io/
CogVLMï¼šhttps://huggingface.co/spaces/THUDM/CogVLM-CogAgent
æ‰“æ ‡

WD1.4ï¼šhttps://gist.github.com/harubaru/8581e780a1cf61352a739f2ec2eef09b
WD1.5ï¼šhttps://saltacc.notion.site/saltacc/WD-1-5-Beta-3-Release-Notes-1e35a0ed1bb24c5b93ec79c45c217f63
ç¾Žå­¦è¯„åˆ†

20 ä¸‡åŠ¨æ¼«ï¼šhttps://huggingface.co/spaces/Laxhar/anime-thetic


å‹å•†

https://github.com/steven2358/awesome-generative-ai



![alt text](assets/610614/image-4.png)

https://huggingface.co/spaces/sczhou/CodeFormer




# outpainting-differential-diffusion
https://huggingface.co/blog/OzzyGT/outpainting-differential-diffusion



 raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/datasets/diffusers/community-pipelines-mirror/resolve/main/v0.29.1/pipeline_stable_diffusion_xl_differential_img2img.py


raise EntryNotFoundError(message, response) from e
huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-66754f9f-3a2993354103c03c2837e594;6430dfdf-d5a4-4fda-8c89-1f2875a72d6d)

Entry Not Found for url: https://huggingface.co/datasets/diffusers/community-pipelines-mirror/resolve/main/v0.29.1/pipeline_stable_diffusion_xl_differential_img2img.py.


curl æ˜¯å¯ä»¥çš„     
ä½†æ˜¯å¥½åƒè¿™å°æœºé“¾æŽ¥hugæœ‰é—®é¢˜å—ï¼Ÿï¼Ÿ


è¿™ä¸ªç½‘ç«™ç¡®å®žè¢«banäº†

ç„¶åŽç›´æŽ¥é‡‡ç”¨

    pipeline = StableDiffusionXLPipeline.from_pretrained(
        "SG161222/RealVisXL_V4.0",
        torch_dtype=torch.float16,
        variant="fp16",
        custom_pipeline="newlytest/diffusers/examples/community/pipeline_stable_diffusion_xl_differential_img2img.py",
        #custom_pipeline="newlytest/outpaint/differiential",
        è¿™æ ·ç«Ÿç„¶ä¸å¯ä»¥ï¼Œmixdqå°±æ˜¯è¿™æ ·åŠ è½½çš„

        #custom_pipeline="pipeline_stable_diffusion_xl_differential_img2img",
    ).to("cuda")


# æ¨¡åž‹è¾“å…¥å±‚ç»´åº¦å†³å®šçš„æ¨¡åž‹ç±»åž‹
    def switchAssumption(channelCount):
        return {
            4: "traditional",
            5: "sdv2 depth2img",
            7: "sdv2 upscale 4x",
            8: "instruct-pix2pix",
            9: "inpainting"
        }.get(channelCount, "Â¯\_(ãƒ„)_/Â¯")


æ€Žä¹ˆæ„Ÿè§‰iclightå¾ˆåƒ instruct 


    5: "sdv2 depth2img",
    7: "sdv2 upscale 4x",

è¿™ä¸¤ä¸ªæ¯”è¾ƒå¥½å¥‡


## instruct-pix2pix
instructPix2Pixæ–‡å­—ç¼–è¾‘å›¾ç‰‡æ˜¯ä¸€ç§çº¯æ–‡æœ¬ç¼–è¾‘å›¾åƒçš„æ–¹æ³•ï¼Œç”¨æˆ·æä¾›ä¸€å¼ å›¾ç‰‡å’Œæ–‡æœ¬ç¼–è¾‘æŒ‡ä»¤ï¼Œå‘Šè¯‰æ¨¡åž‹è¦åšä»€ä¹ˆï¼Œæ¨¡åž‹æ ¹æ®ç¼–è¾‘æŒ‡ä»¤ç¼–è¾‘è¾“å…¥çš„å›¾åƒï¼Œæœ€ç»ˆè¾“å‡ºç”¨æˆ·æƒ³è¦çš„å›¾åƒã€‚

å®ƒå¯ä»¥è®©ä½ é€šè¿‡ç®€å•çš„è¯­è¨€æè¿°æ¥ç”Ÿæˆç¬¦åˆè¦æ±‚çš„å›¾ç‰‡ï¼Œè€Œä¸éœ€è¦æ‰‹åŠ¨ç¼–å†™ä»£ç æˆ–è¿›è¡Œå¤æ‚çš„æ“ä½œï¼Œè¿™ä½¿å¾—å›¾åƒç”Ÿæˆæ›´åŠ é«˜æ•ˆå’Œä¾¿æ·ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ æƒ³å°†ä¸€å¼ å¤©ç©ºç…§ç‰‡è½¬æ¢ä¸ºå¤œæ™šç…§ç‰‡ï¼Œä½ åªéœ€è¦è¾“å…¥æŒ‡ä»¤ï¼šâ€œå°†å¤©ç©ºæ›¿æ¢ä¸ºé»‘æš—çš„å¤œæ™šâ€ï¼Œç„¶åŽæ¨¡åž‹å°±ä¼šè‡ªåŠ¨å°†å¤©ç©ºæ›¿æ¢ä¸ºæ˜Ÿæ˜Ÿå’Œæœˆäº®ï¼Œå¹¶å°†é¢œè‰²å’Œå…‰çº¿è°ƒæ•´ä¸ºå¤œæ™šçš„æ„Ÿè§‰ã€‚

instructPix2Pix å’ŒStable Diffusionçš„ä¸åŒ

1.instructPix2Pixä¼šå¤šå‡ºä¸€ä¸ªå›¾åƒè¾“å…¥ï¼Œåœ¨è¾“å…¥çš„æ—¶å€™å°†åŽŸå›¾æ‹¼æŽ¥åˆ°å™ªå£°å›¾ï¼Œä¼šéœ€è¦é¢å¤–çš„channelï¼Œè¿™äº›é¢å¤–çš„channelä¼šè¢«åˆå§‹åŒ–ä¸º0ï¼Œå…¶ä»–çš„æƒé‡ä¼šä½¿ç”¨é¢„è®­ç»ƒçš„Stable Diffusionåˆå§‹åŒ–ã€‚

2.åœ¨è®­ç»ƒä¸­å¢žåŠ äº†éšæœºçš„æ¡ä»¶Dropoutæ¥å¹³è¡¡æ¨¡åž‹çš„ç”Ÿæˆçš„æ ·æœ¬çš„å¤šæ ·æ€§ã€‚

3.æŽ¨ç†çš„è¿‡ç¨‹ä¸­åŠ å…¥äº†å‚æ•°å¯ä»¥è°ƒèŠ‚åŽŸå§‹å›¾åƒæ‰€å çš„æ¯”ä¾‹ã€‚


æ‰€ä»¥iclightå°±æ˜¯è¿™ä¸ªåŽŸç†

iclight fc åˆå§‹å™ªå£°å’Œå…‰æº + ï¼Œå†concatå‰æ™¯å›¾

fbcå†å¤šä¸€ä¸ªèƒŒæ™¯4é€šé“


ip2p concat å‚è€ƒå›¾


InstructPix2Pix ç”± 2 ä¸ªæ¨¡åž‹ï¼šä¸€ä¸ªè‡ªç„¶è¯­è¨€æ¨¡åž‹ï¼ˆGPT-3ï¼‰å’Œä¸€ä¸ªæ–‡å­—è½¬å›¾åƒæ¨¡åž‹ï¼ˆSDï¼‰æ‹¼æŽ¥èµ·æ¥ ä½œè€…ï¼šåˆ¹é‚£-Ksana- https://www.bilibili.com/read/cv24644846/ å‡ºå¤„ï¼šbilibili

æ¨¡åž‹çš„è®­ç»ƒéš¾åº¦åœ¨äºŽï¼Œç¬¦åˆæ¡ä»¶çš„æ•°æ®é›†åŸºæœ¬ä¸å­˜åœ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå°†è®­ç»ƒåˆ†æˆä¸¤ä¸ªéƒ¨åˆ†

    Fine-Tuning GPT-3

    å°†å‰åŽä¸¤ä¸ªæŒ‡ä»¤ç”Ÿæˆå…¶å„è‡ªå¯¹åº”çš„å›¾åƒ

é¦–å…ˆï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ä¸€ä¸ªäººå·¥è¾“å…¥çš„æ•°æ®é›†ä½œä¸º fine-tuning GPT-3 çš„è®­ç»ƒé›†ï¼Œè¿™ä¸ªè®­ç»ƒé›†çš„æ ¼å¼ä¸ºè¾“å…¥æè¿°ï¼ˆInput Captionï¼‰ï¼Œç¼–è¾‘æŒ‡ä»¤ï¼ˆEdit Instructionï¼‰å’Œç¼–è¾‘åŽæè¿°ï¼ˆEditted Captionï¼‰ 


è¿™ä¸ªè¾“å…¥çš„æ–‡æœ¬åšäº†ç‰¹æ®Šå¤„ç†     

Prompt-to-Prompt

ä¸ºäº†ä½¿ç¼–è¾‘å‰å’Œç¼–è¾‘åŽçš„å›¾åƒå…·æœ‰ä¸€è‡´æ€§ (å³ç»“æž„ã€æž„å›¾ä¸Šçš„ä¸€è‡´)ï¼Œæ¨¡åž‹é‡‡ç”¨äº† prompt-to-prompt 

prompt-to-prompt çš„å·§å¦™ä¹‹å¤„åœ¨äºŽï¼Œå›¢é˜Ÿå‘çŽ°ï¼Œæ–‡å­—å’Œå›¾åƒåƒç´ å…³è”å¯¹åº”çš„ cross-attention å±‚åŒ…å«äº†å¤§é‡çš„ç»“æž„æ€§æ¯ 


ä¾‹å¦‚ï¼Œæ›¿æ¢æ–‡å­—ï¼ˆå°†ä¸€éƒ¨åˆ† attention map æ›¿æ¢ä¸ºæ–°çš„ mapï¼‰ï¼ŒåŠ å…¥æ–°çš„æ–‡å­—ï¼ˆåœ¨åŽŸæœ‰çš„ map åŸºç¡€ä¸ŠåŠ å…¥æ–°çš„ mapï¼‰ ï¼Œä»¥åŠæ”¹å˜é‡ç‚¹ï¼ˆæ”¹å˜åŽŸæœ‰ map çš„æ¯”é‡ï¼‰

æ‰€ä»¥ï¼Œæ ¹æ® prompt-to-promptï¼Œå¯ä»¥ç”Ÿæˆå¤§é‡çš„é…å¯¹çš„å›¾åƒä½œä¸ºè®­ç»ƒç´ æã€‚ 


![alt text](assets/610614/image-5.png)


prompt-to-prompt çš„å·§å¦™ä¹‹å¤„åœ¨äºŽï¼Œå›¢é˜Ÿå‘çŽ°ï¼Œæ–‡å­—å’Œå›¾åƒåƒç´ å…³è”å¯¹åº”çš„ cross-attention å±‚åŒ…å«äº†å¤§é‡çš„ç»“æž„æ€§æ¯ 

InstructPix2Pix
Learning to Follow Image Editing Instructions
Tim Brooks*, Aleksander Holynski*, Alexei A. Efros
University of California, Berkeley
*Denotes equal contribution

CVPR 2023 (Highlight)

webui

The checkpoint is fully supported in img2img tab. No additional actions are required. Previously an extension by a contributor was required to generate pictures: it's no longer required, but should still work. Most of img2img implementation is by the same person.


To reproduce results of the original repo, use denoising of 1.0, Euler a sampler, and edit the config in configs/instruct-pix2pix.yaml to say:

    use_ema: true
    load_ema: true
instead of:

    use_ema: false

![alt text](assets/610614/image-6.png)

ä½ç½®ä¸å¤ªä¸€æ ·

ç†è®ºä¸Šå¯ä»¥åˆ©ç”¨       

ä¼°è®¡webuiç›´æŽ¥åˆ¤æ–­8é€šé“ä¸ºip2p å¯¼è‡´å†™æ­»   
å†å¤šä¸€ä¸ªå…‰æº+ çš„æ“ä½œå…¶å®žå°±èƒ½ç”¨äº†    
+ ç›¸å½“äºŽåŠ å™ªæå‡è´¨é‡äº†       

https://github.com/Klace/stable-diffusion-webui-instruct-pix2pix



æŒ‰ç†è¯´webuiä¹Ÿå¯ä»¥åŽŸç”Ÿæ”¯æŒiclight

å·²å¼ƒç”¨ï¼šæ­¤æ‰©å±•ä¸å†éœ€è¦ã€‚æˆ‘å·²å°†ä»£ç é›†æˆåˆ° Automatic1111 img2img ç®¡é“ä¸­ï¼Œå¹¶ä¸” WebUI çŽ°åœ¨å·²å°†ç”¨äºŽ instruct-pix2pix æ¨¡åž‹çš„å›¾åƒ CFG æ¯”ä¾‹å†…ç½®åˆ° img2img ç•Œé¢ä¸­ã€‚æ­¤æ‰©å±•å·²è¿‡æ—¶ã€‚

å¦‚æžœæ­¤æ‰©å±•ä¸­ç¼ºå°‘ img2img ä¸­æ‚¨æƒ³è¦çš„ä»»ä½•å†…å®¹ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼ˆå³éšæœº CFGï¼‰ï¼Œæˆ‘å¯ä»¥åˆ¶ä½œå•ç‹¬çš„æ‰©å±•æ¥æ‰©å±• webui img2imgã€‚


ä»–è¿™ä¸ªç›´æŽ¥æ”¹äº†cfg denoiser

    class CFGDenoiser(nn.Module):
        def __init__(self, model):
            super().__init__()
            self.inner_model = model

        def forward(self, z, sigma, cond, uncond, text_cfg_scale, image_cfg_scale):
            cfg_z = einops.repeat(z, "1 ... -> n ...", n=3)
            cfg_sigma = einops.repeat(sigma, "1 ... -> n ...", n=3)
            cfg_cond = {
                "c_crossattn": [torch.cat([cond["c_crossattn"][0], uncond["c_crossattn"][0], uncond["c_crossattn"][0]])],
                "c_concat": [torch.cat([cond["c_concat"][0], cond["c_concat"][0], uncond["c_concat"][0]])],
            }
            out_cond, out_img_cond, out_uncond = self.inner_model(cfg_z, cfg_sigma, cond=cfg_cond).chunk(3)
            return out_uncond + text_cfg_scale * (out_cond - out_img_cond) + image_cfg_scale * (out_img_cond - out_uncond)


è¿™æ ·å¾ˆå®¹æ˜“å’Œæ–°ç‰ˆæœ¬ä¸å…¼å®¹

 318 stars
Watchers
 6 watching
Forks
 20 forks

ä¸€å¹´å‰


Iclightçš„model patcher ä»¿ä½›ä¸ä¼š      
ä½†æ˜¯ä»è¦è¦æ±‚æœ‰launchå‡½æ•°     
ç¨³å®šæ€§å¥½ä¸€äº›


## SD2 Variation Models

reimagen

support for stable-diffusion-2-1-unclip checkpoints that are used for generating image variations.

It works in the same way as the current support for the SD2.0 depth model, in that you run it from the img2img tab, it extracts information from the input image (in this case, CLIP or OpenCLIP embeddings), and feeds those into the model in addition to the text prompt. Normally you would do this with denoising strength set to 1.0, since you don't actually want the normal img2img behaviour to have any influence on the generated image.


![alt text](assets/610614/image-7.png)


SD2.0 depth model éš¾é“ä¹Ÿæ˜¯å°† depth å›¾è½¬æˆembeddingå—

Stable unCLIP    
unCLIP is the approach behind OpenAI's DALLÂ·E 2, trained to invert CLIP image embeddings. We finetuned SD 2.1 to accept a CLIP ViT-L/14 image embedding in addition to the text encodings. This means that the model can be used to produce image variations, but can also be combined with a text-to-image embedding prior to yield a full text-to-image model at 768x768 resolution.

We provide two models, trained on OpenAI CLIP-L and OpenCLIP-H image embeddings, respectively, available from https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip. 


![alt text](assets/610614/image-8.png)


    #Start the StableUnCLIP Image variations pipeline
    pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-1-unclip", torch_dtype=torch.float16, variation="fp16"
    )
    pipe = pipe.to("cuda")

    #Get image from URL
    url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/tarsila_do_amaral.png"
    response = requests.get(url)
    init_image = Image.open(BytesIO(response.content)).convert("RGB")

    #Pipe to make the variation
    images = pipe(init_image).images
    images[0].save("tarsila_variation.png")



to launch a streamlit script than can be used to make image variations with both models (CLIP-L and OpenCLIP-H). These models can process a noise_level, which specifies an amount of Gaussian noise added to the CLIP embeddings. This can be used to increase output variance as in the following examples.



Recently, KakaoBrain openly released Karlo, a pretrained, large-scale replication of unCLIP. We introduce Stable Karlo, a combination of the Karlo CLIP image embedding prior, and Stable Diffusion v2.1-768.


## SD2.0 depth model

This stable-diffusion-2-depth model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.



Stable Diffusion v1 


Hardware Type: A100 PCIe 40GB
Hours used: 200000
Cloud Provider: AWS

## sdv2 upscale 4x

x4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model. In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.

Hardware: 32 x 8 x A100 GPUs

SD_4X å‡çº§å™¨æ›´èƒœä¸€ç­¹ï¼ˆåœ¨æˆ‘çœ‹æ¥ï¼‰ï¼Œå› ä¸ºå®ƒä¼šè€ƒè™‘ä½ çš„æ¡ä»¶ï¼Œä»Žè€Œæ ¹æ®ä½ çš„é¢„æœŸæž„å›¾ç»†åŒ–ç»†èŠ‚ã€‚å¦‚æžœåŽŸå§‹å›¾åƒæ˜¯ä½¿ç”¨ SD æ£€æŸ¥ç‚¹ç”Ÿæˆçš„ï¼Œé‚£ä¹ˆå…·æœ‰ç›¸åŒæ¡ä»¶çš„ 4X å‡çº§å™¨åº”è¯¥ä¼šäº§ç”Ÿæ›´ä¸€è‡´çš„å‡çº§æ•ˆæžœã€‚

![alt text](assets/610614/image-9.png)


https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler


7: "sdv2 upscale 4x",


, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.



    import requests
    from PIL import Image
    from io import BytesIO
    from diffusers import StableDiffusionUpscalePipeline
    import torch

    # load model and scheduler
    model_id = "stabilityai/stable-diffusion-x4-upscaler"
    pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)
    pipeline = pipeline.to("cuda")

    # let's download an  image
    url = "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png"
    response = requests.get(url)
    low_res_img = Image.open(BytesIO(response.content)).convert("RGB")
    low_res_img = low_res_img.resize((128, 128))

    prompt = "a white cat"

    upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]
    upscaled_image.save("upsampled_cat.png")

![alt text](assets/610614/image-10.png)


è„¸éƒ¨æ²¡é‚£ä¹ˆå¥½    
è¡£æœä»¿ä½›å¥½ä¸€äº›

å¯¹äºŽé’»çŸ³å¾ˆå¥½

æˆ‘å·²ç»å°è¯•è¿‡å®ƒï¼Œæ—¢æœ‰å¸¸è§„å‡çº§ï¼Œä¹Ÿæœ‰ Ultimate SD Upscaleï¼ŒTBH æˆ‘å‘çŽ° Ultimate SD Upscale æ›´å¥½ï¼Œå®ƒå…è®¸æ‚¨è½»æ¾åœ°å°†æ¨¡åž‹ä¸Žä½¿ç”¨çš„å›¾åƒäº¤æ¢ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥è®¾ç½®æ¯”ä¾‹ï¼Œå¦å¤–æˆ‘å‘çŽ°ä½¿ç”¨ SD_4X æ—¶ï¼Œå³ä½¿ä½¿ç”¨ 3090ï¼Œæˆ‘ä¹Ÿç»å¸¸è€—å°½å†…å­˜


Stable-diffusion-4x-upscaler

Stable Diffusion 4x upscaler is a text-conditioned latent diffusion model capable of upscaling images to 4x resolution.

"in_channels": 7,

ç¡®å®žæ˜¯7 ä½†æˆ‘å¥½å¥‡æ˜¯æŠŠä»€ä¹ˆä¸œè¥¿åŠ è¿›åŽ»äº†         
ä¸‰é€šé“èƒ½æ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ     

upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]

ä¸»è¦æƒ³è¿™ low_res_img å¦‚ä½•ä½¿ç”¨ï¼Œ è¿‡vaeç»“æžœä¹Ÿä¸å¯¹        


diffuser

A method for increasing the inpainting image quality is to use the [`padding_mask_crop`](https://huggingface.co/docs/diffusers/v0.25.0/en/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline.__call__.padding_mask_crop) parameter. When enabled, this option crops the masked area with some user-specified padding and it'll also crop the same area from the original image. Both the image and mask are upscaled to a higher resolution for inpainting, and then overlaid on the original image. This is a quick and easy way to improve image quality without using a separate pipeline like [`StableDiffusionUpscalePipeline`].

Add the `padding_mask_crop` parameter to the pipeline call and set it to the desired padding value.

   
image = pipeline("boat", image=base, mask_image=mask, strength=0.75, generator=generator, padding_mask_crop=32).images[0]



upscale

    def check_inputs(
        self,
        prompt,
        image,
        noise_level, è¿™ä¸ªæ¯”è¾ƒç‰¹æ®Š
        callback_steps,
        negative_prompt=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
    ):


        # check noise level
        if noise_level > self.config.max_noise_level:
            raise ValueError(f"`noise_level` has to be <= {self.config.max_noise_level} but is {noise_level}")

init

max_noise_level: int = 350,

call init

        num_inference_steps: int = 75,
        guidance_scale: float = 9.0,
        noise_level: int = 20,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        callback_steps: int = 1,
        è¿™ä¸ªä¸çŸ¥é“æ˜¯ä»€ä¹ˆ

callback_steps (`int`, *optional*, defaults to 1):
    The frequency at which the `callback` function is called. If not specified, the callback is called at
    every step.

eta (`float`, *optional*, defaults to 0.0):
    Corresponds to parameter eta (Î·) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies
    to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.



img2img

    def check_inputs(
        self,
        prompt,
        strength,
        callback_steps,
        negative_prompt=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        ip_adapter_image=None,
        ip_adapter_image_embeds=None,
        callback_on_step_end_tensor_inputs=None,
    ):



txt2img

    def check_inputs(
        self,
        prompt,
        height,
        width,
        callback_steps,
        negative_prompt=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        ip_adapter_image=None,
        ip_adapter_image_embeds=None,
        callback_on_step_end_tensor_inputs=None,
    ):



upscale

call

    # 5. set timesteps
    self.scheduler.set_timesteps(num_inference_steps, device=device)
    timesteps = self.scheduler.timesteps

    # 5. Add noise to image
    noise_level = torch.tensor([noise_level], dtype=torch.long, device=device)
    noise = randn_tensor(image.shape, generator=generator, device=device, dtype=prompt_embeds.dtype)
    åˆ›å»ºä¸€ä¸ªåŽŸå›¾å½¢çŠ¶çš„éšæœºå™ªå£°
    image = self.low_res_scheduler.add_noise(image, noise, noise_level)
    åŠ å™ªæ°´å¹³350 ä»–è¿™ä¸ªè®­ç»ƒæœ‰ç‚¹ç‰¹æ®Š åŠ å™ª350 å°±èƒ½èŽ·å–é«˜åˆ†è¾¨çŽ‡ï¼Ÿ å¥½åƒè¯´æ˜¯tileæœºåˆ¶ ç›¸å½“äºŽæ¯éƒ¨åˆ†é‡æ–°åŽ»å™ª   
    ä½†è¿˜æ˜¯å¾ˆæœ‰æ„æ€ï¼Œä¸è¿‡vae ç›´æŽ¥è¿™æ ·æžï¼Ÿ      
    é€šé“æ•°ä¹Ÿä¸å¯¹ç§°

    batch_multiplier = 2 if do_classifier_free_guidance else 1
    image = torch.cat([image] * batch_multiplier * num_images_per_prompt)
    
    cfgç¿»å€ latenåœ¨åŽé¢å†ç¿»å€
    
    noise_level = torch.cat([noise_level] * image.shape[0])

    # 6. Prepare latent variables
    height, width = image.shape[2:]
    num_channels_latents = self.vae.config.latent_channels
    latents = self.prepare_latents(
        batch_size * num_images_per_prompt,
        num_channels_latents,
        height,
        width,
        prompt_embeds.dtype,
        device,
        generator,
        latents,
    )

è¿™æ ·çœ‹å’Œiclightä¾æ—§å¾ˆæŽ¥è¿‘

ä»–ä»¬è¿™ç§concatçš„åˆ›æ–°ä¹‹å¤„æœ€å¤§åœ¨äºŽæ•°æ®å¤„ç†æ–¹å¼å’Œè®­ç»ƒæ–¹å¼


    with self.progress_bar(total=num_inference_steps) as progress_bar:
        for i, t in enumerate(timesteps):
            # expand the latents if we are doing classifier free guidance
            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents

            # concat latents, mask, masked_image_latents in the channel dimension

            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

            latent_model_input = torch.cat([latent_model_input, image], dim=1)

ç»ˆäºŽåˆäº†

ä½†æ˜¯ç»´åº¦ä¸ä¸€æ ·ï¼Œè¿™ä¸ªæ¨¡åž‹å†…éƒ¨è‡ªå·±å¤„ç†äº†å—ï¼Ÿ

            # predict the noise residual
            noise_pred = self.unet(
                latent_model_input,
                t,
                encoder_hidden_states=prompt_embeds,
                cross_attention_kwargs=cross_attention_kwargs,
                class_labels=noise_level,
                ç¡®å®žç‰¹æ®Š
                return_dict=False,
            )[0]


å¥½åƒç”¨äº†æ ‡ç­¾ï¼Ÿ

i2i

            # predict the noise residual
            noise_pred = self.unet(
                latent_model_input,
                t,
                encoder_hidden_states=prompt_embeds,
                timestep_cond=timestep_cond,
                cross_attention_kwargs=self.cross_attention_kwargs,
                added_cond_kwargs=added_cond_kwargs,
                return_dict=False,
            )[0]



### sd upscale
What's the implementation difference between highres fix and SD upscale?

According to the docs they both split into tiles and then use SD on the upscaled tiles, but they behave very differently.

tiles å›¾å—    

äº¤å‰æ·¡å…¥æ·¡å‡º

SD Upscale can leave obvious areas where it's crossfading between the tiles, which I never really notice in Highres fix.

Highres fix tends to duplicate faces across the whole image, whereas SD Upscale seems to, at a minimum, generate different faces in each tile.


hiresçš„å¤§å›¾å¥½åƒæ˜¯å› ä¸ºvae tileçš„åŽŸå› ï¼Œæ€»ä½“æ„Ÿè§‰è¿˜å¯ä»¥å§   
è‡³ä»Š24gç”¨èµ·æ¥è¿˜å¥½        
ä¸ªäººæ²¡é‡åˆ°è¿‡bad case    
é™¤éžç›´æŽ¥ç”Ÿå›¾   

SD Upscale ï¼šå¯¹æ¯ä¸ªå›¾å—æ‰§è¡Œ img2imgï¼Œç„¶åŽå°†å®ƒä»¬é‡æ–°æ··åˆåœ¨ä¸€èµ·ï¼Œå› æ­¤å®ƒå¯ä»¥å¤„ç†ä»»æ„å¤§å°çš„å›¾åƒï¼ˆåªæ˜¯æ›´å¤šç‹¬ç«‹çš„å›¾å—ï¼‰ï¼Œä½†åŽ»å™ªå¼ºåº¦å¿…é¡»éžå¸¸ä½Žï¼Œå¦åˆ™å›¾å—ä¸ä¼šå¯¹é½ã€‚

 If you enable the live preview in settings it's more obvious that the SD upscaler script upscales in tiles.

 æ„Ÿè§‰å¾ˆåƒpoor man outpaint

æ®æˆ‘ä¸ªäººäº†è§£hiresæ˜¯å¯¹latentæ”¾å¤§å†upscale å½“ç„¶å…¶å®ƒupscaleræˆ‘æ²¡å°è¯•è¿‡   
å¦‚æžœæ˜¯ganæ˜¯ä¸æ˜¯ä¸ç»è¿‡img2img?        

è‡³ä»Šæˆ‘ä»ç„¶ä¸çŸ¥é“boundingåœ¨å“ªé‡Œ ä¸€ç›´æ„Ÿè§‰è¿™äº›gan hires tilediffusion å°±å¾ˆå¥½        
å¥½åƒæ²¡å¿…è¦hidiffusionç­‰     

Really the only point in using SD upscale or Ultimate SD upscaler is if you're VRAM limited or upscaling something huge (also due to VRAM), because it performs worse at a higher denoise value since the prompt then no longer directly correlates to a full image but instead individual tiles.

Technically all the GAN upscalers should be directly upscaling the full image to 2x/4x as well (this is why the filenames usually contain these), but they're done as tiles by default instead to also save VRAM. This can be changed in settings under Upscaling.


å†…ç½®é»˜è®¤å¼€å¯tile       
æ¯”è¾ƒæ™ºèƒ½    
torchå®žçŽ°åˆ¤æ–­     

Txt2Img Highrez fix performs the Upscaling in latent space without converting to pixel space.

GAN upscalers works in pixel space.

This is only true if you choose a latent upscaler. Latent upscalers actually perform worse because upscaling in the latent space is incredibly noisy and requires a higher denoise to clean up, which eventually diverges off from the original image, so it's better to use anything else, such as a GAN-based upscaler.

æ‰€ä»¥iclight gradioä¸­ diffuserså®žçŽ°çš„åŠŸèƒ½ ä½ ä¹Ÿèƒ½è¯´ä¸å¥½å—?            


 To see what each latent space upscaler does, you can set Denoising strength to 0 and Hires steps to 1 - you'll get a very good approximation of what stable diffusion would be working with on upscaled image.


![alt text](assets/610614/image-11.png)

æ²¡è®²gan

æ‰€ä»¥æˆ‘å¦‚æžœè¦å†™hidiffusion åº”è¯¥ä¹Ÿè¦å‡†å¤‡Gançš„çŸ¥è¯†













# ç»“å°¾
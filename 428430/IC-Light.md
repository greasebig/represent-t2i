Impose Constant Light




# è®ºæ–‡ä¿¡æ¯ï¼š
controlnetä½œè€…



IC-Light æ˜¯ä¸€ä¸ªæ§åˆ¶å›¾åƒç…§æ˜çš„é¡¹ç›®ã€‚

â€œIC-Lightâ€è¿™ä¸ªåç§°ä»£è¡¨â€œImpose Constant Lightâ€ï¼ˆæˆ‘ä»¬å°†åœ¨æœ¬é¡µæœ«å°¾ç®€è¦æè¿°è¿™ä¸€ç‚¹ï¼‰ã€‚

ç›®å‰ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸¤ç§ç±»å‹çš„æ¨¡å‹ï¼šæ–‡æœ¬æ¡ä»¶é‡æ–°å…‰ç…§æ¨¡å‹å’ŒèƒŒæ™¯æ¡ä»¶æ¨¡å‹ã€‚ä¸¤ç§ç±»å‹éƒ½å°†å‰æ™¯å›¾åƒä½œä¸ºè¾“å…¥ã€‚



Related Work

Also read ...

Total Relighting: Learning to Relight Portraits for Background Replacement

Relightful Harmonization: Lighting-aware Portrait Background Replacement

SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting
About

å‹å·æ³¨é‡Š 

iclight_sd15_fc.safetensors - é»˜è®¤çš„é‡æ–°ç…§æ˜æ¨¡å‹ï¼Œä»¥æ–‡æœ¬å’Œå‰æ™¯ä¸ºæ¡ä»¶ã€‚æ‚¨å¯ä»¥ä½¿ç”¨åˆå§‹æ½œä¼æ¥å½±å“é‡æ–°ç…§æ˜ã€‚

iclight_sd15_fcon.safetensors - ä¸â€œiclight_sd15_fc.safetensorsâ€ç›¸åŒï¼Œä½†ä½¿ç”¨åç§»å™ªå£°è¿›è¡Œè®­ç»ƒã€‚è¯·æ³¨æ„ï¼Œåœ¨ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œé»˜è®¤çš„â€œiclight_sd15_fc.safetensorsâ€ç¨å¾®ä¼˜äºæ­¤æ¨¡å‹ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé»˜è®¤æ¨¡å‹æ˜¯æ²¡æœ‰åç§»å™ªå£°çš„æ¨¡å‹çš„åŸå› ã€‚    
Same as "iclight_sd15_fc.safetensors" but trained with offset noise. Note that the default "iclight_sd15_fc.safetensors" outperform this model slightly in a user study. And this is the reason why the default model is the model without offset noise.      
å†å²ç»éªŒå¯ä»¥å¾—åˆ°æ›´çº¯çš„å›¾ç‰‡é¢œè‰²

iclight_sd15_fbc.safetensors - ä»¥æ–‡æœ¬ã€å‰æ™¯å’ŒèƒŒæ™¯ä¸ºæ¡ä»¶çš„é‡æ–°ç…§æ˜æ¨¡å‹ã€‚





[ç«]5.13æ›´æ–°   
Currently ComfyUI and Forge versions are available:     
â— https://github.com/huchenlei/ComfyUI-IC-Light-Native   
â— https://github.com/huchenlei/sd-forge-ic-light    
â— https://github.com/kijai/ComfyUI-IC-Light   
I will work on A1111 extension soon.    



fbcæ¯”fcå¤šä¸€ä¸ªå›¾åƒè¾“å…¥é€šé“ï¼Œç½‘ç»œçš„è¾“å…¥è¾“å‡ºéƒ¨åˆ†ä¸å¤ªä¸€æ ·


    File "/root/miniconda3/envs/comfy/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
        return F.conv2d(input, weight, bias, self.stride,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    RuntimeError: Given groups=1, weight of size [320, 8, 3, 3], expected input[8, 12, 96, 96] to have 8 channels, but got 12 channels instead




# åŸç†

æ¢èƒŒæ™¯ï¼Œæ¢ç¯å…‰æ–¹å‘ï¼ˆå››ä¸ªï¼šä¸Šä¸‹å·¦å³ï¼‰     
æ§åˆ¶å…‰ç…§æŸ”å’Œä¸å¼ºçƒˆç¨‹åº¦ï¼Œå…‰ç§     


## Text-Conditioned Model   
è¾“å…¥ï¼šæä¾›äººç‰©å›¾ç‰‡ï¼ˆä¼šè¢«è‡ªåŠ¨æå–æœªå‰æ™¯å†è¾“å…¥æ¨¡å‹ï¼‰ï¼Œåˆ å»èƒŒæ™¯è·å–å‰æ™¯å›¾

(Note that the "Lighting Preference" are just initial latents - eg., if the Lighting Preference is "Left" then initial latent is left white right black.)      

Prompt: beautiful woman, detailed face, warm atmosphere, at home, bedroom

Lighting Preference: Left

## Background-Conditioned Model     
è¾“å…¥ï¼šæä¾›äººç‰©å›¾ç‰‡ï¼Œçº¯èƒŒæ™¯å›¾    

èƒŒæ™¯å›¾å¯ä»¥flip     


![alt text](assets/IC-Light/image-2.png)



æ¥è‡ªâ€œå¤–è§‚æ··åˆâ€å’Œâ€œå…‰æºæ··åˆâ€çš„ä¸¤ä¸ªå›¾åƒæ˜¯ä¸€è‡´çš„ï¼ˆç†æƒ³æƒ…å†µä¸‹ï¼Œåœ¨ HDR ç©ºé—´ä¸­æ•°å­¦ä¸Šæ˜¯ç­‰æ•ˆçš„ï¼‰ã€‚

åœ¨è®­ç»ƒé‡æ–°ç…§æ˜æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¼ºåŠ äº†è¿™ç§ä¸€è‡´æ€§ï¼ˆåœ¨æ½œåœ¨ç©ºé—´ä¸­ä½¿ç”¨ MLPï¼‰ã€‚

å› æ­¤ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé«˜åº¦ä¸€è‡´çš„é‡æ–°å…‰ç…§ -å¦‚æ­¤ä¸€è‡´ï¼Œç”šè‡³å¯ä»¥å°†ä¸åŒçš„é‡æ–°å…‰ç…§åˆå¹¶ä¸ºæ³•çº¿è´´å›¾ï¼å°½ç®¡äº‹å®ä¸Šè¿™äº›æ¨¡å‹æ˜¯æ½œåœ¨æ‰©æ•£çš„ã€‚
As a result, the model is able to produce highly consistent relight - so consistent that different relightings can even be merged as normal maps! Despite the fact that the models are latent diffusion.

ä»æ‰©æ•£æ¨¡å‹çš„è§’åº¦å®ç°æ‰“å…‰ï¼Œå‡ åå¹´å‰çš„æŠ€æœ¯å¤ç°


![alt text](assets/IC-Light/image-4.png)
ä»å·¦åˆ°å³ä¾æ¬¡æ˜¯è¾“å…¥ã€æ¨¡å‹è¾“å‡ºã€é‡æ–°ç…§æ˜ã€åˆ†å‰²çš„é˜´å½±å›¾åƒå’Œåˆå¹¶çš„æ³•çº¿è´´å›¾ã€‚è¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹æœªä½¿ç”¨ä»»ä½•æ³•çº¿è´´å›¾æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸ªæ­£å¸¸çš„ä¼°è®¡æ¥è‡ªäºé‡æ–°ç‚¹äº®çš„ä¸€è‡´æ€§ã€‚










## æ–½åŠ ä¸€è‡´çš„å…‰
åœ¨ HDR ç©ºé—´ä¸­ï¼Œç…§æ˜å…·æœ‰æ‰€æœ‰å…‰ä¼ è¾“éƒ½æ˜¯ç‹¬ç«‹çš„å±æ€§ã€‚     
illumination has a property that all light transports are independent.



å› æ­¤ï¼Œä¸åŒå…‰æºçš„å¤–è§‚æ··åˆç›¸å½“äºæ··åˆå…‰æºçš„å¤–è§‚ï¼š   
the blending of appearances of different light sources is equivalent to the appearance with mixed light sources:
![alt text](assets/IC-Light/image-3.png)



# ä»£ç 

## å†…éƒ¨
ç®—æ³•æµç¨‹ï¼šè¾“å…¥å‚è€ƒå›¾ -> RMBG-1.4 å‰æ™¯æå– -> i2i -> i2i

æ‰“å…‰æ–¹å‘åŸç†
the "Lighting Preference" are just initial latents - eg., if the Lighting Preference is "Left" then initial latent is left white right black.

æ¨¡å‹ç»†èŠ‚
we release two types of models: text-conditioned relighting model and background-conditioned model. Both types take foreground images as inputs.
ä½œè€…ç»™äº†ä¸¤ç§unetæ¨¡å‹ï¼Œä½¿ç”¨æ—¶åˆ†åˆ«èåˆåˆ°åº•æ¨¡ä¸­
sd_merged = {k: sd_origin[k] + sd_offset[k] for k in sd_origin.keys()}
unet.load_state_dict(sd_merged, strict=True)
unetæ¨¡å‹ç»“æ„è½»å¾®ä¿®æ”¹


å…‰æ–¹å‘åˆå§‹latent çº¿æ€§å…³ç³»

    if bg_source == BGSource.NONE:
            pass
        elif bg_source == BGSource.LEFT:
            gradient = np.linspace(255, 0, image_width)
            image = np.tile(gradient, (image_height, 1))
            input_bg = np.stack((image,) * 3, axis=-1).astype(np.uint8)
        elif bg_source == BGSource.RIGHT:
            gradient = np.linspace(0, 255, image_width)
            image = np.tile(gradient, (image_height, 1))
            input_bg = np.stack((image,) * 3, axis=-1).astype(np.uint8)
        elif bg_source == BGSource.TOP:
            gradient = np.linspace(255, 0, image_height)[:, None]
            image = np.tile(gradient, (1, image_width))
            input_bg = np.stack((image,) * 3, axis=-1).astype(np.uint8)
        elif bg_source == BGSource.BOTTOM:
            gradient = np.linspace(0, 255, image_height)[:, None]
            image = np.tile(gradient, (1, image_width))
            input_bg = np.stack((image,) * 3, axis=-1).astype(np.uint8)
        else:
            raise 'Wrong initial latent!'




conds, unconds = encode_prompt_pair(positive_prompt=prompt + ', ' + a_prompt, negative_prompt=n_prompt)


    fg = resize_and_center_crop(input_fg, image_width, image_height)

    concat_conds = numpy2pytorch([fg]).to(device=vae.device, dtype=vae.dtype)
    concat_conds = vae.encode(concat_conds).latent_dist.mode() * vae.config.scaling_factor


ç¬¬ä¸€é˜¶æ®µ i2i ï¼šLighting Preference latent ä½œä¸ºåˆå§‹åŒ– latent    
ç¬¬äºŒé˜¶æ®µ i2i ï¼šæ ¹æ®Highres scaleæ”¾å¤§

ç¬¬ä¸€é˜¶æ®µ

    bg = resize_and_center_crop(input_bg, image_width, image_height)
    bg_latent = numpy2pytorch([bg]).to(device=vae.device, dtype=vae.dtype)
    bg_latent = vae.encode(bg_latent).latent_dist.mode() * vae.config.scaling_factor
    latents = i2i_pipe(
        image=bg_latent,
        strength=lowres_denoise,
        prompt_embeds=conds,
        negative_prompt_embeds=unconds,
        width=image_width,
        height=image_height,
        num_inference_steps=int(round(steps / lowres_denoise)),
        æ•´ä¸ªè¡¨è¾¾å¼çš„ä½œç”¨å°±æ˜¯å¯¹ steps é™¤ä»¥ lowres_denoise çš„ç»“æœè¿›è¡Œå››èˆäº”å…¥ï¼Œè¿”å›æœ€æ¥è¿‘çš„æ•´æ•°å€¼ã€‚
        num_images_per_prompt=num_samples,
        generator=rng,
        output_type='latent',
        guidance_scale=cfg,
        cross_attention_kwargs={'concat_conds': concat_conds},
        è¿™ä¸ªåœ°æ–¹ç±»ä¼¼controlnet     
    ).images.to(vae.dtype) / vae.config.scaling_factor

    pixels = vae.decode(latents).sample
    pixels = pytorch2numpy(pixels)
    pixels = [resize_without_crop(
        image=p,
        target_width=int(round(image_width * highres_scale / 64.0) * 64),
        target_height=int(round(image_height * highres_scale / 64.0) * 64))
    for p in pixels]
    pixelç©ºé—´è¿›è¡Œå›¾ç‰‡æ”¾å¤§ï¼Œresize    


    pixels = numpy2pytorch(pixels).to(device=vae.device, dtype=vae.dtype)
    latents = vae.encode(pixels).latent_dist.mode() * vae.config.scaling_factor
    latents = latents.to(device=unet.device, dtype=unet.dtype)

    image_height, image_width = latents.shape[2] * 8, latents.shape[3] * 8 
    è¿™ä¸ªæ“ä½œä¸æ˜ç™½      

    fg = resize_and_center_crop(input_fg, image_width, image_height)
    concat_conds = numpy2pytorch([fg]).to(device=vae.device, dtype=vae.dtype)
    concat_conds = vae.encode(concat_conds).latent_dist.mode() * vae.config.scaling_factor

    æ²¡æœ‰å¿…è¦åšä¸¤æ¬¡



ç¬¬äºŒé˜¶æ®µ

    latents = i2i_pipe(
        image=latents,
        strength=highres_denoise,
        prompt_embeds=conds,
        negative_prompt_embeds=unconds,
        width=image_width,
        height=image_height,
        num_inference_steps=int(round(steps / highres_denoise)),
        num_images_per_prompt=num_samples,
        generator=rng,
        output_type='latent',
        guidance_scale=cfg,
        cross_attention_kwargs={'concat_conds': concat_conds},
    ).images.to(vae.dtype) / vae.config.scaling_factor

    pixels = vae.decode(latents).sample

















## gradioè¿è¡Œé”™è¯¯è§£å†³

### ç¬¬ä¸€ä¸ªé”™è¯¯

    File "/root/miniconda3/envs/iclight/lib/python3.10/site-packages/torch/cuda/__init__.py", line 293, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.

pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121    

torch                     2.3.0+cu121        
torchvision               0.18.0+cu121

ä¸åŒ¹é…        
nvcc 11.8         
nvidia-smi CUDA 11.4        



pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118


é™ä½torchç‰ˆæœ¬å¯ä»¥äº†

### ç¬¬äºŒæ¬¡é”™è¯¯
æ¨ç†æ—¶å€™ã€‚    
Segmentation fault (core dumped)     
åº”è¯¥æ˜¯c++ cå±‚é¢çš„é”™è¯¯ï¼Œç©ºæŒ‡é’ˆï¼Œå †æ ˆæº¢å‡º,tensoré—®é¢˜ç­‰      

ä»¥å‰åœ¨jetsonä¸Šä½¿ç”¨c++ç¨‹åºä¹Ÿé‡åˆ°è¿‡    

æ¢æœºå™¨é‡è£…

Nvidia-smi CUDA Version: 12.2     
Nvcc 11.8   
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121     
å¯ä»¥äº†   






## comfyui è¿è¡Œ

Chilloutmix-Ni-pruned-fp16-fix.safetensorsåº•æ¨¡     
ç”Ÿå›¾å¾ˆèŠ±     

Photon_v1_fp16.safetensorsç¬¬ä¸€æ¬¡ä¸‹è½½ä¸­æ–­ç»­ä¸‹ï¼Œè¯»å–æ—¶headeræœ‰é—®é¢˜

ç¬¬äºŒæ¬¡å®Œæ•´ä¸‹è½½

!!! Exception during processing!!! With local_files_only set to False, you must first locally save the configuration in the following path: 'openai/clip-vit-large-patch14'.

Photon_v1_fp16ä¸å«clipï¼Œéœ€è¦è°ƒç”¨ Chilloutmix-Ni-pruned-fp16-fix.safetensors çš„ clip

ç”Ÿæˆè´¨é‡è¾ƒå·®

æ’ä»¶ä½œè€…è¿˜åœ¨ä¿®æ”¹ï¼Œæ‰“è¡¥ä¸      


# forge
## æŠ¥é”™
æ¨ç†å‡ºç° Segmentation fault (core dumped)

å¹¶ä¸”è¿™ä¸ªå¹³å°ä¸ä¼šè¿”å›å…·ä½“é”™è¯¯ä¿¡æ¯     

å¡åœ¨åå‘æ¨ç†ã€‚ä¸­æ–­

    torch                     2.2.2+cu118
    torchaudio                2.2.2+cu118
    torchdiffeq               0.2.3
    torchmetrics              1.4.0
    torchsde                  0.2.6
    torchvision               0.17.2+cu118


pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121     


open-clip-torch 2.20.0 requires protobuf<4, but you have protobuf 4.25.3 which is incompatible.


ile "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/stable-diffusion-webui-forge/modules/launch_utils.py", line 431, in prepare_environment
    raise RuntimeError(
RuntimeError: Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check


pip install -U open-clip-torch

Successfully installed open-clip-torch-2.24.0

è¿˜æ˜¯å¯åŠ¨ä¸äº†launch 

    File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/stable-diffusion-webui-forge/launch.py", line 39, in main
        prepare_environment()
    File "/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/stable-diffusion-webui-forge/modules/launch_utils.py", line 431, in prepare_environment
        raise RuntimeError(
    RuntimeError: Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check


File "/root/miniconda3/envs/iclight/lib/python3.10/site-packages/torch/cuda/__init__.py", line 293, in _lazy_init
    torch._C._cuda_init()
RuntimeError: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.


nvcc 11.8     
CUDA Version: 11.4    

æˆ‘ç»™å¿˜è®°2æœºæ˜¯ä¸ªé—®é¢˜æœºäº†            


å§‹ç»ˆæ˜¯æœºå­å’ŒCUDAå’Œtorchçš„å‚»é€¼é—®é¢˜

æ¢æœºå™¨ç›´æ¥launchä¸€é”®æˆåŠŸ


## æ€§èƒ½
forgeé—²æ—¶åŠ è½½sd1.5æ˜¾å­˜2.5g   
![alt text](assets/IC-Light/image-23.png)     
![alt text](assets/IC-Light/image-24.png)




## æƒé‡
æƒé‡ä½¿ç”¨æ–¹å¼å’Œæ­£å¸¸çš„ç•¥æœ‰ä¸åŒ   
In order to load it with UnetLoader in Forge, state_dict keys need to convert to ldm format. You can download models with ldm keys here:Â https://huggingface.co/huchenlei/IC-Light-ldm/tree/main    
There are 2 models:   
â— iclight_sd15_fc_unet_ldm: Use this in FG workflows   
â— iclight_sd15_fbc_unet_ldm: Use this in BG workflows


## ç‰¹ç‚¹
UNet Patcher    
Note that Forge does not use any other software as backend. The full name of the backend is Stable Diffusion WebUI with Forge backend, or for simplicity, the Forge backend. The API and python symbols are made similar to previous software only for reducing the learning cost of developers.

Now developing an extension is super simple. We finally have a patchable UNet.

Below is using one single file with 80 lines of codes to support FreeU:

extensions-builtin/sd_forge_freeu/scripts/forge_freeu.py













# åŒç±»å·²æœ‰äº§å“æ¯”è¾ƒ
Portrait Light on Google Pixel phones

![alt text](assets/IC-Light/image-21.png)    
![alt text](assets/IC-Light/image-22.png)     



# a1111æ’ä»¶ç¼–å†™
https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Developing-extensions     


## a1111 å’Œ forge
forgeæ ¹ç›®å½•å¤šä¸¤ä¸ªæ–‡ä»¶å¤¹     
ldm_patchedå’Œmodules_forge   
æ¨¡å‹åŠ è½½å¯èƒ½ç•¥ä¸åŒ     
forge   

    from ldm_patched.modules.utils import load_torch_file
    from ldm_patched.modules.model_patcher import ModelPatcher
    from ldm_patched.modules.sd import VAE
a1111

    from modules import images, sd_samplers, processing, sd_models, sd_vae, sd_samplers_kdiffusion, errors
    from modules.processing import process_images, Processed, StableDiffusionProcessingTxt2Img
    from modules.shared import opts, state
    import modules.shared as shared
    import modules.sd_samplers
    import modules.sd_models
    import modules.sd_vae


Stable Diffusion WebUI Forge æ˜¯ä¸€ä¸ªåŸºäºStable Diffusion WebUIï¼ˆåŸºäºGradioï¼‰çš„å¹³å°ï¼Œå¯ç®€åŒ–å¼€å‘ã€ä¼˜åŒ–èµ„æºç®¡ç†å¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚      
stable diffusion webuiæ˜¯åŸºäºgradioæ¡†æ¶æ„å»ºï¼Œgradioæ˜¯ä¸€ä¸ªå¼€æºçš„pythonåº“ï¼Œå®ƒç”¨äºå¸®åŠ©ç§‘ç ”ä¸æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å·¥ä½œè€…å¿«é€Ÿçš„æ¼”ç¤ºåº”ç”¨ï¼Œä½¿ç”¨è€…ä»…éœ€è¦å‡ è¡Œä»£ç ï¼Œå°±å¯ä»¥å¿«é€Ÿæ„é€ ä¸€ä¸ªç®€å•ã€ä¸°å¯Œçš„HTMLç•Œé¢ï¼Œä¸éœ€è¦æœ‰å‰ç«¯å¼€å‘åŸºç¡€ï¼Œä»…éœ€è¦pythonåŸºç¡€å°±è¡Œã€‚    
https://github.com/gradio-app/gradio     





Forge å¸¦æ¥çš„å¦ä¸€ä¸ªéå¸¸é‡è¦çš„å˜åŒ–æ˜¯Unet Patcherã€‚ä½¿ç”¨ Unet Patcherï¼ŒSelf-Attention Guidanceã€Kohya High Res Fixã€FreeUã€StyleAlignã€Hypertile ç­‰æ–¹æ³•éƒ½å¯ä»¥åœ¨å¤§çº¦ 100 è¡Œä»£ç ä¸­å®ç°ã€‚

è¿™ä¸ªåœ¨comfyuiä¹Ÿæœ‰

æ„Ÿè°¢ Unet Patcherï¼Œè®¸å¤šæ–°çš„ä¸œè¥¿ç°åœ¨éƒ½å¯ä»¥åœ¨ Forge ä¸­å®ç°å¹¶å¾—åˆ°æ”¯æŒï¼ŒåŒ…æ‹¬ SVDã€Z123ã€masked Ip-adapterã€masked controlnetã€photomaker ç­‰ã€‚

æ— éœ€å†å¯¹ UNet è¿›è¡Œ Monkeypatch å¹¶ä¸å…¶ä»–æ‰©å±•å‘ç”Ÿå†²çªï¼

Forgeè¿˜æ·»åŠ äº†ä¸€äº›é‡‡æ ·å™¨ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºDDPMã€DDPM Karrasã€DPM++ 2M Turboã€DPM++ 2M SDE Turboã€LCM Karrasã€Euler A Turboç­‰ï¼ˆLCMä»1.7.0å¼€å§‹å°±å·²ç»åœ¨åŸå§‹webuiä¸­ï¼‰ã€‚



æ‚¨å¯ä»¥çœ‹åˆ° Forge ä¸ä¼šæ›´æ”¹ WebUI ç»“æœã€‚å®‰è£… Forge å¹¶ä¸æ˜¯ä¸€ä¸ªé‡å¤§æ”¹å˜ã€‚

å³ä½¿å¯¹äºæœ€å¤æ‚çš„æç¤ºï¼ˆä¾‹å¦‚fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5].

æ‚¨ä¹‹å‰çš„æ‰€æœ‰ä½œå“ä»ç„¶å¯ä»¥åœ¨ Forge ä¸­ä½¿ç”¨ï¼


Forge backend removes all WebUI's codes related to resource management and reworked everything. All previous CMD flags like medvram, lowvram, medvram-sdxl, precision full, no half, no half vae, attention_xxx, upcast unet, ... are all REMOVED. Adding these flags will not cause error but they will not do anything now. We highly encourage Forge users to remove all cmd flags and let Forge to decide how to load models.

æ²¡æœ‰ä»»ä½• cmd æ ‡å¿—ï¼ŒForge å¯ä»¥è¿è¡Œå…·æœ‰ 4GB vram çš„ SDXL å’Œå…·æœ‰ 2GB vram çš„ SD1.5ã€‚

å†æ¬¡å¼ºè°ƒï¼ŒForge ä¸å»ºè®®ç”¨æˆ·ä½¿ç”¨ä»»ä½• cmd æ ‡å¿—ï¼Œé™¤éæ‚¨éå¸¸ç¡®å®šç¡®å®éœ€è¦è¿™äº›æ ‡å¿—ã€‚


UNet Patcher

æ–°åŠŸèƒ½ï¼ˆåŸå§‹ WebUI ä¸­æ²¡æœ‰çš„ï¼‰    
æ„Ÿè°¢ Unet Patcherï¼Œè®¸å¤šæ–°çš„ä¸œè¥¿ç°åœ¨éƒ½å¯ä»¥åœ¨ Forge ä¸­å®ç°å¹¶å¾—åˆ°æ”¯æŒï¼ŒåŒ…æ‹¬ SVDã€Z123ã€masked Ip-adapterã€masked controlnetã€photomaker ç­‰ã€‚



ç„¶è€Œï¼Œå¦‚æœè¾ƒæ–°çš„æ‰©å±•ä½¿ç”¨ Forgeï¼Œå®ƒä»¬çš„ä»£ç å¯èƒ½ä¼šçŸ­å¾—å¤šã€‚

Usually if an old extension rework using Forge's unet patcher, é€šå¸¸ï¼Œå¦‚æœä½¿ç”¨ Forge çš„unet patcher å¯¹æ—§æ‰©å±•è¿›è¡Œè¿”å·¥ï¼Œ80% çš„ä»£ç å¯ä»¥è¢«åˆ é™¤ï¼Œç‰¹åˆ«æ˜¯å½“å®ƒä»¬éœ€è¦è°ƒç”¨controlnet æ—¶ã€‚



## å¼€å‘å¯¹æ¯”
comfyui 2023.1.17é¦–æ¬¡å‘å¸ƒ     

forgeä¸»é¡µå¥½åƒæ˜¯ä¸¤å¹´å‘å¸ƒ      

ä½†æ˜¯forgeå¤ç”¨äº†comfyuiçš„ä»£ç ï¼Œæœ‰äº›å°±æ˜¯å¾ˆåƒ      

    # 1st edit by https://github.com/comfyanonymous/ComfyUI
    # 2nd edit by Forge Official










â— https://github.com/huchenlei/sd-forge-ic-light   

â— https://github.com/kijai/ComfyUI-IC-Light   

å·²ç»æœ‰ä½œè€…çš„gradio     
æ’ä»¶å·²ç»æœ‰forgeå’Œcomfyui    
comfyuiç”šè‡³æœ‰ä¸¤ä¸ªç‰ˆæœ¬äº†      

è€Œä¸”comfyuiç»™çš„exampleè¿˜æ”¯æŒåŠ¨å›¾ï¼ŒåŠ¨æ€ä¿®æ”¹light preference     
ä½†æ˜¯forgeç‰ˆæœ¬çš„å°±å·®ä¸€äº›ï¼Œä½¿ç”¨ä½“éªŒä¸Šè¿˜ä¸å¦‚gradioã€‚gradioè¿˜èƒ½é€‰æ‹©å¤šç§exampleå‘¢ã€‚     

forgeå’Œcomfyuiéƒ½æœ‰å„è‡ªçš„æ¨¡å‹patcherã€‚ç”¨ä»¥èŠ‚çœå†…å­˜åŠ é€Ÿï¼Ÿ       
gradioåŸºæœ¬éƒ½æ˜¯ç”¨safetensor.loadfileå’Œdiffusers.from_pretrain     
forgeå¤§éƒ¨åˆ†åœ¨å¤ç”¨gradioä»£ç ã€‚å› ä¸ºæœ¬ä¸€å®¶      

comfyuiä¸Šæ²¡çœ‹è§å‰æ™¯æå–ä½¿ç”¨RMBGã€‚æœ‰ç‚¹å¥‡æ€ª      
å…¶åœ¨ä½¿ç”¨ä¸Šæ˜¯ç›´æ¥å°†åŸå›¾resizeè¿‡vae encoderè¾“å…¥åˆ°fgèŠ‚ç‚¹    

â— https://github.com/kijai/ComfyUI-IC-Light       
è¾“å…¥çš„åŒ…è£…

    for conditioning in [positive, negative]:
        c = []
        for t in conditioning:
            d = t[1].copy()
            d["concat_latent_image"] = concat_latent * multiplier
            n = [t[0], d]
            c.append(n)
        out.append(c)
    return (out[0], out[1], {"samples": out_latent})

å®ç°æ–¹æ³•      
å®åœ¨æ˜¯æ²¡çœ‹æ‡‚ä»–çš„å‰æ™¯æ˜¯æ€ä¹ˆæå–çš„ï¼Œ        
ç”¨äº†ip2pçš„æ–¹æ³•ï¼Ÿï¼Ÿï¼Ÿ     

        #Patch ComfyUI's LoRA weight application to accept multi-channel inputs. Thanks @huchenlei
        try:
            ModelPatcher.calculate_weight = calculate_weight_adjust_channel(ModelPatcher.calculate_weight)
        except:
            raise Exception("IC-Light: Could not patch calculate_weight")
        # Mimic the existing IP2P class to enable extra_conds
        def bound_extra_conds(self, **kwargs):
                return ICLight.extra_conds(self, **kwargs)
        new_extra_conds = types.MethodType(bound_extra_conds, model_clone.model)
        model_clone.add_object_patch("extra_conds", new_extra_conds)

        return (model_clone, )

    import comfy
    class ICLight:
        def extra_conds(self, **kwargs):
            out = {}
            
            image = kwargs.get("concat_latent_image", None)
            noise = kwargs.get("noise", None)
            device = kwargs["device"]

            if image is None:
                image = torch.zeros_like(noise)

            if image.shape[1:] != noise.shape[1:]:
                image = comfy.utils.common_upscale(image.to(device), noise.shape[-1], noise.shape[-2], "bilinear", "center")

            image = comfy.utils.resize_to_batch_size(image, noise.shape[0])

            process_image_in = lambda image: image
            out['c_concat'] = comfy.conds.CONDNoiseShape(process_image_in(image))
            
            adm = self.encode_adm(**kwargs)
            if adm is not None:
                out['y'] = comfy.conds.CONDRegular(adm)
            return out











## comfyuiæ’ä»¶huchenlei
https://github.com/huchenlei/ComfyUI-IC-Light-Native

[Important!] Required nodes     
You MUST install following nodes first for IC light to work properly.

ComfyUI-layerdiffuse: Although not used in the workflow, the patching of weight load in layerdiffuse is a dependency for IC-Light nodes to work properly.

Recommended nodes    

    ComfyUI-KJNodes: Provides various mask nodes to create light map.
    ComfyUI-Easy-Use: A giant node pack of everything. The remove bg node used in workflow comes from this pack.
    ComfyUI_essentials: Many useful tooling nodes. Image resize node used in the workflow comes from this pack.

è¿™ä¸ªæ’ä»¶å€’æ˜¯ä½¿ç”¨äº†RMBG      
åŸç†åº”è¯¥å’Œgradioå·®ä¸å¤š      

å®ç°ä¸Šçœ‹ä¸Šå»æ¯”ä¸Šä¸€ä¸ªcomfyuiæ–¹æ³•ç®€æ´



## a1111 webui è°ƒè¯•ç¡®å®šè¿‡ç¨‹
webui.pyè®¾ç½®äº†5ç§’é—´éš”ï¼Œç”¨å¤„æ˜¯åœ¨ç¨‹åºè¿è¡Œæ—¶å€™æ¯5ç§’ç›‘å¬ä¸€æ¬¡æœåŠ¡å™¨ç«¯çš„è¾“å…¥ï¼Œ5ç§’çš„æ—¶é—´æ®µé‡Œé¢åˆ™åœ¨è·‘ä»£ç ï¼Œå¦‚æ¨ç†     
æ¯”è¾ƒè ¢çš„è¿›å»å…·ä½“å¿åŸæ–¹æ³•æ˜¯ï¼Œåœ¨è·‘çš„é‚£5ç§’å¿«é€Ÿæš‚åœï¼Œç„¶ååˆ°è¿›ç¨‹é‚£é‡Œç‚¹ä¸‹ä¸€æ­¥ã€‚ä½†æ˜¯è¿™æ ·ä¼šæ¯è·³ä¸€æ­¥å›åˆ°æ—¶é—´ç›‘å¬ç¨‹åºä¸€æ¬¡     

è¿™æ ·è°ƒè¯•å¤ªæ…¢äº†ã€‚    
æ¯æ¬¡åªèƒ½ç§¯ç´¯åˆ°æ­£å¥½æ‰€åœæ­¥çš„å †æ ˆæŸ¥çœ‹   

æ¯”å¦‚è¿™æ¬¡    
æ­£å¥½åœåˆ°å‰å‘ä¼ æ’­çš„unetçš„SpatialTransformer    

samples_ddim = p.sample(conditioning=p.c, unconditional_conditioning=p.uc, seeds=p.seeds, subseeds=p.subseeds, subseed_strength=p.subseed_strength, prompts=p.prompts)

å…·ä½“æ¥è¯´è¿˜åŒ…è£…äº†å¾ˆå¤šä¸œè¥¿ã€‚æ¯ä¸€å±‚å®ç°ä¸€äº›åŠŸèƒ½ã€‚å¦‚cfg dpm++ç­‰   
sample_dpmpp_sde    
cfg_denoiser     
epsddpm_denoiser   
latentdiffusion     
diffusionwarpper     

pè™½ç„¶åªæœ‰4å±‚åŒ…è£…ã€‚ä½†ä¸æ˜¯è¿›å»æ¯ä¸€ä¸ªå‡½æ•°éƒ½è§£å¼€ã€‚   



### forgeå®ç° 

    work_model: ModelPatcher = p.sd_model.forge_objects.unet.clone()
    å®ƒåˆ›å»ºäº†ä¸€ä¸ªåä¸º work_model çš„å˜é‡ï¼Œè¯¥å˜é‡è¢«èµ‹äºˆäº†ä¸€ä¸ªå€¼ï¼Œè¿™ä¸ªå€¼æ˜¯ä½¿ç”¨æŸç§æ¨¡å‹åº“ï¼ˆå¯èƒ½æ˜¯ PyTorch æˆ– TensorFlow ç­‰ï¼‰ä¸­çš„ ModelPatcher ç±»çš„æ–¹æ³•æ¥åˆ›å»ºçš„ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒModelPatcher å¯èƒ½æ˜¯ä¸€ä¸ªç”¨äºä¿®æ”¹æˆ–åˆ›å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å·¥å…·ç±»æˆ–å‡½æ•°ã€‚


    vae: VAE = p.sd_model.forge_objects.vae.clone()
    unet_path = os.path.join(models_path, "unet", args.model_type.model_name)
    ic_model_state_dict = load_torch_file(unet_path, device=device)
    node = ICLight()

    patched_unet: ModelPatcher = node.apply(
        model=work_model,
        ic_model_state_dict=ic_model_state_dict,
        c_concat=args.get_c_concat(input_rgb, vae, p, device=device),
    )[0]

    p.sd_model.forge_objects.unet = patched_unet

![alt text](assets/IC-Light/WeChatd8abbe999aee08e8ecb66c0a10728b40.jpg)   
![alt text](assets/IC-Light/image-26.png)

è°ƒè¯•å¤ªæ–¹ä¾¿äº†ã€‚ä¸ä¼šè·³æ¥è·³å»

![alt text](assets/IC-Light/image-27.png)

![alt text](assets/IC-Light/image-28.png)    
![alt text](assets/IC-Light/image-29.png)
ä»–ä¸a1111çš„åŒºåˆ«çœŸçš„å¤ªå°ã€‚ä¸ä»…repoæ–‡ä»¶å¤¹åªæ˜¯æ–°å¢ï¼Œè€Œä¸”æ‰€ç”¨çš„å †æ ˆå’Œå˜é‡å†…éƒ¨ï¼Œä¹Ÿåªæ˜¯æ–°å¢ã€‚     
æ¯”å¦‚ forge_objects: åªæ˜¯åœ¨åŸæœ¬åŸºç¡€ä¸Šæ–°å¢äº† `forge_objectsï¼Œunet_patcher, BaseModel`(è¿™ä¸ªæ¥æºäºæ–°å¢ldmæ¨¡å—çš„ module) æ›¿æ¢äº†diffusion_wrapper, 

å¯èƒ½å¾—åœ¨wrapperè¿™ä¸ªåœ°æ–¹æ¢ï¼Œç„¶åè°ƒç”¨applyæ–¹æ³•    

è™½ç„¶ï¼Œè¿™ç§é‡æ„è¿˜æ˜¯å¾ˆå‰å®³çš„ï¼Œå‰åï¼Œä¸­é—´

è¿™ä¸ªdebugå®åœ¨å¤ªæ–¹ä¾¿äº†     

samples_ddim = p.sample(conditioning=p.c, unconditional_conditioning=p.uc, seeds=p.seeds, subseeds=p.subseeds, subseed_strength=p.subseed_strength, prompts=p.prompts)    
å¯åŠ¨è¿è¡Œ


è¿›å…¥è¿™é‡Œ


moduleçš„process.py

    class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):

    def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):



        if self.scripts is not None:
                    self.scripts.process_before_every_sampling(self,
                                                            x=x,
                                                            noise=x,
                                                            c=conditioning,
                                                            uc=unconditional_conditioning)

![alt text](assets/IC-Light/image-30.png)      
æ’ä»¶åŠ è½½


    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))


è¿™ä¸ªçš„å¥½å¤„è¿˜åœ¨äºå®ƒæ˜¯åœ¨ä¸»çº¿ç¨‹æ¨ç†ã€‚

a1111åˆ™åœ¨ä¸»çº¿ç¨‹è¿›è¡Œæ—¶é—´ç›‘å¬ã€‚

å¥½åƒå¯¹äºcnæ”¯æŒæ›´å¥½      
é¢„å…ˆåŠ è½½    

![alt text](assets/IC-Light/WeChat5c0fcd512f33bda79f408c044457888e.jpg)

![alt text](assets/IC-Light/WeChate71984368ee0d20bf2c69ed4c3710d74.jpg)

åˆ°è¿™é‡Œä¹‹ååŸºæœ¬æ˜¯å’Œa1111ä¸€è‡´çš„

![alt text](assets/IC-Light/image-37.png)











### a1111

![alt text](assets/IC-Light/WeChatc551c99e3a70f40a6a597427e9b4f761.jpg)

![alt text](assets/IC-Light/WeChate2115b0d48c0e6bc846b21cd6ce7f1a4.jpg)

![alt text](assets/IC-Light/WeChat110ad86b1ba569be5b154b895127809c.jpg)

UnetModel

a1111ç”¨æ¥debugç¡®å®ä¼šåœ¨shared_stateå’Œæ¨ç†ä»£ç ä¹‹é—´è·³æ¥è·³å»   
æœ€å·®åŠ²çš„è¿˜ä¼šå›è·³å›ä¸Šæ¬¡è¿è¡Œ    
è€Œä¸”è¿˜éœ€è¦æ‰“æ–­ç‚¹åœ¨å†…éƒ¨æ‰èƒ½è‡ªå·±è·³è¿›çº¿ç¨‹é‡Œé¢    

å°±æ˜¯ä¼šåœ¨ main_tread å’Œ anyio_worker_thread è·³æ¥è·³å»    

å¾ˆä¸¥é‡çš„é—®é¢˜æ˜¯ä¼šå¡ä½å¾ˆä¹…     

æŠŠç­‰å¾…äº”ç§’æ”¹æˆ0.05ç§’åè¿å¯åŠ¨ç•Œé¢éƒ½å¾ˆéš¾    
æ„Ÿè§‰å°±æ˜¯å¾ˆå¡å¾ˆæ…¢   


ä¸Šç™¾æ¬¡ç‚¹å‡»éƒ½åŠ è½½ä¸å‡ºç•Œé¢     

ä¸»è¦è¿˜æ˜¯æ–­ç‚¹è€å¸ˆåœ¨ç›‘å¬ä½ç½®è·³ï¼Œä¸ä¼šè¿›å»æ¨ç†å‡½æ•°   
ä»£ç è¿˜æ²¡æ³¨é‡Š    

å†…éƒ¨åˆåˆ†å‡ºå°çº¿ç¨‹    

å–æ¶ˆä¸»çº¿ç¨‹çš„æ–­ç‚¹ï¼Œåªæ‰“åˆ†æ”¯çº¿ç¨‹æ–­ç‚¹ï¼Œå°±å¯ä»¥åœ¨é‡Œé¢çœ‹äº†   



ä½¿ç”¨äº†treadingåŒ…çš„ _bootstrap     

ä½†æ˜¯è¿™æ ·è°ƒè¯•ä¼šå¡ä½      

ä»£ç æ˜¯èƒ½è¿›å»çœ‹ä½†æ˜¯æ€»æ˜¯å¾ˆå¡ï¼Œéº»çƒ¦ï¼Œä¸èƒ½æµç•…deåœ°ä¸€æ­¥æ­¥æ¥  

ä¸åœ¨ç­‰å¾…æ—¶é—´é‚£é‡Œæ‰“æ–­ç‚¹ï¼Œå°±ä¼šç”Ÿæˆä¸€å¼ å›¾ç‰‡åå¡ä½   
å¹¶ä¸æ˜¯å¡ä½ï¼Œåªæ˜¯åœ¨ç­‰å¾…ç½‘é¡µç«¯çš„å‘½ä»¤ï¼Œç„¶åæ¨ç†æ‰§è¡Œ    
å…«ä¸ªçº¿ç¨‹å…¨éƒ½åœ¨ç­‰å¾…     

æ‰§è¡Œè¿‡ç¨‹å¯ä»¥ç‚¹æš‚åœè¿›å…¥æŸ¥çœ‹    

è¿™ä¸ªæ—¶å€™å¯ä»¥æ­£å¸¸è¿è¡Œæ–­ç‚¹è¿›å…¥ä¸€æ­¥æ­¥æŸ¥çœ‹ï¼Œè¿™æ—¶å€™æ˜¯æ­£å¸¸ä½¿ç”¨çš„ï¼Œä¸»çº¿ç¨‹åœä½ï¼Œç„¶åç¨‹åºé‡Œé¢ä¸€æ­¥æ­¥    


è¿™æ˜¯å¦‚æœç½‘é¡µç‚¹æš‚åœï¼Œæ˜¯æ²¡æœ‰ååº”çš„ï¼Œå¥½åƒæ˜¯å› ä¸ºç­‰å¾…5ç§’   

å¥½åƒè¿è¡Œè¿‡ç¨‹ä¸­é‡‡æ ·çš„timestepsè¢«è½¬æˆäº†sigmas    
å–å€¼èŒƒå›´0-15    

æ‰€ä»¥aysåœ¨comfyuiä½¿ç”¨æ—¶å€™ä¹Ÿæ˜¯ä¸“é—¨æäº†ä¸€ä¸ªsigmasè¾“å‡ºçš„æ¨¡å—    
ä¸ºä»€ä¹ˆè¦è¿™æ ·æŠ½è±¡åŒ–     

![alt text](assets/IC-Light/image-38.png)

sigmasæ—¢ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œä¹Ÿç”¨ä»¥åœ¨samplerä¸­è®¡ç®—å™ªå£°   

![alt text](assets/IC-Light/image-40.png)

    m_sigma_min, m_sigma_max = self.model_wrap.sigmas[0].item(), self.model_wrap.sigmas[-1].item()
    sigma_min, sigma_max = (0.1, 10) if opts.use_old_karras_scheduler_sigmas else (m_sigma_min, m_sigma_max)


è¿è¡Œä¸€æ¬¡æ¨ç†åï¼Œæš‚åœç»ˆäºç›¸åº”äº†ï¼Œæ•´ä¸ªä»£ç æ˜¾ç¤ºæ­£è¿è¡Œï¼Œä½†æ²¡æœ‰å †æ ˆä¿¡æ¯    


æœ‰æ—¶è¿˜éœ€è¦é‡æ–°åˆ·æ–°ç•Œé¢ï¼Œæ‰èƒ½ä»ç½‘é¡µç«¯ä¼ è¾“è¿‡æœåŠ¡å™¨    

æ–­ç‚¹ä½ç½®æœ€å¥½å°±æ‰“åœ¨ç­‰å¾…å‡½æ•°å‰é¢ã€‚     
å…·ä½“è¿›å…¥å°±æŒ‰æš‚åœ     







## forgeè°ƒè¯•
è¿›å…¥ç½‘é¡µç«¯æ²¡é‚£ä¹ˆéº»çƒ¦ã€‚ä¸éœ€è¦ç›‘å¬æ—¶é—´å°±æ¸²æŸ“å‡ºäº†ç•Œé¢     
![alt text](assets/IC-Light/WeChatd8abbe999aee08e8ecb66c0a10728b40.jpg)

é€šè¿‡è°ƒç”¨tread.pyçš„TASKç±»è¿›è¡Œæ¯ä¸ªåŠŸèƒ½å®ç°ï¼Œå¦‚åŠ è½½ æ¨ç†    

    while True:
        time.sleep(0.01)
        if len(waiting_list) > 0:

ç›‘å¬æ—¶é—´å¤ªçŸ­      
éœ€è¦å°†è¿‘åæ¬¡æ‰ååº”è¿‡æ¥-è¿›å…¥å‡½æ•°ï¼š       




## forgeæ’ä»¶ä»£ç ä¿®æ”¹è¿‡ç¨‹ 

    """ Convert diffusers weight to ldm weight. """

    import os
    import folder_paths
    import safetensors.torch

    from comfy.diffusers_convert import convert_unet_state_dict


    def convert_weight():
        src = "iclight_sd15_fbc.safetensors"
        dest = "iclight_sd15_fbc_unet_ldm.safetensors"

        ic_light_root = os.path.join(folder_paths.models_dir, "ic_light")
        model_path = os.path.join(ic_light_root, src)

        sd_dict = convert_unet_state_dict(safetensors.torch.load_file(model_path))
        sd_dict = {key: sd_dict[key].half() for key in sd_dict.keys()}
        safetensors.torch.save_file(sd_dict, dest)

æ‰€ä»¥cnä½œè€…å¼€æºçš„æ˜¯diffusersç±»å‹çš„unetï¼Œä¹Ÿæ˜¯ä»é‚£é‡Œè®­ç»ƒæ¥çš„   
cnä½œè€…çš„gradio      
ä»diffusers.from_pretrainåŠ è½½åº•æ¨¡ï¼Œç›´æ¥å»é™¤é‡Œé¢çš„unet  

    unet = UNet2DConditionModel.from_pretrained(sd15_name, subfolder="unet")

    # Change UNet

    with torch.no_grad():
        new_conv_in = torch.nn.Conv2d(8, unet.conv_in.out_channels, unet.conv_in.kernel_size, unet.conv_in.stride, unet.conv_in.padding)
        new_conv_in.weight.zero_()
        new_conv_in.weight[:, :4, :, :].copy_(unet.conv_in.weight)
        new_conv_in.bias = unet.conv_in.bias
        unet.conv_in = new_conv_in

    unet_original_forward = unet.forward


    def hooked_unet_forward(sample, timestep, encoder_hidden_states, **kwargs):
        c_concat = kwargs['cross_attention_kwargs']['concat_conds'].to(sample)
        c_concat = torch.cat([c_concat] * (sample.shape[0] // c_concat.shape[0]), dim=0)
        new_sample = torch.cat([sample, c_concat], dim=1)
        kwargs['cross_attention_kwargs'] = {}
        return unet_original_forward(new_sample, timestep, encoder_hidden_states, **kwargs)


    unet.forward = hooked_unet_forward


    # Load

    model_path = '/teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/ComfyUI/models/unet/iclight_sd15_fc.safetensors'

    if not os.path.exists(model_path):
        download_url_to_file(url='https://huggingface.co/lllyasviel/ic-light/resolve/main/iclight_sd15_fc.safetensors', dst=model_path)

    sd_offset = sf.load_file(model_path)
    sd_origin = unet.state_dict()
    keys = sd_origin.keys()
    sd_merged = {k: sd_origin[k] + sd_offset[k] for k in sd_origin.keys()}
    unet.load_state_dict(sd_merged, strict=True)
    del sd_offset, sd_origin, sd_merged, keys


![alt text](assets/IC-Light/image-41.png)




vae =      p.sd_model.p.sd_model.first_stage_model    
clip =     p.sd_model.cond_stage_model   
warpper_unet = p.sd_model.model   


å…³äºDiffusionWrapperå®ç°     
modules/models/diffusion/ddpm_edit.py    

    """
    wild mixture of
    https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py
    å¥½åƒæ˜¯è€ä»£ç 
    https://github.com/openai/improved-diffusion/blob/e94489283bb876ac1477d5dd7709bbbd2d9902ce/improved_diffusion/gaussian_diffusion.py
    DALLE2ç»å…¸ç»“æ„trickï¼Œè¶…åˆ† 
    https://github.com/CompVis/taming-transformers
    é«˜æ•ˆtransformers
    -- merci
    """

    # File modified by authors of InstructPix2Pix from original (https://github.com/CompVis/stable-diffusion).
    # See more details in LICENSE.


    import pytorch_lightning as pl

    class DiffusionWrapper(pl.LightningModule):
        def __init__(self, diff_model_config, conditioning_key):
            super().__init__()
            self.diffusion_model = instantiate_from_config(diff_model_config)
            self.conditioning_key = conditioning_key
            assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm']
        å‰ä¸¤ä¸ªå°±æ˜¯sdç»å…¸æ¨¡å¼ï¼Œæœ€åæ˜¯unclipæ¨¡å¼    
        ä¸‹é¢å·²ç»å†™å¾—å¾ˆæ¸…æ¥šäº†ï¼Œä¸»è¦æ˜¯æŒ‡å®šæ¡ä»¶æ³¨å…¥çš„ä¸€èˆ¬æ€§æ–¹æ³•   

        def forward(self, x, t, c_concat: list = None, c_crossattn: list = None):
            if self.conditioning_key is None:
                out = self.diffusion_model(x, t)
            elif self.conditioning_key == 'concat':
                xc = torch.cat([x] + c_concat, dim=1)
                out = self.diffusion_model(xc, t)
            elif self.conditioning_key == 'crossattn':
                cc = torch.cat(c_crossattn, 1)
                out = self.diffusion_model(x, t, context=cc)
            elif self.conditioning_key == 'hybrid':
                xc = torch.cat([x] + c_concat, dim=1)
                cc = torch.cat(c_crossattn, 1)
                out = self.diffusion_model(xc, t, context=cc)
            elif self.conditioning_key == 'adm':
                cc = c_crossattn[0]
                out = self.diffusion_model(x, t, y=cc)
            else:
                raise NotImplementedError()

            return out


## forge_objects

    class ForgeSD:
        def __init__(self, unet, clip, vae, clipvision):
            self.unet = unet
            self.clip = clip
            self.vae = vae
            self.clipvision = clipvision

        def shallow_copy(self):
            return ForgeSD(
                self.unet,
                self.clip,
                self.vae,
                self.clipvision
            )




    forge_objects = load_checkpoint_guess_config(
        state_dict,
        output_vae=True,
        output_clip=True,
        output_clipvision=True,
        embedding_directory=cmd_opts.embeddings_dir,
        output_model=True
    )

å¥½åƒæ˜¯åšäº†ä¸€äº›æ›¿æ¢   
åå­—å˜äº†ä¸€ä¸‹   
å…¶ä»–æ„Ÿè§‰æ²¡å˜ï¼Ÿï¼Ÿï¼Ÿ    
åˆ’åˆ†äº†ä¸€ä¸‹    

    def load_checkpoint_guess_config(sd, output_vae=True, output_clip=True, output_clipvision=False, embedding_directory=None, output_model=True):
        sd_keys = sd.keys()
        clip = None
        clipvision = None
        vae = None
        model = None
        model_patcher = None
        clip_target = None

        parameters = ldm_patched.modules.utils.calculate_parameters(sd, "model.diffusion_model.")
        unet_dtype = model_management.unet_dtype(model_params=parameters)
        load_device = model_management.get_torch_device()
        manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device)

        class WeightsLoader(torch.nn.Module):
            pass

        model_config = model_detection.model_config_from_unet(sd, "model.diffusion_model.", unet_dtype)
        model_config.set_manual_cast(manual_cast_dtype)

        if model_config is None:
            raise RuntimeError("ERROR: Could not detect model type")

        if model_config.clip_vision_prefix is not None:
            if output_clipvision:
                clipvision = ldm_patched.modules.clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)

        if output_model:
            inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)
            offload_device = model_management.unet_offload_device()
            model = model_config.get_model(sd, "model.diffusion_model.", device=inital_load_device)
            model.load_model_weights(sd, "model.diffusion_model.")

        if output_vae:
            vae_sd = ldm_patched.modules.utils.state_dict_prefix_replace(sd, {"first_stage_model.": ""}, filter_keys=True)
            vae_sd = model_config.process_vae_state_dict(vae_sd)
            vae = VAE(sd=vae_sd)

        if output_clip:
            w = WeightsLoader()
            clip_target = model_config.clip_target()
            if clip_target is not None:
                clip = CLIP(clip_target, embedding_directory=embedding_directory)
                w.cond_stage_model = clip.cond_stage_model
                sd = model_config.process_clip_state_dict(sd)
                load_model_weights(w, sd)

        left_over = sd.keys()
        if len(left_over) > 0:
            print("left over keys:", left_over)

        if output_model:
            model_patcher = UnetPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)
            if inital_load_device != torch.device("cpu"):
                print("loaded straight to GPU")
                model_management.load_model_gpu(model_patcher)

        return ForgeSD(model_patcher, clip, vae, clipvision)









## a1111 webuiæ¶æ„
![alt text](assets/IC-Light/229259967-15556a72-774c-44ba-bab5-687f854a0fc7.png)






# å…¶ä»–

## webuiç»„ä»¶
GfpGAN, è¿™ä¸ªæ˜¯è…¾è®¯æ¨å‡ºçš„ä¸€æ¬¾åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¨¡å‹çš„ç”¨äºäººè„¸ä¿®å¤çš„ä¼˜ç§€ç»„ä»¶

pyngrok, ngrokçš„pythonå°è£…åº“ï¼Œç”¨äºç½‘ç»œé€šä¿¡ï¼Œå¯ä»¥å®ç°å†…ç½‘ç©¿é€    
ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½ç»„ä»¶å®‰è£…ï¼Œå¦‚æœå·²ä¸‹è½½ï¼Œä¼šå¿½ç•¥ä¸‹è½½å’Œå®‰è£…ã€‚é‡Œé¢ä¸»è¦æ¶‰åŠåˆ°çš„æ ¸å¿ƒç»„ä»¶æœ‰ï¼š   
2ã€taming transformers, ä¸€å¥—ç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆçš„Transformer   
3ã€k-diffusion, å¯ä»¥ç†è§£å®ƒæ˜¯å„ç§æ‰©æ•£æ¨¡å‹çš„åŒ…è£…å™¨   
4ã€CodeFormerï¼Œä¸€å¥—å¾ˆæ£’çš„å›¾åƒä¿®å¤ï¼Œè§†é¢‘å»ç çš„Pythonå·¥å…·åº“   






## SD.Next
SD.Next: Advanced Implementation of Stable Diffusion and other Diffusion-based generative image models

åˆ¶ä½œäººå‘˜   
ä¸»è¦å½’åŠŸäºAutomatic1111 WebUI çš„åŸå§‹ä»£ç åº“   
é¢å¤–å­¦åˆ†åˆ—äºå­¦åˆ†ä¸­   
æ¨¡å—çš„è®¸å¯è¯åœ¨è®¸å¯è¯ä¸­åˆ—å‡º   



![alt text](assets/IC-Light/image-25.png)

https://github.com/vladmandic/automatic

https://github.com/vladmandic/automatic/wiki/Diffusers


SD.Next supports two main backends: Diffusers and Original:

Diffusers: Based on new Huggingface Diffusers implementation   
Supports all models listed below  
This backend is set as default for new installations  
See wiki article for more information  

Original: Based on [LDM](https://github.com/Stability-AI/stablediffusion) reference implementation and significantly expanded on by A1111   
This backend and is fully compatible with most existing functionality and extensions written for A1111 SDWebUI
Supports SD 1.x and SD 2.x models   
All other model types such as SD-XL, LCM, PixArt, Segmind, Kandinsky, etc. require backend Diffusers






## controlnetä½œè€…
https://github.com/lllyasviel

å¼ å•æ•ï¼ˆLyuminZhangï¼‰æ˜¯ä¸€ååšå£«ã€‚è‡ª2022å¹´èµ·ï¼Œä»–åœ¨æ–¯å¦ç¦å¤§å­¦Maneesh Agrawalaæ•™æˆçš„æŒ‡å¯¼ä¸‹æ”»è¯»è®¡ç®—æœºç§‘å­¦ä¸“ä¸šã€‚åœ¨æ­¤ä¹‹å‰ï¼Œä»–è‡ª2021å¹´èµ·åœ¨é¦™æ¸¯ä¸­æ–‡å¤§å­¦é»„å¤©è¿›æ•™æˆå®éªŒå®¤æ‹…ä»»ç ”ç©¶åŠ©ç†ã€‚ä»–è¿˜ä¸æ•™æˆåˆä½œåŸƒå¾·åŠ Â·è¥¿è«-å¡æ‹‰ (Edgar Simo-Serra)å‚ä¸äº†è®¸å¤šæœ‰è¶£çš„é¡¹ç›®ã€‚ä»–è·å¾—äº†å·¥ç¨‹å­¦å­¦å£«å­¦ä½ã€‚ 2021å¹´äºè‹å·å¤§å­¦è·å¾—åšå£«å­¦ä½ï¼Œå¯¼å¸ˆä¸ºå­£æ¯…æ•™æˆå’Œ åˆ˜æ˜¥å¹³æ•™æˆã€‚

é—²æš‡æ—¶ï¼Œå•æ•å–œæ¬¢å¼€å‘æ¸¸æˆã€‚ Lvmin æ˜¯ä¸€æ¬¾åä¸º YGOPro2 çš„ Unity å¡ç‰Œæ¸¸æˆçš„ä½œè€…ã€‚å¦‚æœä½ åœ¨Googleæˆ–YouTubeä¸Šæœç´¢è¿™ä¸ªæ¸¸æˆï¼Œä½ ä¼šå‘ç°å®ƒå¾ˆå—æ¬¢è¿ã€‚è¯¥æ¸¸æˆå·²è¢«ç¿»è¯‘æˆå¤šç§è¯­è¨€ï¼Œåœ¨ä¸–ç•Œå„åœ°æ‹¥æœ‰ç²‰ä¸ã€‚

![alt text](assets/IC-Light/image-12.png)


### PaintingLight

Generating Digital Painting Lighting Effects via RGB-space Geometry (SIGGRAPH2020/TOG2020)


ACM Transactions on Graphics (Presented in ACM SIGGRAPH 2020), January 2020

Lvmin Zhang, Edgar Simo-Serra, Yi Ji, and Chunping Liu


æ‰“å…‰æ–¹å‘å¦ä¸€ç§å®ç°     

ic-lightæœ€å¤§çš„ç‰¹è‰²æ˜¯å…‰ç§ï¼Œå…‰æ–¹å‘çš„å¤šæ ·æ€§ï¼Œå……åˆ†ä½“ç°æ‰©æ•£æ¨¡å‹çš„ç‰¹ç‚¹ï¼Œcontrolçš„ç‰¹è‰²      
controlnetä¹Ÿå¯ä»¥ç®€å•å®ç°æ‰“å…‰æ–¹å‘     


æ—¨åœ¨å¯»æ‰¾ä¸€ç§æ“çºµæ•°å­—ç»˜ç”»ä¸­çš„ç…§æ˜çš„æ–¹æ³•ã€‚è¯¥é¡¹ç›®äº2019å¹´1æœˆå·¦å³å¯åŠ¨ï¼Œæ ¸å¿ƒç®—æ³•äº2020å¹´è¢«ACM Transitions on Graphicsæ¥å—ã€‚

ç”±äºæ•°å­—ç»˜ç”»å…‰ç…§æ•°æ®ä¸æ˜“è·å¾—ï¼Œå› æ­¤è¯¥ç®—æ³•æ²¡æœ‰ä½¿ç”¨æ·±åº¦å­¦ä¹ ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨é¢œè‰²å‡ ä½•æ¥æ„å»ºä¸€ä¸ªæ„ŸçŸ¥ä¸Šå¯è¡Œçš„é‡æ–°ç…§æ˜ç³»ç»Ÿã€‚è¿™ç§é‡æ–°ç…§æ˜å¯èƒ½åœ¨ç‰©ç†ä¸Šä¸å‡†ç¡®ï¼Œä½†å¯¹äºè‰ºæœ¯ç”¨ä¾‹æ¥è¯´å·²ç»è¶³å¤Ÿå¥½äº†ã€‚     
Because digital painting illumination data is not easy to obtain, this algorithm does not use deep learning. The core idea is to make use of `color geometry to build up a perceptually workable relighting system`. Such relighting may not be physically accurate, but are good enough for artistic use cases.     

![alt text](assets/IC-Light/image-15.png)

Q: It is mentioned that this project does not using 
   deep learning, then why it is still required to install tensorflow?

A: This is because we use SRCNN, a tensorflow neural network, to 
   pre-process input images in order to remove JPEG artifacts. Therefore 
   you still need to install tensorflow with a proper version.






æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å•ä¸ªå›¾åƒç”Ÿæˆæ•°å­—ç»˜ç”»ç…§æ˜æ•ˆæœçš„ç®—æ³•ã€‚æˆ‘ä»¬çš„ç®—æ³•åŸºäºä¸€ä¸ªå…³é”®çš„è§‚å¯Ÿï¼šè‰ºæœ¯å®¶ä½¿ç”¨è®¸å¤šé‡å çš„ç¬”ç”»æ¥ç»˜åˆ¶ç…§æ˜æ•ˆæœï¼Œå³å…·æœ‰å¯†é›†ç¬”ç”»å†å²çš„åƒç´ å¾€å¾€ä¼šæ”¶é›†æ›´å¤šçš„ç…§æ˜ç¬”ç”»ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®—æ³•ï¼Œæ—¢å¯ä»¥ä½¿ç”¨é¢œè‰²å‡ ä½•æ¥ä¼°è®¡æ•°å­—ç»˜ç”»ä¸­çš„ç¬”ç”»å¯†åº¦ï¼Œç„¶åé€šè¿‡æ¨¡ä»¿è‰ºæœ¯å®¶ä»ç²—åˆ°ç»†çš„å·¥ä½œæµç¨‹æ¥ç”Ÿæˆæ–°é¢–çš„ç¯å…‰æ•ˆæœã€‚é¦–å…ˆä½¿ç”¨æ³¢å½¢å˜æ¢ç”Ÿæˆç²—ç•¥çš„ç¯å…‰æ•ˆæœï¼Œç„¶åæ ¹æ®åŸå§‹æ’å›¾çš„ç¬”åˆ’å¯†åº¦ä¿®é¥°ä¸ºå¯ç”¨çš„ç¯å…‰æ•ˆæœã€‚
æˆ‘ä»¬çš„ç®—æ³•æ˜¯å†…å®¹æ„ŸçŸ¥çš„ï¼Œç”Ÿæˆçš„ç¯å…‰æ•ˆæœè‡ªç„¶é€‚åº”å›¾åƒç»“æ„ï¼Œå¹¶ä¸”å¯ä»¥ç”¨ä½œäº¤äº’å¼å·¥å…·æ¥ç®€åŒ–å½“å‰ä¸ºæ•°å­—å’Œå“‘å…‰ç»˜ç”»ç”Ÿæˆç¯å…‰æ•ˆæœçš„åŠ³åŠ¨å¯†é›†å‹å·¥ä½œæµç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç®—æ³•è¿˜å¯ä»¥ä¸ºç…§ç‰‡æˆ– 3D æ¸²æŸ“å›¾åƒç”Ÿæˆå¯ç”¨çš„ç¯å…‰æ•ˆæœã€‚æˆ‘ä»¬é€šè¿‡æ·±å…¥çš„å®šæ€§å’Œå®šé‡åˆ†æï¼ˆåŒ…æ‹¬æ„ŸçŸ¥ç”¨æˆ·ç ”ç©¶ï¼‰æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸ä»…èƒ½å¤Ÿç›¸å¯¹äºç°æœ‰æ–¹æ³•äº§ç”Ÿè‰¯å¥½çš„ç…§æ˜æ•ˆæœï¼Œè€Œä¸”è¿˜èƒ½å¤Ÿæ˜¾ç€å‡å°‘æ‰€éœ€çš„äº¤äº’æ—¶é—´ã€‚








### Stable Diffusion WebUI Forge

Stable Diffusion WebUI Forge æ˜¯ä¸€ä¸ªåŸºäºStable Diffusion WebUIï¼ˆåŸºäºGradioï¼‰çš„å¹³å°ï¼Œå¯ç®€åŒ–å¼€å‘ã€ä¼˜åŒ–èµ„æºç®¡ç†å¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚

â€œForgeâ€è¿™ä¸ªåå­—çš„çµæ„Ÿæ¥è‡ªäºâ€œMinecraft Forgeâ€ã€‚è¯¥é¡¹ç›®æ—¨åœ¨æˆä¸º SD WebUI çš„ Forgeã€‚

ä¸åŸå§‹ WebUIï¼ˆé’ˆå¯¹ 1024 åƒç´ çš„ SDXL æ¨ç†ï¼‰ç›¸æ¯”ï¼Œæ‚¨å¯ä»¥æœŸå¾…ä»¥ä¸‹åŠ é€Ÿï¼š

å¦‚æœæ‚¨ä½¿ç”¨å¸¸è§çš„ GPUï¼ˆå¦‚ 8GB vramï¼‰ï¼Œæ‚¨å¯ä»¥é¢„æœŸæ¨ç†é€Ÿåº¦ï¼ˆit/sï¼‰ä¼šæé«˜çº¦30~45%ï¼ŒGPU å†…å­˜å³°å€¼ï¼ˆåœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­ï¼‰å°†ä¸‹é™çº¦ 700MB è‡³ 1.3GBï¼Œæœ€å¤§æ‰©æ•£åˆ†è¾¨ç‡ï¼ˆä¸ä¼š OOMï¼‰å°†å¢åŠ çº¦ 2 å€åˆ° 3 å€ï¼Œæœ€å¤§æ‰©æ•£æ‰¹é‡å¤§å°ï¼ˆä¸ä¼š OOMï¼‰å°†å¢åŠ çº¦ 4 å€åˆ° 6 å€ã€‚

å¦‚æœæ‚¨ä½¿ç”¨åŠŸèƒ½è¾ƒå¼±çš„ GPUï¼ˆä¾‹å¦‚ 6GB vramï¼‰ï¼Œåˆ™é¢„è®¡æ¨ç†é€Ÿåº¦ï¼ˆit/sï¼‰å°†æé«˜çº¦ 60~75%ï¼ŒGPU å†…å­˜å³°å€¼ï¼ˆåœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­ï¼‰å°†ä¸‹é™çº¦ 800MB è‡³ 1.5GBï¼ˆæœ€å¤§ï¼‰æ‰©æ•£åˆ†è¾¨ç‡ï¼ˆä¸ä¼š OOMï¼‰å°†å¢åŠ çº¦ 3 å€ï¼Œæœ€å¤§æ‰©æ•£æ‰¹é‡å¤§å°ï¼ˆä¸ä¼š OOMï¼‰å°†å¢åŠ çº¦ 4 å€ã€‚

å¦‚æœæ‚¨ä½¿ç”¨åƒ 4090 è¿™æ ·å…·æœ‰ 24GB vram çš„å¼ºå¤§ GPUï¼Œæ‚¨å¯ä»¥é¢„æœŸæ¨ç†é€Ÿåº¦ (it/s) ä¼šæé«˜çº¦3~6%ï¼ŒGPU å†…å­˜å³°å€¼ï¼ˆåœ¨ä»»åŠ¡ç®¡ç†å™¨ä¸­ï¼‰å°†ä¸‹é™çº¦ 1GB è‡³ 1.4GBï¼Œæœ€å¤§æ‰©æ•£åˆ†è¾¨ç‡ï¼ˆä¸ä¼š OOMï¼‰å°†å¢åŠ çº¦ 1.6 å€ï¼Œæœ€å¤§æ‰©æ•£æ‰¹é‡å¤§å°ï¼ˆä¸ä¼š OOMï¼‰å°†å¢åŠ çº¦ 2 å€ã€‚

å¦‚æœä½¿ç”¨ ControlNet for SDXLï¼Œæœ€å¤§ ControlNet æ•°é‡ï¼ˆä¸ä¼š OOMï¼‰å°†å¢åŠ çº¦ 2 å€ï¼Œä½¿ç”¨ SDXL+ControlNet çš„é€Ÿåº¦å°†åŠ å¿«çº¦ 30~45%ã€‚

Forge å¸¦æ¥çš„å¦ä¸€ä¸ªéå¸¸é‡è¦çš„å˜åŒ–æ˜¯Unet Patcherã€‚ä½¿ç”¨ Unet Patcherï¼ŒSelf-Attention Guidanceã€Kohya High Res Fixã€FreeUã€StyleAlignã€Hypertile ç­‰æ–¹æ³•éƒ½å¯ä»¥åœ¨å¤§çº¦ 100 è¡Œä»£ç ä¸­å®ç°ã€‚

å¤šäºäº† Unet Patcherï¼Œè®¸å¤šæ–°çš„ä¸œè¥¿ç°åœ¨éƒ½å¯ä»¥åœ¨ Forge ä¸­å®ç°å¹¶å¾—åˆ°æ”¯æŒï¼ŒåŒ…æ‹¬ SVDã€Z123ã€masked Ip-adapterã€masked controlnetã€photomaker ç­‰ã€‚

æ— éœ€å†å¯¹ UNet è¿›è¡Œ Monkeypatch å¹¶ä¸å…¶ä»–æ‰©å±•å‘ç”Ÿå†²çªï¼

Forge è¿˜æ·»åŠ äº†ä¸€äº›é‡‡æ ·å™¨ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº DDPMã€DDPM Karrasã€DPM++ 2M Turboã€DPM++ 2M SDE Turboã€LCM Karrasã€Euler A Turbo ç­‰ï¼ˆLCM ä» 1.7.0 å¼€å§‹å°±å·²ç»åœ¨åŸå§‹ webui ä¸­ï¼‰ã€‚

æœ€åï¼ŒForge æ‰¿è¯ºæˆ‘ä»¬åªä¼šåšå¥½æˆ‘ä»¬çš„å·¥ä½œã€‚ Forge æ°¸è¿œä¸ä¼šå¯¹ç”¨æˆ·ç•Œé¢æ·»åŠ ä¸å¿…è¦çš„ä¸»è§‚æ›´æ”¹ã€‚æ‚¨ä»åœ¨ä½¿ç”¨ 100% è‡ªåŠ¨ 1111 WebUIã€‚




### Style2Paints
sketch + style = paints ğŸ¨ (TOG2018/SIGGRAPH2018ASIA)

![alt text](assets/IC-Light/image-8.png)

éæ‰©æ•£æ¨¡å‹    


    2022.08.15 - Lvmin's article is accepted to SIGGRAPH ASIA 2022, journal track.
    2022.06.15 - See some recent announcements of Style2Paints (Project SEPA) here.
    2022.01.09 - See some recent announcements of Style2Paints (Project SEPA) here.
    2021.06.09 - An article on shadow drawing is accepted to ICCV 2021 as Oral.
    2021.06.01 - The Project SEPA is decided to be released before 2022.
    2021.03.22 - The next version of Style2Paints will be called Project SEPA. See also the twitter post.





Help human in their standard coloring workflow!
Most human artists are familiar with this workflow:

sketching -> color filling/flattening -> gradients/details adding -> shading
And the corresponding layers are:

lineart layers + flat color layers + gradient layers + shading layers
Style2paints V4 is designed for this standard coloring workflow! In style2paints V4, you can automatically get separated results from each step!

![alt text](assets/IC-Light/image-9.png)

![alt text](assets/IC-Light/image-10.png)

![alt text](assets/IC-Light/image-11.png)


![alt text](assets/IC-Light/image-13.png)

![alt text](assets/IC-Light/image-14.png)



### fooocus


https://github.com/lllyasviel/Fooocus


About    
Focus on prompting and generating

![alt text](assets/IC-Light/image-1.png)

Fooocus is an image generating software (based on Gradio).

Fooocus is a rethinking of Stable Diffusion and Midjourneyâ€™s designs:

Learned from Stable Diffusion, the software is offline, open source, and free.

Learned from Midjourney, the manual tweaking is not needed, and users only need to focus on the prompts and images.












## Swarm UI
alternate comfyui

https://github.com/Stability-AI/StableSwarmUI


StableSwarmUI, A Modular Stable Diffusion Web-User-Interface, with an emphasis on making powertools easily accessible, high performance, and extensibility.

![alt text](assets/IC-Light/image.png)


## æ—©æœŸç ”ç©¶ä¹Ÿèƒ½æ§åˆ¶æ‰“å…‰ã€‚è€Œä¸”æ•°æ®é›†å®Œå–„


### Acquiring the Reflectance Field of a Human Face
https://www.pauldebevec.com/Research/LS/

https://www.pauldebevec.com/Research/LS/debevec-siggraph2000-high.pdf

è·å–äººè„¸åå°„åœº    
Paul Debevecã€Tim Hawkinsã€Chris Tchouã€Haarm-Pieter Duikerã€Westley Sarokin å’ŒMark Sagar      
SIGGRAPH 2000 ä¼šè®®è®ºæ–‡é›†

2004 å¹´ 4 æœˆ 10 æ—¥



æ‘˜è¦ï¼š

æˆ‘ä»¬æå‡ºäº†ä¸€ç§è·å–äººè„¸åå°„åœºçš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨è¿™äº›æµ‹é‡ç»“æœåœ¨å…‰ç…§å’Œè§†ç‚¹çš„ä»»æ„å˜åŒ–ä¸‹æ¸²æŸ“äººè„¸ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å…‰å°åœ¨å…¥å°„ç…§æ˜æ–¹å‘çš„å¯†é›†é‡‡æ ·ä¸‹ä»ä¸€å°ç»„è§†ç‚¹è·å–é¢éƒ¨å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®ç…§æ˜æ–¹å‘ç©ºé—´ä¸Šçš„æ¯ä¸ªè§‚å¯Ÿåˆ°çš„å›¾åƒåƒç´ çš„å€¼æ„å»ºåå°„å‡½æ•°å›¾åƒã€‚æ ¹æ®åå°„ç‡å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥ä»»ä½•å½¢å¼çš„é‡‡æ ·æˆ–è®¡ç®—ç…§æ˜ä»åŸå§‹è§†ç‚¹ç›´æ¥ç”Ÿæˆé¢éƒ¨å›¾åƒã€‚ä¸ºäº†æ”¹å˜è§†ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨çš®è‚¤åå°„ç‡æ¨¡å‹æ¥ä¼°è®¡æ–°è§†ç‚¹çš„åå°„ç‡å‡½æ•°çš„å¤–è§‚ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ–°é¢–çš„ç…§æ˜å’Œè§†ç‚¹ä¸‹åˆæˆäººè„¸çš„æ¸²æŸ“æ¥æ¼”ç¤ºè¯¥æŠ€æœ¯ã€‚

![alt text](assets/IC-Light/image-6.png)




### GeoWizard
GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image    

[Submitted on 18 Mar 2024]     
GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image     

https://github.com/fuxiao0719/GeoWizard

æˆ‘ä»¬å¼•å…¥äº† GeoWizardï¼Œä¸€ç§æ–°çš„ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ä»å•ä¸ªå›¾åƒä¼°è®¡å‡ ä½•å±æ€§ï¼Œä¾‹å¦‚æ·±åº¦å’Œæ³•çº¿ã€‚å°½ç®¡è¯¥é¢†åŸŸå·²ç»è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼Œä½†ç”±äºå…¬å¼€æ•°æ®é›†çš„å¤šæ ·æ€§ä½å’Œè´¨é‡å·®ï¼Œè¿›å±•å—åˆ°å¾ˆå¤§é™åˆ¶ã€‚å› æ­¤ï¼Œå…ˆå‰çš„å·¥ä½œè¦ä¹ˆå—é™äºæœ‰é™çš„åœºæ™¯ï¼Œè¦ä¹ˆæ— æ³•æ•æ‰å‡ ä½•ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜ç”Ÿæˆæ¨¡å‹ä¸ä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹ï¼ˆä¾‹å¦‚ CNN å’Œ Transformerï¼‰ç›¸åï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³å›ºæœ‰çš„ä¸é€‚å®šé—®é¢˜ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œåˆ©ç”¨æ‰©æ•£å…ˆéªŒå¯ä»¥æ˜¾ç€æé«˜æ³›åŒ–èƒ½åŠ›ã€ç»†èŠ‚ä¿ç•™å’Œèµ„æºä½¿ç”¨æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ‰©å±•äº†åŸå§‹çš„ç¨³å®šæ‰©æ•£æ¨¡å‹æ¥è”åˆé¢„æµ‹æ·±åº¦å’Œæ³•çº¿ï¼Œä»è€Œå…è®¸ä¸¤ç§è¡¨ç¤ºä¹‹é—´çš„ç›¸äº’ä¿¡æ¯äº¤æ¢å’Œé«˜åº¦ä¸€è‡´æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥ï¼Œå°†å„ç§åœºæ™¯çš„å¤æ‚æ•°æ®åˆ†å¸ƒåˆ†ç¦»æˆä¸åŒçš„å­åˆ†å¸ƒã€‚è¿™ç§ç­–ç•¥ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«ä¸åŒçš„åœºæ™¯å¸ƒå±€ï¼Œä»¥å“è¶Šçš„ä¿çœŸåº¦æ•è· 3D å‡ ä½•å›¾å½¢ã€‚ GeoWizard ä¸ºé›¶é•œå¤´æ·±åº¦å’Œæ³•çº¿é¢„æµ‹è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œæ˜¾ç€å¢å¼ºäº†è®¸å¤šä¸‹æ¸¸åº”ç”¨ï¼Œä¾‹å¦‚ 3D é‡å»ºã€2D å†…å®¹åˆ›å»ºå’Œæ–°é¢–çš„è§†ç‚¹åˆæˆã€‚     



åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒGeoWizard å°†å›¾åƒã€GT æ·±åº¦å’Œ GT æ³•çº¿é€šè¿‡å†»ç»“çš„ VAE ç¼–ç åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¹¶å½¢æˆä¸¤ä¸ªä¸²è”çš„å‡ ä½•ç»„ã€‚æ¯ä¸ªç»„éƒ½è¢«è¾“å…¥ U-Netï¼Œåœ¨å‡ ä½•åˆ‡æ¢å™¨çš„æŒ‡å¯¼ä¸‹ç”Ÿæˆæ·±åº¦æˆ–æ­£å¸¸åŸŸçš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†åœºæ™¯æç¤ºï¼Œä»¥ä½¿ç”¨ä¸‰ç§å¯èƒ½çš„åœºæ™¯å¸ƒå±€ï¼ˆå®¤å†…/å®¤å¤–/ç‰©ä½“ï¼‰ä¹‹ä¸€ç”Ÿæˆç»“æœã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç»™å®šå›¾åƒã€åœºæ™¯æç¤ºã€åˆå§‹æ·±åº¦å™ªå£°å’Œæ³•çº¿å™ªå£°ï¼ŒGeoWizard å¯ä»¥è”åˆç”Ÿæˆé«˜è´¨é‡çš„æ·±åº¦å’Œæ³•çº¿ã€‚

![alt text](assets/IC-Light/image-7.png)





### switchlight
https://arxiv.org/pdf/2402.18848




### Total Relighting:
Learning to Relight Portraits for Background Replacement   
https://augmentedperception.github.io/total_relighting/   
SIGGRAPH 2021 æŠ€æœ¯è§†é¢‘

![alt text](assets/IC-Light/image-5.png)    
è€æ–¹æ³•æ•ˆæœå·²ç»å¾ˆå¥½     
åŒ…æ‹¬ç°åœ¨è…¾è®¯ä¼šè®®çš„æ¢èƒŒæ™¯ï¼Œå°±æ˜¯æœ‰æ—¶å€™æœ‰ç‚¹æŠ–ã€‚ic-lightä¸»æ‰“çš„æ‰“å…‰ç”šè‡³éƒ½ä¸èƒ½ç®—æ–°é¢–æŠ€æœ¯    
å¯èƒ½æ˜¯å¯¹äºlimitçš„ä¼˜åŒ–å§      


æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºäººåƒé‡æ–°ç…§æ˜å’ŒèƒŒæ™¯æ›¿æ¢çš„æ–°é¢–ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä¿æŒé«˜é¢‘è¾¹ç•Œç»†èŠ‚å¹¶å‡†ç¡®åˆæˆæ–°é¢–ç…§æ˜ä¸‹çš„ä¸»ä½“å¤–è§‚ï¼Œä»è€Œä¸ºä»»ä½•æ‰€éœ€åœºæ™¯ç”Ÿæˆé€¼çœŸçš„åˆæˆå›¾åƒã€‚æˆ‘ä»¬çš„æŠ€æœ¯åŒ…æ‹¬é€šè¿‡ Alpha æŠ å›¾ã€é‡æ–°ç…§æ˜å’Œåˆæˆè¿›è¡Œå‰æ™¯ä¼°è®¡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™äº›é˜¶æ®µä¸­çš„æ¯ä¸€ä¸ªéƒ½å¯ä»¥åœ¨é¡ºåºç®¡é“ä¸­å¤„ç†ï¼Œæ— éœ€ä½¿ç”¨å…ˆéªŒï¼ˆä¾‹å¦‚å·²çŸ¥èƒŒæ™¯æˆ–å·²çŸ¥ç…§æ˜ï¼‰ï¼Œä¹Ÿæ— éœ€ä¸“é—¨çš„é‡‡é›†æŠ€æœ¯ï¼Œä»…ä½¿ç”¨å•ä¸ª RGB è‚–åƒå›¾åƒå’Œæ–°é¢–çš„ç›®æ ‡ HDR ç…§æ˜ç¯å¢ƒä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬ä½¿ç”¨åœ¨å…‰çº§è®¡ç®—ç…§æ˜ç³»ç»Ÿä¸­æ•è·çš„å¯¹è±¡çš„é‡ç…§è‚–åƒæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¯¥ç³»ç»Ÿè®°å½•äº†å¤šç§ç…§æ˜æ¡ä»¶ã€é«˜è´¨é‡çš„å‡ ä½•å½¢çŠ¶å’Œå‡†ç¡®çš„ alpha é®ç½©ã€‚ä¸ºäº†æ‰§è¡Œé€¼çœŸçš„é‡æ–°ç…§æ˜ä»¥è¿›è¡Œåˆæˆï¼Œæˆ‘ä»¬åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ¯åƒç´ ç…§æ˜è¡¨ç¤ºï¼Œå®ƒæ˜ç¡®åœ°æ¨¡æ‹Ÿäº†å¤–è§‚çš„æ¼«åå°„å’Œé•œé¢åå°„åˆ†é‡ï¼Œç”Ÿæˆå…·æœ‰ä»¤äººä¿¡æœçš„æ¸²æŸ“éæœ—ä¼¯æ•ˆæœï¼ˆå¦‚é•œé¢é«˜å…‰ï¼‰çš„é‡æ–°ç…§æ˜è‚–åƒã€‚å¤šæ¬¡å®éªŒå’Œæ¯”è¾ƒè¡¨æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•åº”ç”¨äºé‡å¤–å›¾åƒæ—¶çš„æœ‰æ•ˆæ€§ã€‚




### Relightful Harmonization
[Submitted on 11 Dec 2023 (v1), last revised 7 Apr 2024 (this version, v2)]      
Relightful Harmonization: Lighting-aware Portrait Background Replacement

è‚–åƒåè°ƒæ—¨åœ¨å°†æ‹æ‘„å¯¹è±¡åˆæˆåˆ°æ–°çš„èƒŒæ™¯ä¸­ï¼Œè°ƒæ•´å…¶ç¯å…‰å’Œé¢œè‰²ä»¥ç¡®ä¿ä¸èƒŒæ™¯åœºæ™¯çš„å’Œè°ã€‚ç°æœ‰çš„åè°ƒæŠ€æœ¯é€šå¸¸åªä¸“æ³¨äºè°ƒæ•´å‰æ™¯çš„å…¨å±€é¢œè‰²å’Œäº®åº¦ï¼Œè€Œå¿½ç•¥äº†èƒŒæ™¯ä¸­çš„å…³é”®ç…§æ˜çº¿ç´¢ï¼Œä¾‹å¦‚æ˜æ˜¾çš„ç…§æ˜æ–¹å‘ï¼Œä»è€Œå¯¼è‡´ä¸åˆ‡å®é™…çš„æ„å›¾ã€‚æˆ‘ä»¬æ¨å‡º Relightful Harmonizationï¼Œè¿™æ˜¯ä¸€ç§ç…§æ˜æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ä½¿ç”¨ä»»ä½•èƒŒæ™¯å›¾åƒæ— ç¼åè°ƒå‰æ™¯è‚–åƒçš„å¤æ‚ç…§æ˜æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸‰ä¸ªé˜¶æ®µå±•å¼€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªç…§æ˜è¡¨ç¤ºæ¨¡å—ï¼Œè¯¥æ¨¡å—å…è®¸æˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹å¯¹æ¥è‡ªç›®æ ‡å›¾åƒèƒŒæ™¯çš„ç…§æ˜ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯¹é½ç½‘ç»œï¼Œå®ƒå°†ä»å›¾åƒèƒŒæ™¯ä¸­å­¦ä¹ åˆ°çš„ç…§æ˜ç‰¹å¾ä¸ä»å…¨æ™¯ç¯å¢ƒåœ°å›¾ä¸­å­¦ä¹ åˆ°çš„ç…§æ˜ç‰¹å¾å¯¹é½ï¼Œè¿™æ˜¯åœºæ™¯ç…§æ˜çš„å®Œæ•´è¡¨ç¤ºã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥æé«˜æ‰€æå‡ºæ–¹æ³•çš„çœŸå®æ„Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ•°æ®æ¨¡æ‹Ÿç®¡é“ï¼Œè¯¥ç®¡é“å¯ä»¥ä»å„ç§è‡ªç„¶å›¾åƒä¸­ç”Ÿæˆåˆæˆè®­ç»ƒå¯¹ï¼Œç”¨äºç»†åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰ä¿çœŸåº¦å’Œç…§æ˜è¿è´¯æ€§æ–¹é¢ä¼˜äºç°æœ‰åŸºå‡†ï¼Œåœ¨ç°å®æµ‹è¯•åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œçªå‡ºäº†å…¶å¤šåŠŸèƒ½æ€§å’Œå®ç”¨æ€§ã€‚




## Photon (sd1.5åº•æ¨¡)
Photon aims to generate photorealistic and visually appealing images effortlessly.

Recommendation for generating the first image with Photon:

Prompt: A simple sentence in natural language describing the image.

Negative: "cartoon, painting, illustration, (worst quality, low quality, normal quality:2)"

Sampler: DPM++ 2M Karras | Steps: 20 | CFG Scale: 6

Size: 512x768 or 768x512

Hires.fix: R-ESRGAN 4x+ | Steps: 10 | Denoising: 0.45 | Upscale x 2

(avoid using negative embeddings unless absolutely necessary)


### development
The development process was somewhat chaotic but essentially:

It started from an old mix.

LORAs were trained on various topics using AI-generated photorealistic images.

These LORAs were mixed within the model using different weights.

In the midst of this mixing, hand generation broke.

LORAs were generated and remixed in an attempt to fix hand generation (not entirely successful).

### limit
In future versions, I will try to:

Completely eliminate the need for a negative prompt to generate high-quality images.

Fix the hand generation issue to minimize instances of poorly drawn hands.

Explore more automated training processes. I would love to have 5,000 or 50,000 high-quality AI-generated photorealistic images for training purposes.




## maskè·å–
comfyui sam mask     



## å‰æ™¯æå–å·¥å…· briaai/RMBG-1.4 

BRIA Background Removal v1.4 


![alt text](assets/IC-Light/image-16.png)

MBG v1.4 æ˜¯æˆ‘ä»¬æœ€å…ˆè¿›çš„èƒŒæ™¯å»é™¤æ¨¡å‹ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å°†å„ç§ç±»åˆ«å’Œå›¾åƒç±»å‹çš„å‰æ™¯ä¸èƒŒæ™¯åˆ†å¼€ã€‚è¯¥æ¨¡å‹å·²ç»åœ¨ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šä¸€èˆ¬åº“å­˜å›¾åƒã€ç”µå­å•†åŠ¡ã€æ¸¸æˆå’Œå¹¿å‘Šå†…å®¹ï¼Œä½¿å…¶é€‚åˆæ”¯æŒå¤§è§„æ¨¡ä¼ä¸šå†…å®¹åˆ›å»ºçš„å•†ä¸šç”¨ä¾‹ã€‚å…¶å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¤šåŠŸèƒ½æ€§å¯ä¸ç›®å‰é¢†å…ˆçš„å¯ç”¨æºæ¨¡å‹ç›¸åª²ç¾ã€‚å½“å†…å®¹å®‰å…¨ã€åˆæ³•è®¸å¯çš„æ•°æ®é›†å’Œåè§ç¼“è§£è‡³å…³é‡è¦æ—¶ï¼Œå®ƒæ˜¯ç†æƒ³çš„é€‰æ‹©ã€‚


Bria-RMBG æ¨¡å‹ä½¿ç”¨è¶…è¿‡ 12,000 å¼ é«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡ã€æ‰‹åŠ¨æ ‡è®°ï¼ˆåƒç´ ç²¾åº¦ï¼‰ã€å®Œå…¨è®¸å¯çš„å›¾åƒè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„åŸºå‡†åŒ…æ‹¬å¹³è¡¡çš„æ€§åˆ«ã€å¹³è¡¡çš„ç§æ—å’Œä¸åŒç±»å‹çš„æ®‹ç–¾äººã€‚

å›¾ç‰‡åˆ†å¸ƒï¼š

ç±»åˆ«	åˆ†é…
ä»…å¯¹è±¡	45.11%
æœ‰ç‰©ä½“/åŠ¨ç‰©çš„äºº	25.24%
ä»…é™äºº	17.35%
å¸¦æœ‰æ–‡å­—çš„äºº/ç‰©ä½“/åŠ¨ç‰©	8.52%
çº¯æ–‡æœ¬	2.52%
ä»…é™åŠ¨ç‰©	1.89%

ç±»åˆ«	åˆ†é…
é€¼çœŸ	87.70%
éçœŸå®æ„Ÿ	12.30%

ç±»åˆ«	åˆ†é…
éçº¯è‰²èƒŒæ™¯	52.05%
çº¯è‰²èƒŒæ™¯	47.95%

ç±»åˆ«	åˆ†é…
å•ä¸ªä¸»è¦å‰æ™¯å¯¹è±¡	51.42%
å‰æ™¯ä¸­æœ‰å¤šä¸ªå¯¹è±¡	48.58%

Architecture

RMBG v1.4 is developed on the IS-Net enhanced with our unique training scheme and proprietary dataset. These modifications significantly improve the modelâ€™s accuracy and effectiveness in diverse image-processing scenarios.

RMBG v1.4 æ˜¯åœ¨IS-Netä¸Šå¼€å‘çš„ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬ç‹¬ç‰¹çš„è®­ç»ƒæ–¹æ¡ˆå’Œä¸“æœ‰æ•°æ®é›†è¿›è¡Œäº†å¢å¼ºã€‚è¿™äº›ä¿®æ”¹æ˜¾ç€æé«˜äº†æ¨¡å‹åœ¨ä¸åŒå›¾åƒå¤„ç†åœºæ™¯ä¸­çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚

### Dichotomous Image Segmentation (DIS)
https://github.com/xuebinqin/DIS

è¿™æ˜¯æˆ‘ä»¬æ–°é¡¹ç›®é«˜ç²¾åº¦äºŒåˆ†å›¾åƒåˆ†å‰²çš„å­˜å‚¨åº“

é«˜ç²¾åº¦äºŒåˆ†å›¾åƒåˆ†å‰²ï¼ˆECCV 2022ï¼‰    
ç§¦å­¦æ–Œã€æˆ´èˆªã€èƒ¡æ™“æ–Œã€èŒƒé‚“å¹³*ã€é‚µå‡Œã€Luc Van Goolã€‚

![alt text](assets/IC-Light/image-17.png)

![alt text](assets/IC-Light/image-18.png)


![alt text](assets/IC-Light/image-19.png)


![alt text](assets/IC-Light/image-20.png)

æˆ‘ä»¬ä¹‹å‰çš„ä½œå“ï¼šU 2 -Netï¼ŒBASNetã€‚



## GLIDE


GLIDE

GLIDE[1] æ˜¯ OpenAI åœ¨ 2021 å¹´åº•æ¨å‡ºçš„æ–‡æœ¬å¼•å¯¼å›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ã€‚GLIDE æ²¿ç”¨äº† ADM[2] æ¶æ„ï¼Œä½†æ˜¯æ›´å¤§ï¼Œæœ‰ 2.3 billion å‚æ•°ã€‚

GLIDE æ²¿ç”¨äº† ADM[2] æ¶æ„ï¼Œä½†æ˜¯æ›´å¤§ï¼Œæœ‰ 2.3 billion å‚æ•°ã€‚ä¸ºäº†å‘å…¶ä¸­æ³¨å…¥æ–‡æœ¬æ¡ä»¶ï¼Œä½œè€…é¦–å…ˆå°†è¾“å…¥æ–‡æœ¬é€šè¿‡ BPE tokenizer ç¼–ç æˆäº† ä¸ª tokensï¼Œç„¶åç»ç”±ä¸€ä¸ªæœ‰ 1.2 billion å‚æ•°çš„ Transformer å¾—åˆ° ä¸ª token embeddingsï¼Œå®ƒä»¬è¢«èå…¥äº† UNet çš„æ¯ä¸€ä¸ª Attention Block ä¹‹ä¸­ï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ï¼›å¦å¤–ï¼Œå–æœ€åä¸€ä¸ª token embedding ç»è¿‡ç»´åº¦æ˜ å°„åä¸ time embedding ç›¸åŠ ï¼Œèå…¥ UNet çš„æ¯ä¸€ä¸ª ResBlock ä¹‹ä¸­ï¼Œç›¸å½“äºæ›¿æ¢äº† ADM ä¸­çš„ class embedding. 

ç®€è€Œè¨€ä¹‹ï¼Œé™¤äº†ä½¿ç”¨ AdaGNï¼ŒGLIDE è¿˜åœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚èå…¥äº†æ–‡æœ¬æ¡ä»¶ã€‚å¦å¤–ï¼Œä¸Šè¿° 3.5 billion å‚æ•°ï¼ˆ2.3+1.2=3.5ï¼‰çš„æ¨¡å‹åªç”Ÿæˆ 64x64 å›¾åƒï¼Œä½œè€…è¿˜æ„å»ºäº†å¦ä¸€ä¸ªç±»ä¼¼çš„ã€æœ‰ 1.5 billion å‚æ•°çš„æ¨¡å‹æŠŠå›¾åƒä¸Šé‡‡æ ·è‡³ 256x256.

å…³äºæ–‡æœ¬å¼•å¯¼ï¼Œä½œè€…å°è¯•äº†ä¸¤ç§æ–¹æ³•â€”â€”CLIP guidance å’Œ classifier-free guidanceï¼Œå®éªŒå‘ç°åè€…æ•ˆæœæ›´å¥½ã€‚

![alt text](assets/IC-Light/image-31.png)


## unclip
2022.3

è™½ç„¶æœ‰äº† GLIDEï¼Œä½† OpenAI è¿˜ä¸æ»¡è¶³ï¼Œå››ä¸ªæœˆååˆæ¨å‡ºäº†å¦ä¸€ä¸ªæ–‡æœ¬å¼•å¯¼å›¾åƒç”Ÿæˆæ¨¡å‹ unCLIP[3]ï¼Œä¹Ÿç§°ä½œ DALLÂ·E 2.

DALLÂ·E 2 æ˜¯ä¸€ä¸ª two-stage æ¨¡å‹ï¼šé¦–å…ˆä½¿ç”¨ä¸€ä¸ª prior ä» text embedding ç”Ÿæˆå¯¹åº”çš„ image embeddingï¼›ç„¶åä½¿ç”¨ä¸€ä¸ª decoder æ ¹æ® image embedding ç”Ÿæˆå›¾åƒï¼Œå¦‚ä¸‹å›¾è™šçº¿ä»¥ä¸‹éƒ¨åˆ†æ‰€ç¤ºï¼š

![alt text](assets/IC-Light/v2-c7a5595ef3927b800bf602fcb7ada16b_720w.webp)


ä¸‹é¢æˆ‘ä»¬åˆ†åˆ«å°± prior å’Œ decoder åšè¿›ä¸€æ­¥çš„è¯´æ˜ã€‚

Decoder

Decoder æ˜¯ä¸€ä¸ªä»¥ CLIP image embedding ä¸ºæ¡ä»¶çš„æ‰©æ•£æ¨¡å‹ï¼Œå…¶èå…¥æ¡ä»¶çš„æ–¹å¼æ˜¯åœ¨ GLIDE çš„åŸºç¡€ä¸Šä¿®æ”¹è€Œæ¥ï¼š

è¿™é‡Œæ˜¯è®²è®­ç»ƒè¿‡ç¨‹ã€‚éæ¨ç†

    å°† image embedding æŠ•å½±åä¸ time embedding ç›¸åŠ ï¼›
    å°† image embedding æŠ•å½±ä¸ºå››ä¸ªé¢å¤–çš„ tokensï¼Œconcatenate åˆ° GLIDE text encoder çš„è¾“å‡ºåºåˆ—ä¹‹åã€‚ä½œè€…ä¿ç•™äº† GLIDE çš„ text conditioning pathwayï¼Œå¸Œæœ›èƒ½ä¸ºæ¨¡å‹å¸¦æ¥ CLIP ä¸å…·å¤‡çš„æ€§è´¨ï¼ˆå¦‚ variable bindingï¼‰ï¼Œä½†å®éªŒå‘ç°è¿™å¹¶æ²¡æœ‰å‘æŒ¥ä½œç”¨ã€‚

å¦å¤–ï¼Œä½œè€…ä¹Ÿé‡‡å–äº† classifier-free guidanceï¼Œåœ¨è®­ç»ƒæ—¶ä»¥ 10% çš„æ¦‚ç‡å°† image embedding ç½®é›¶ï¼ˆæˆ–ç½®ä¸ºä¸€ä¸ªå¯å­¦ä¹ çš„ embeddingï¼‰ï¼Œå¹¶ä»¥ 50% çš„æ¦‚ç‡ä¸¢å¼ƒ text caption.

ä¸ºäº†ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä½œè€…è¿˜ç”¨äº†ä¸¤ä¸ªä¸Šé‡‡æ ·æ‰©æ•£æ¨¡å‹ï¼Œ64x64 â†’ 256x256 â†’ 1024x1024. åŒ SR3[4] å’Œ CDM[5] ä¸€æ ·ï¼Œä½œè€…å…ˆå°†ä½åˆ†è¾¨ç‡å›¾ç•¥å¾®é€€åŒ–åå†ç»™åˆ°è¶…åˆ†æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œä½œè€…å¯¹ç¬¬ä¸€ä¸ªä¸Šé‡‡æ ·é˜¶æ®µä½¿ç”¨é«˜æ–¯æ¨¡ç³Šï¼Œå¯¹ç¬¬äºŒä¸ªä¸Šé‡‡æ ·é˜¶æ®µä½¿ç”¨æ›´å¤šæ ·çš„ BSR é€€åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸¤ä¸ªè¶…åˆ†æ¨¡å‹åªç”¨äº†çº¯å·ç§¯è€Œæ²¡ç”¨ attention layersï¼Œæ‰€ä»¥è®­ç»ƒæ—¶å¯ä»¥åªå¯¹ 1/4 å¤§å°çš„ random crops è®­ç»ƒæ¥å‡å°è®¡ç®—é‡å¹¶ä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼Œæ¨æ–­æ—¶æ”¹ç”¨å®Œæ•´å¤§å°ã€‚

æ„ä¹‰ï¼Ÿï¼Ÿ

ç”±äº decoder å¯ä»¥çœ‹ä½œæ˜¯ä» image embedding å¾—åˆ°å›¾åƒï¼Œå’Œ CLIP ä»å›¾åƒå¾—åˆ° image embedding æ­£å¥½æ˜¯ç›¸åçš„è¿‡ç¨‹ï¼Œæ‰€ä»¥ä½œè€…å°†æ•´ä¸ªæ–‡ç”Ÿå›¾æ¨¡å‹å‘½åä¸º unCLIP.


ä¸å¤ªæ•¢ç¡®å®šè¿™ä¸ªçš„è®­ç»ƒæ–¹å¼ã€‚æŠŠåŸå›¾è¾“å…¥ï¼Ÿï¼Ÿï¼Ÿ      


Prior

ç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼ˆtext captionï¼‰åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ pretrained CLIP å¾—åˆ° text embeddingï¼Œä½†æ˜¯ç”±äº decoder çš„è¾“å…¥æ˜¯ image embeddingï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è®­ç»ƒä¸€ä¸ª prior æ¨¡å‹ä» text embedding é¢„æµ‹ image embedding.
ä½œè€…å°è¯•äº†ä¸¤ç§æ–¹æ¡ˆï¼š

    è‡ªå›å½’æ¨¡å‹ï¼šå°† image embedding è½¬æ¢ä¸ºä¸€åˆ—ç¦»æ•£ç¼–ç ï¼Œç„¶åç”¨è‡ªå›å½’çš„æ–¹å¼é€ä¸ªé¢„æµ‹ï¼›
    æ‰©æ•£æ¨¡å‹ï¼šä»¥ text embedding ä¸ºæ¡ä»¶ï¼Œç”¨æ‰©æ•£æ¨¡å‹å¯¹ image embedding å»ºæ¨¡ã€‚


å®éªŒå‘ç°ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ•ˆæœæ›´ä½³ã€‚å…³äºå¦‚ä½•èå…¥æ¡ä»¶çš„å…·ä½“ç»†èŠ‚æœ‰äº¿äº›ç¹çï¼Œæ„Ÿå…´è¶£çš„è¯»è€…ç›´æ¥çœ‹è®ºæ–‡å§ã€‚

ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜æ˜¯ï¼Œprior æ˜¯å¦æ˜¯å¿…è¦çš„ï¼Ÿæˆ‘ä»¬ä¸ºä»€ä¹ˆä¸ç›´æ¥æŠŠ CLIP text embeddingã€ç”šè‡³æ˜¯ text caption ç»™åˆ° decoder åšç”Ÿæˆå‘¢ï¼Ÿå…¶å®è¿™æ ·åšä¹Ÿæ²¡æ¯›ç—…ï¼Œä¸è¿‡ä½œè€…åšäº†ä¸€ä¸ª ablation studyï¼Œå‘ç°ç”¨ prior æ¥é¢„æµ‹ image embedding æ•ˆæœæ›´å¥½ã€‚


å¥½åƒæœ‰åäººè¯„ä»·è¿‡ä¼šç¡®å®ä¸€äº›å¤æ‚é€»è¾‘è¯­ä¹‰     

![alt text](assets/IC-Light/image-32.png)

æ“ä½œmanipulations 

![alt text](assets/IC-Light/image-33.png)

æœ€åï¼Œä½œè€…ä¹Ÿå‘ç° DALLÂ·E 2 çš„ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚åœ¨ç‰©ä½“-å±æ€§çš„å¯¹åº”å…³ç³»ä¸Šå¾€å¾€ä¸å¦‚ GLIDE. ä¾‹å¦‚ï¼Œè¾“å…¥æ–‡æœ¬ä¸ºâ€œä¸€ä¸ªçº¢è‰²æ–¹å—åœ¨ä¸€ä¸ªè“è‰²æ–¹å—ä¹‹ä¸Šâ€ï¼ŒDALLÂ·E 2 ç”Ÿæˆçš„ç»“æœä¸æ˜¯æŠŠä½ç½®æé”™ï¼Œå°±æ˜¯æŠŠé¢œè‰²æé”™ï¼Œä½† GLIDE å°±é è°±å¾ˆå¤šã€‚ä½œè€…æ¨æµ‹è¿™ä¸ CLIP embedding æœ¬èº«æ²¡æœ‰æ˜¾å¼åœ°ç»‘å®šç‰©ä½“ä¸å±æ€§æœ‰å…³ã€‚


## Stable unCLIP 2.1
2023.3

åˆ†è¾¨ç‡ä¸º768x768ï¼ŒåŸºäºSD2.1-768ã€‚

è¿™ä¸ªæ¨¡å‹å…è®¸å›¾åƒå˜åŒ–ï¼Œä»¥åŠæ··åˆæ“ä½œã€‚

ç”±äºå…¶æ¨¡å—åŒ–ï¼Œå¯ä»¥ä¸å…¶ä»–æ¨¡å‹ï¼Œå¦‚KARLOï¼Œè¿›è¡Œç»“åˆã€‚è¿™é‡Œé¢æœ‰ä¸¤ä¸ªå˜ä½“ï¼šStable unCLIP-Lå’ŒStable unCLIP-Hï¼Œå®ƒä»¬åˆ†åˆ«ä»¥CLIP ViT-Lå’ŒViT-Hå›¾åƒåµŒå…¥ä¸ºæ¡ä»¶ã€‚








## StableStudio
è‡ªä»Stable Diffusionå‘å¸ƒä»¥æ¥ï¼ŒDreamStudioå°±æ˜¯StabilityAIæœ€ä¸»è¦çš„åº”ç”¨ï¼Œç”¨æ¥å±•ç¤ºæœ€æ–°çš„æ¨¡å‹å’ŒåŠŸèƒ½ã€‚     
è¿½æº¯èµ·DreamStudioçš„èµ·æºï¼Œå®ƒæœ€åˆæ˜¯Disco Diffusionä¸‹çš„ä¸€ä¸ªåŠ¨ç”»å·¥ä½œå®¤ã€‚éšç€å»å¹´å¤å¤©Stable Diffusionçš„å‘å¸ƒï¼ŒDisco Diffusionçš„é‡ç‚¹ä¹Ÿä»åŠ¨ç”»è½¬åˆ°äº†å›¾åƒç”Ÿæˆã€‚    












## Imagen
https://www.assemblyai.com/blog/how-imagen-actually-works/    

2022.5

çœ‹åˆ° OpenAI åˆæ˜¯ GLIDE åˆæ˜¯ DALLÂ·E 2 çš„ï¼ŒGoogle è¿™è¾¹ç»ˆäºåä¸ä½äº†ï¼Œæ¨å‡ºäº†æ›´å¼ºçš„æ–‡æœ¬ç”Ÿæˆå›¾åƒå¤§æ¨¡å‹â€”â€”Imagen    


ç°åœ¨å·²ç»æœ‰image fxå’Œ veo

ç›¸æ¯” DALLÂ·E 2ï¼ŒImagen çš„æ•´ä½“æ€è·¯æ›´ç®€å•ä¸€äº›ï¼šå…ˆç”¨ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸º text embeddingï¼Œç„¶åä»¥æ­¤ä¸ºæ¡ä»¶å¹¶åˆ©ç”¨ classifier-free guidance æŒ‡å¯¼ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ç”Ÿæˆ 64x64 å¤§å°çš„å›¾åƒï¼Œéšåç”¨ä¸¤ä¸ªä¸Šé‡‡æ ·æ‰©æ•£æ¨¡å‹ï¼ˆä¹ŸåŠ å…¥äº†æ–‡æœ¬æ¡ä»¶å¹¶ä½¿ç”¨äº† classifier-free guidanceï¼‰å°†å›¾åƒä¸Šé‡‡æ ·è‡³ 256x256 å’Œ 1024x1024ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š


![alt text](assets/IC-Light/v2-12063fe936c47c103e479c7687e6e3be_720w.webp)


Pretrained text encoders

ä¸ GLIDE ä¸åŒçš„æ˜¯ï¼ŒImagen é‡‡ç”¨é¢„è®­ç»ƒå¥½ä¸”å›ºå®šä¸åŠ¨çš„æ–‡æœ¬ç¼–ç å™¨è€Œéä»å¤´è®­ç»ƒã€‚å¸¸è§çš„ LLMï¼ŒåŒ…æ‹¬ BERTã€CLIPã€T5 éƒ½æ˜¯å¯è¡Œçš„é€‰æ‹©ï¼Œä½œè€…å‘ç° T5 æ•ˆæœæœ€ä½³ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä½œè€…å‘ç°æ‰©å¤§ text encoder çš„è§„æ¨¡æ¯”æ‰©å¤§ image diffusion model çš„è§„æ¨¡æ˜¾è‘—æ›´æœ‰æ•ˆã€‚

![alt text](assets/IC-Light/image-34.png)

![alt text](assets/IC-Light/v2-d747c24036762d93021acf4e6e76c9ed_r.png)


Cascaded diffusion models

åŒ SR3ã€CDMã€DALLÂ·E 2 ç­‰ä¸€æ ·ï¼ŒImagen ä½œè€…ä¹Ÿå‘ç°å¯¹è¶…åˆ†æ¨¡å‹è€Œè¨€ï¼Œå°†ä½åˆ†è¾¨ç‡å›¾åƒåšä¸€å®šçš„å¢å¼ºï¼ˆé«˜æ–¯å™ªå£°ï¼‰åä½œä¸ºè¶…åˆ†æ¨¡å‹çš„æ¡ä»¶ï¼Œèƒ½è®©æ¨¡å‹æ›´é²æ£’ã€‚


Network architecture

å¯¹äºç¬¬ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œä½œè€…é™¤äº†å°† text embedding ä¸ time embedding ç›¸åŠ æ¥èå…¥æ¡ä»¶ï¼Œè¿˜é‡‡ç”¨äº† cross attention.

å¯¹äºä¸¤ä¸ªä¸Šé‡‡æ ·æ‰©æ•£æ¨¡å‹ï¼Œä½œè€…æå‡ºäº†æ›´ç®€å•ã€æ”¶æ•›æ›´å¿«ã€æ›´ memory efficient çš„ Efficient U-Net. ç›¸æ¯”å¸¸ç”¨çš„ U-Netï¼ŒEfficient U-Net åšäº†å¦‚ä¸‹æ”¹å˜ï¼š


![alt text](assets/IC-Light/image-35.png)

ä¸ DALLÂ·E 2 ç±»ä¼¼ï¼Œç¬¬äºŒä¸ªè¶…åˆ†æ¨¡å‹æ˜¯åœ¨ image crops ä¸Šè®­ç»ƒçš„ï¼Œå› æ­¤æ²¡æœ‰ä½¿ç”¨ self-attention layersï¼Œä½†æ˜¯ä¿ç•™äº† text cross-attention layers. æ›´å¤šç»†èŠ‚å¯ä»¥åœ¨è®ºæ–‡çš„ Appendix F ä¸­æ‰¾åˆ°ã€‚

ä»¥ä¸Šå°±æ˜¯ Imagen çš„åŸºæœ¬å†…å®¹ï¼Œæ›´å¤šç»†èŠ‚è¯·å‚é˜…åŸè®ºæ–‡ã€‚äº‹å®ä¸Šï¼Œä½œè€…è¿˜åœ¨è®ºæ–‡ä¸­è¿˜æå‡ºäº† DrawBench è¯„æµ‹æŒ‡æ ‡ï¼Œè¿™é‡ŒæŒ‰ä¸‹ä¸è¡¨ã€‚



## eDiff-I
åœ¨ OpenAI å’Œ Google æ‰“å¾—æœ‰æ¥æœ‰å›ä¹‹é™…ï¼ŒNVIDIA ç»ˆäºä¹Ÿå‚ä¸äº†è¿›æ¥ï¼Œæ¨å‡ºäº† eDiff-I. 

é€šè¿‡åˆ†ææ–‡ç”Ÿå›¾æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼Œä½œè€…å‘ç°ï¼šåœ¨å»å™ªå‰æœŸï¼Œæ¨¡å‹éå¸¸ä¾èµ–äºæ–‡æœ¬æ¡ä»¶æ¥ç”Ÿæˆç¬¦åˆæè¿°çš„å›¾åƒï¼›è€Œåœ¨å»å™ªåæœŸï¼Œæ¨¡å‹ä¼šå‡ ä¹å¿½ç•¥æ–‡æœ¬ï¼Œå…³æ³¨äºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚

tgateä»¥åŠç¤¾åŒºçš„æŠ€å·§    

å› æ­¤ï¼Œç°æœ‰çš„æ–¹æ³•åœ¨ä¸åŒå»å™ªé˜¶æ®µéƒ½ä½¿ç”¨åŒä¸€ä¸ª UNet æ¨¡å‹ä¹Ÿè®¸å¹¶ä¸å¥½ï¼ˆå°½ç®¡ time embedding æŒ‡ç¤ºäº†å»å™ªæ—¶é—´æ­¥ï¼‰ã€‚äºæ˜¯ï¼ŒeDiff-I å¯¹ä¸åŒå»å™ªé˜¶æ®µé‡‡ç”¨äº†å¤šä¸ªä¸“å®¶å»å™ªæ¨¡å‹ã€‚

è‹±ä¼Ÿè¾¾è¿™ä¸¤å¹´å¥½åƒéƒ½åœ¨å…³æ³¨æ•ˆç‡çš„äº‹æƒ…ï¼Œåœ¨åŠ é€Ÿä¸Šåšå·¥ä½œï¼Œåˆ†æå»å™ªåŠ å™ªè¿‡ç¨‹ï¼Œæ²¡æœ‰ç‰¹åˆ«é«˜è´¨é‡ç”Ÿæˆçš„æ¨¡å‹     
å¦‚aysï¼Œtensorrt     
åŒ…æ‹¬å¾®è½¯ï¼Œéƒ½åœ¨ç ”ç©¶ä¸€äº›åŸºå»ºæ¶æ„åŠ é€Ÿï¼Œcnnå±‚é—´é«˜æ•ˆé€šä¿¡ä»€ä¹ˆçš„ï¼Œç»“æœæ˜¯æ²¡æœ‰ä»€ä¹ˆå¤§çªç ´

MoE

ä¸ºäº†è®­ç»ƒæ•ˆç‡çš„è€ƒè™‘ï¼Œä½œè€…å…ˆåªè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åé€æ¸å°†å…¶åˆ†è§£ä¸ºå„ä¸ªé˜¶æ®µçš„ä¸“å®¶æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æ¨¡å‹å¤šäº†ï¼Œä½†æ¨ç†æ—¶é—´è¿˜æ˜¯ä¸å˜çš„ã€‚   
åŒ…æ‹¬ç°åœ¨çš„åˆ†é˜¶æ®µloraä¹Ÿæ˜¯è¿™ç§æ€æƒ³     
é‚£ä¹ˆè®­ç»ƒæ˜¯ä¸æ˜¯éº»çƒ¦ï¼ŒåŠ è½½æ¨¡å‹å¤šï¼Œè½¬æ¢åˆéœ€è¦æ—¶é—´    

å¦å¤–ï¼Œä½œè€…è¿˜ç ”ç©¶äº†ä¸åŒæ¡ä»¶çš„ä½œç”¨ï¼ŒåŒ…æ‹¬ T5 text embeddingã€CLIP text embedding å’Œ CLIP image embedding. å…¶ä¸­ï¼ŒCLIP image embedding å¯ä»¥ç”¨æ¥è¿ç§»å‚è€ƒå›¾åƒçš„é£æ ¼ã€‚æœ€åï¼Œä½œè€…è¿˜å±•ç¤ºäº† â€œpaint-with-wordsâ€ åŠŸèƒ½ï¼Œå³åœ¨ç”»å¸ƒä¸Šæ ‡æ³¨åŒºåŸŸå’Œæ–‡å­—ï¼Œé‚£ä¹ˆæ¨¡å‹èƒ½åœ¨æŒ‡å®šåŒºåŸŸä¸Šä¾ç…§å¯¹åº”æ–‡å­—ä½œå›¾ã€‚

![alt text](assets/IC-Light/v2-13a9b00bb62a5ece71c3dd14f1526055_720w.png)

ç°åœ¨è€å–œæ¬¢ç”»è¿™ä¸ªå™ªå£°æµï¼Œè‡³ä»Šæ²¡çœ‹æ‡‚    
æ„æ€æ˜¯æ­£å¸¸é«˜æ–¯å™ªå£°é‡‡æ ·å‡ºå„ç§é«˜æ–¯ç»„åˆå—ï¼Ÿï¼Ÿï¼Ÿ    

å¦‚å›¾æ‰€ç¤ºï¼ŒeDiff-I ç”±ä¸€ä¸ªåŸºç¡€æ¨¡å‹å’Œä¸¤ä¸ªè¶…åˆ†æ¨¡å‹æ„æˆï¼Œè¿™ä¸€ç‚¹ä¸ Imagen å®Œå…¨ä¸€è‡´ã€‚æ¯ä¸ªåˆ†è¾¨ç‡ä¸‹çš„æ¨¡å‹éƒ½ç”±å¤šä¸ªä¸“å®¶æ¨¡å‹ç»„æˆã€‚

å¤šä¸ªtext_encoderçš„æ–¹æ³•ä¹Ÿæ—©æœŸå°±æµè¡Œèµ·æ¥äº†å•Šã€‚åº”è¯¥éƒ½æœ‰è®ºæ–‡åˆ†æçš„ã€‚     

è¿™ç§è¿˜æ˜¯å–å†³äºå…¬å¸æ°´å¹³ï¼Œæ‰èƒ½æ”¯æ’‘å‘˜å·¥å»å…³æ³¨ä»€ä¹ˆã€‚

![alt text](assets/IC-Light/image-36.png)

ç”±äºæ–‡æœ¬å¹¶ä¸å¥½æè¿°ç‰©ä½“çš„ä½ç½®ï¼Œä½œè€…æå‡ºäº† â€œpaint-with-wordsâ€ æŠ€æœ¯ï¼Œé€šè¿‡æŒ‡å®šåŒºåŸŸå’Œå¯¹åº”æ–‡æœ¬æ¥æ§åˆ¶ä½ç½®ã€‚è¿™ä¸ªæ–¹æ³•å¹¶ä¸éœ€è¦è®­ç»ƒï¼Œä¸»è¦æ€è·¯æ˜¯ä¿®æ”¹ attention mapï¼Œè¿™å…¶å®ä¸å¾ˆå¤šå›¾åƒç¼–è¾‘å·¥ä½œï¼ˆå¦‚ Prompt-to-Promptï¼‰çš„åšæ³•ç›¸åŒï¼Œå…·ä½“å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![alt text](assets/IC-Light/v2-24486958ef9522e65f0bed23cd375d5b_720w.webp)


## DeepFloyd IF
DeepFloyd IF æ˜¯ DeepFloyd Lab å’Œ StabilityAI å¼€æºçš„æ–‡ç”Ÿå›¾å¤§æ¨¡å‹ï¼Œæ•´ä½“æ²¿ç”¨ Imagen çš„æŠ€æœ¯è·¯çº¿ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ Imagen çš„å¼€æºç‰ˆæœ¬ã€‚å› æ­¤æœ¬èº«æ²¡æœ‰ä»€ä¹ˆå€¼å¾—å¤šè¯´çš„ã€‚

![alt text](assets/IC-Light/v2-a6c38191e913db87c47f85c1a8009e2c_720w.webp)

DeepFloyd IFæ˜¯ä¸€æ¬¾åƒç´ çº§AIæ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹è§£å†³äº†å‡†ç¡®ç»˜åˆ¶æ–‡å­—ã€å‡†ç¡®ç†è§£ç©ºé—´å…³ç³»ç­‰AIæ–‡ç”Ÿå›¾éš¾é¢˜ï¼Œæ”¯æŒéå•†ä¸šã€ç ”ç©¶ç”¨é€”ã€‚

å¯ä»¥çœ‹åˆ°ç¡®å®ä¸ Imagen æ˜¯å·®ä¸å¤šçš„ï¼Œä¸è¿‡ DeepFloyd IF åœ¨æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ä¸åŒå¤§å°çš„æ¨¡å‹å¯ä»¥é€‰æ‹©ã€‚

ç¼–è¾‘äº 2023-11-09 20:54


### æ¨¡å‹list

å›¾ç”Ÿå›¾ä¸å›¾åƒæ¢å¤
xyfJASONï¼šæ‰©æ•£æ¨¡å‹åº”ç”¨Â·å›¾ç”Ÿå›¾ä¸å›¾åƒæ¢å¤45 èµåŒ Â· 1 è¯„è®ºæ–‡ç« 

    SR3
    SDEdit
    ILVR
    Palette
    RePaint
    DDRM
    DDIB
    DDNM

æ–‡ç”Ÿå›¾å¤§æ¨¡å‹
xyfJASONï¼šæ‰©æ•£æ¨¡å‹åº”ç”¨Â·æ–‡ç”Ÿå›¾å¤§æ¨¡å‹5 èµåŒ Â· 0 è¯„è®ºæ–‡ç« 

    GLIDE
    DALLÂ·E 2 (unCLIP)
    Imagen
    Stable Diffusion
    eDiff-I
    RAPHAEL
    DeepFloyd IF
    DALLÂ·E 3

åŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘
xyfJASONï¼šæ‰©æ•£æ¨¡å‹åº”ç”¨Â·åŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘23 èµåŒ Â· 3 è¯„è®ºæ–‡ç« 

    DiffusionCLIP
    Blended Diffusion
    SDG (Semantic Diffusion Guidance)
    Prompt-to-Prompt
    DiffuseIT
    Imagic
    DiffEdit
    Null-text Inversion
    InstructPix2Pix
    Pix2pix-zero

å¯»æ‰¾è¯­ä¹‰ç©ºé—´

    Diffusion Autoencoders
    Asyrp

ä¸ªæ€§åŒ–ç”Ÿæˆ
xyfJASONï¼šæ‰©æ•£æ¨¡å‹åº”ç”¨Â·ä¸ªæ€§åŒ–ç”Ÿæˆ8 èµåŒ Â· 0 è¯„è®ºæ–‡ç« 

    Textual Inversion
    DreamBooth
    DreamBooth + LoRA
    Custom Diffusion
    SuTI
    HyperDreamBooth

å¯æ§ç”Ÿæˆ

    ControlNet
    T2I-Adapter
    Mixture of Diffusers
    MultiDiffusion
    Composer




## Daemonçº¿ç¨‹ JVM

ä»€ä¹ˆæ˜¯Daemonçº¿ç¨‹

Daemonçº¿ç¨‹ä¹Ÿæ˜¯å®ˆæŠ¤çº¿ç¨‹ï¼Œå®ƒæ˜¯ä¸€ç§æ”¯æŒå‹çš„çº¿ç¨‹ï¼Œä¸»è¦ç”¨åœ¨ç¨‹åºçš„åå°è°ƒåº¦ä»¥åŠä¸€äº›æ”¯æŒæ€§ï¼ˆæœåŠ¡æ€§ï¼‰çš„å·¥ä½œï¼Œå¸¸è§çš„ä¾‹å­ï¼šJVMä¸­åƒåœ¾å›æ”¶çº¿ç¨‹å°±æ˜¯å…¸å‹çš„å®ˆæŠ¤çº¿ç¨‹

äºŒã€å®ˆæŠ¤çº¿ç¨‹å’Œç”¨æˆ·çº¿ç¨‹çš„åŒºåˆ«

å®ˆæŠ¤çº¿ç¨‹ä¸ç”¨æˆ·çº¿ç¨‹çš„åŒºåˆ«å‘ç”Ÿåœ¨JVMçš„ç¦»å¼€ï¼š

    å¯ä»¥è¯´JVMæƒ³è¦è¿è¡Œï¼Œç”¨æˆ·çº¿ç¨‹ä¹Ÿå¿…é¡»è¿è¡Œ
    å®ˆæŠ¤çº¿ç¨‹æ˜¯æœåŠ¡äºç”¨æˆ·çº¿ç¨‹çš„ï¼Œå¦‚æœç”¨æˆ·çº¿ç¨‹ä¸åœ¨äº†ï¼Œé‚£ä¹ˆå®ˆæŠ¤çº¿ç¨‹çš„å­˜åœ¨æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œæ­¤æ—¶è¯¥ç¨‹åºï¼ˆè¿›ç¨‹ï¼‰å°±æ²¡æœ‰è¿è¡Œçš„å¿…è¦äº†ï¼ŒJVMä¹Ÿå°±é€€å‡ºäº†
    å®ˆæŠ¤çº¿ç¨‹çš„ä¼˜å…ˆçº§æ˜¯ä½äºç”¨æˆ·çº¿ç¨‹çš„


JVMæ˜¯Java Virtual Machine(Javaè™šæ‹Ÿæœº)çš„ç¼©å†™ï¼ŒJVMæ˜¯ä¸€ç§ç”¨äºè®¡ç®—è®¾å¤‡çš„è§„èŒƒï¼Œå®ƒæ˜¯ä¸€ä¸ªè™šæ„å‡ºæ¥çš„è®¡ç®—æœºï¼Œæ˜¯é€šè¿‡åœ¨å®é™…çš„è®¡ç®—æœºä¸Šä»¿çœŸæ¨¡æ‹Ÿå„ç§è®¡ç®—æœºåŠŸèƒ½æ¥å®ç°çš„ã€‚

Javaè¯­è¨€çš„ä¸€ä¸ªéå¸¸é‡è¦çš„ç‰¹ç‚¹å°±æ˜¯ä¸å¹³å°çš„æ— å…³æ€§ã€‚è€Œä½¿ç”¨Javaè™šæ‹Ÿæœºæ˜¯å®ç°è¿™ä¸€ç‰¹ç‚¹çš„å…³é”®ã€‚ä¸€èˆ¬çš„é«˜çº§è¯­è¨€å¦‚æœè¦åœ¨ä¸åŒçš„å¹³å°ä¸Šè¿è¡Œï¼Œè‡³å°‘éœ€è¦ç¼–è¯‘æˆä¸åŒçš„ç›®æ ‡ä»£ç ã€‚è€Œå¼•å…¥Javaè¯­è¨€è™šæ‹Ÿæœºåï¼ŒJavaè¯­è¨€åœ¨ä¸åŒå¹³å°ä¸Šè¿è¡Œæ—¶ä¸éœ€è¦é‡æ–°ç¼–è¯‘ã€‚Javaè¯­è¨€ä½¿ç”¨Javaè™šæ‹Ÿæœºå±è”½äº†ä¸å…·ä½“å¹³å°ç›¸å…³çš„ä¿¡æ¯ï¼Œä½¿å¾—Javaè¯­è¨€ç¼–è¯‘ç¨‹åºåªéœ€ç”Ÿæˆåœ¨Javaè™šæ‹Ÿæœºä¸Šè¿è¡Œçš„ç›®æ ‡ä»£ç (å­—èŠ‚ç )ï¼Œå°±å¯ä»¥åœ¨å¤šç§å¹³å°ä¸Šä¸åŠ ä¿®æ”¹åœ°è¿è¡Œã€‚Javaè™šæ‹Ÿæœºåœ¨æ‰§è¡Œå­—èŠ‚ç æ—¶ï¼ŒæŠŠå­—èŠ‚ç è§£é‡Šæˆå…·ä½“å¹³å°ä¸Šçš„æœºå™¨æŒ‡ä»¤æ‰§è¡Œã€‚è¿™å°±æ˜¯Javaçš„èƒ½å¤Ÿ"ä¸€æ¬¡ç¼–è¯‘ï¼Œåˆ°å¤„è¿è¡Œ"çš„åŸå› ã€‚






# ç»“å°¾
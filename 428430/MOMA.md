MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation     
MoMAï¼šç”¨äºå¿«é€Ÿç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒçš„å¤šæ¨¡æ€ LLM é€‚é…å™¨    

å³æ’å³ç”¨é€‚é…å™¨    
æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸€ä¸ªé€šç”¨é€‚é…å™¨ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒé˜¶æ®µå†»ç»“äº†åŸå§‹æ‰©æ•£æ¨¡å‹ã€‚å®ƒå¯ä»¥æ¨å¹¿åˆ°ä»åŒä¸€åŸºæœ¬æ¨¡å‹å¾®è°ƒçš„è‡ªå®šä¹‰æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬åœ¨ HuggingFace å’Œ CivitAi çš„ç¤¾åŒºæ¨¡å‹ä¸ŠéªŒè¯äº†è¿™ä¸€ç‚¹ï¼ŒåŒ…æ‹¬ Realistic Vision V4.0ã€ReV-Animatedã€Anything v4 å’Œ Esthetic Retro Animeã€‚è¿™äº›æ¨¡å‹éƒ½æ˜¯ä» `SD v1.5` å¼€å§‹è¿›è¡Œå¾®è°ƒçš„ã€‚ MoMAå¯ä»¥ç›´æ¥åº”ç”¨äºè¿™äº›ç¤¾åŒºæ¨¡å‹ï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹ã€‚      

å’Œipadapterå¯¹æ¯”     
å’Œç›´æ¥ä¿®æ”¹èƒŒæ™¯å¯¹æ¯”ï¼Ÿï¼Ÿ    




# è®ºæ–‡ä¿¡æ¯
å­—èŠ‚   
ç½—æ ¼æ–¯å¤§å­¦(ç¾å›½å…¬ç«‹å¤§å­¦ç³»ç»Ÿ)    






[Submitted on 8 Apr 2024]   
MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation     
https://arxiv.org/abs/2404.05674    

é¡¹ç›®åœ°å€    
https://github.com/bytedance/MoMA     
https://moma-adapter.github.io/    

https://huggingface.co/KunpengSong/MoMA_llava_7b

å‘å¸ƒ    
[2024/04/20] ğŸ”¥ æˆ‘ä»¬åœ¨ GitHub ä¸Šå‘å¸ƒäº†æ¨¡å‹ä»£ç ã€‚   
[2024/04/22] ğŸ”¥ æˆ‘ä»¬æ·»åŠ  HuggingFace å­˜å‚¨åº“å¹¶å‘å¸ƒæ£€æŸ¥ç‚¹ã€‚   






æ¨¡å‹ç±»å‹ï¼š MoMA æ˜¯ä¸€ä¸ªå¼€æºå›¾åƒä¸ªæ€§åŒ–æ¨¡å‹ã€‚å®ƒå…·æœ‰æ–°çš„æ³¨æ„åŠ›å±‚å’Œä» LLaVA-7B å¾®è°ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚

![alt text](assets/MOMA/image-7.png)

æ²¡æœ‰issue     
ç»“æœåº”è¯¥ä¸å¥½    




# åŸç†

`SD v1.5`æ¨¡å‹   

æˆ‘ä»¬æ¨å‡º MoMAï¼šä¸€ç§å¼€æ”¾è¯æ±‡ã€å…è®­ç»ƒçš„ä¸ªæ€§åŒ–å›¾åƒæ¨¡å‹ï¼Œæ‹¥æœ‰çµæ´»çš„é›¶æ ·æœ¬åŠŸèƒ½ã€‚éšç€åŸºç¡€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹å¼ºå¤§çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ä¸ºäº†æ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œ`MoMA ä¸“é—¨ç ”ç©¶ä¸»é¢˜é©±åŠ¨çš„ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆ`ã€‚åˆ©ç”¨å¼€æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œæˆ‘ä»¬`è®­ç»ƒ MoMA å……å½“ç‰¹å¾æå–å™¨å’Œç”Ÿæˆå™¨`çš„åŒé‡è§’è‰²ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°ååŒå‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„å›¾åƒç‰¹å¾ï¼Œä¿ƒè¿›å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç”Ÿæˆçš„ç‰¹å¾ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥`å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è‡ªæ³¨æ„åŠ›æ·å¾„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†å›¾åƒç‰¹å¾è½¬ç§»åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œæé«˜ç”Ÿæˆå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„ç›¸ä¼¼åº¦`ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½œä¸ºä¸€ä¸ªå…è°ƒæ•´çš„`å³æ’å³ç”¨`æ¨¡å—ï¼Œæˆ‘ä»¬çš„æ¨¡å‹`ä»…éœ€è¦å•ä¸ªå‚è€ƒå›¾åƒ`ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆå…·æœ‰é«˜ç»†èŠ‚ä¿çœŸåº¦ã€å¢å¼ºçš„èº«ä»½ä¿ç•™å’Œå³æ—¶å¿ å®åº¦çš„å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è‡´åŠ›äºå°†æˆ‘ä»¬çš„å·¥ä½œå¼€æºï¼Œä»è€Œè®©æ‰€æœ‰äººéƒ½èƒ½è·å¾—è¿™äº›è¿›æ­¥ã€‚

![alt text](assets/MOMA/image.png)    
æˆ‘ä»¬æ¨å‡ºäº† MoMAï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç»†ç²’åº¦ç‰¹å¾ä¼ è¾“å¢å¼ºçš„å¤šæ¨¡å¼ LLM é€‚é…å™¨ã€‚æ•´ä½“æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šï¼ˆ1ï¼‰åˆ©ç”¨ç”Ÿæˆå¤šæ¨¡æ€è§£ç å™¨`ä»å‚è€ƒå›¾åƒä¸­æå–å›¾åƒç‰¹å¾ï¼Œå¹¶æ ¹æ®ç›®æ ‡æç¤ºå¯¹å…¶è¿›è¡Œç¼–è¾‘ï¼Œäº§ç”Ÿä¸Šä¸‹æ–‡åŒ–å›¾åƒç‰¹å¾`ï¼› ï¼ˆ2ï¼‰æˆ‘ä»¬å°†åŸå§‹å›¾åƒçš„èƒŒæ™¯æ›¿æ¢ä¸ºç™½è‰²ï¼Œ`åªç•™ä¸‹ç›®æ ‡åƒç´ ï¼Œå¹¶åˆ©ç”¨åŸå§‹UNetçš„è‡ªæ³¨æ„åŠ›å±‚æ¥æå–ç›®æ ‡å›¾åƒç‰¹å¾`ï¼› (3)æœ€åï¼Œåœ¨æ–°å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åˆ†åˆ«`ä½¿ç”¨ä¸“é—¨è®­ç»ƒçš„ä¸Šä¸‹æ–‡äº¤å‰æ³¨æ„å±‚å’Œå¯¹è±¡äº¤å‰æ³¨æ„å±‚å°†ä¸Šä¸‹æ–‡å›¾åƒç‰¹å¾å’Œå¯¹è±¡å›¾åƒç‰¹å¾æ³¨å…¥`åˆ°UNetæ‰©æ•£æ¨¡å‹ä¸­ã€‚    

ä¸ºäº†å®ç°æœ€ä½³æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ç”Ÿæˆå­¦ä¹ é˜¶æ®µï¼Œæˆ‘ä»¬å¯¹å¤šæ¨¡æ€å›¾åƒç‰¹å¾è§£ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶å­¦ä¹ æ ¹æ®ç›®æ ‡æç¤ºç»„åˆä¸»é¢˜çš„å›¾åƒç‰¹å¾ï¼Œå¹¶è¾“å‡ºç›®æ ‡å›¾åƒçš„ CLIP åµŒå…¥ã€‚å…¶æ¬¡ï¼Œè®­ç»ƒä¸»é¢˜å’Œä¸Šä¸‹æ–‡äº¤å‰æ³¨æ„å±‚æ¥æ³¨å…¥è¿™ç§åµŒå…¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºç»†èŠ‚çš„çœŸå®åº¦ï¼Œæˆ‘ä»¬æ¶‰åŠå›¾åƒè‡ªæ³¨æ„åŠ›ç‰¹å¾è½¬ç§»å¹¶åº”ç”¨æ©è”½æœºåˆ¶

![alt text](assets/MOMA/image-4.png)


ä¸å…¶ä»–æ–¹æ³•æ¯”è¾ƒ    
é›¶æ ·æœ¬å®šæ€§æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨ä¸Šé¢æ¿ä¸­å…±äº«é‡æ–°ä¸Šä¸‹æ–‡åŒ–ï¼Œåœ¨ä¸‹é¢æ¿ä¸­å…±äº«çº¹ç†ç¼–è¾‘ã€‚æˆ‘ä»¬çš„ç»“æœä¸ºä¸Šä¸‹æ–‡ç¼–è¾‘æä¾›äº†æ˜æ˜¾æ›´å‡†ç¡®çš„ç»†èŠ‚ï¼Œå¹¶åœ¨çº¹ç†ç¼–è¾‘ä¸­çš„æç¤ºå’Œå›¾åƒä¿çœŸåº¦ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚    

![alt text](assets/MOMA/image-5.png)

å³æ’å³ç”¨é€‚é…å™¨    
æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸€ä¸ªé€šç”¨é€‚é…å™¨ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒé˜¶æ®µå†»ç»“äº†åŸå§‹æ‰©æ•£æ¨¡å‹ã€‚å®ƒå¯ä»¥æ¨å¹¿åˆ°ä»åŒä¸€åŸºæœ¬æ¨¡å‹å¾®è°ƒçš„è‡ªå®šä¹‰æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬åœ¨ HuggingFace å’Œ CivitAi çš„ç¤¾åŒºæ¨¡å‹ä¸ŠéªŒè¯äº†è¿™ä¸€ç‚¹ï¼ŒåŒ…æ‹¬ Realistic Vision V4.0ã€ReV-Animatedã€Anything v4 å’Œ Esthetic Retro Animeã€‚è¿™äº›æ¨¡å‹éƒ½æ˜¯ä» `SD v1.5` å¼€å§‹è¿›è¡Œå¾®è°ƒçš„ã€‚ MoMAå¯ä»¥ç›´æ¥åº”ç”¨äºè¿™äº›ç¤¾åŒºæ¨¡å‹ï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹ã€‚      


![alt text](assets/MOMA/image-6.png)    



# ä½¿ç”¨
![alt text](assets/MOMA/image-1.png)


æ–°ä¸Šä¸‹æ–‡ï¼š    
![alt text](assets/MOMA/image-2.png)

æ–°çº¹ç†ï¼š   
![alt text](assets/MOMA/image-3.png)    



è¶…å‚æ•°ï¼š

åœ¨â€œæ›´æ”¹ä¸Šä¸‹æ–‡â€ä¸­ï¼Œæ‚¨å¯ä»¥å¢åŠ strengthä»¥è·å¾—æ›´å‡†ç¡®çš„è¯¦ç»†ä¿¡æ¯ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œstrength=1.0æ˜¯æœ€å¥½çš„ã€‚å»ºè®®strengthä¸å¤§äº1.2ã€‚

åœ¨â€œæ›´æ”¹çº¹ç†â€ä¸­ï¼Œæ‚¨å¯ä»¥æ›´æ”¹strengthç»†èŠ‚ç²¾åº¦å’Œæç¤ºä¿çœŸåº¦ä¹‹é—´çš„å¹³è¡¡ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„æç¤ºä¿çœŸåº¦ï¼Œåªéœ€å‡å°‘strengthã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œstrength=0.4æ˜¯æœ€å¥½çš„ã€‚å»ºè®®strengthä¸å¤§äº0.6ã€‚

diffusersåŠ è½½æ‰©æ•£æ¨¡å‹      

    VAE: stabilityai--sd-vae-ft-mse
    StableDiffusion: Realistic_Vision_V4.0_noVAE
    MoMA: 
        Multi-modal LLM: MoMA_llava_7b (13 GB)
        Attentions and mappings: attn_adapters_projectors.th (151 Mb)





è¾“å…¥æ­£æ–¹å½¢çš„å•†å“å›¾æŠ¥é”™

1.
permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3


2.

    img,mask = self.moMA_generator.generate_with_MoMA(batch,llava_emb=llava_emb,seed=sample_id+seed,device=self.args.device)                            
        152     self.reset()
        153     ###

    File ~/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)
        112 @functools.wraps(func)
        113 def decorate_context(*args, **kwargs):
        114     with ctx_factory():
    --> 115         return func(*args, **kwargs)

    File /teams/ai_model_1667305326/WujieAITeam/private/lujunda/newlytest/MoMA/model_lib/moMA_generator.py:201, in MoMA_generator.generate_with_MoMA(self, batch, llava_emb, seed, device)
    ...
        455                     _pair(0), self.dilation, self.groups)
    --> 456 return F.conv2d(input, weight, bias, self.stride,
        457                 self.padding, self.dilation, self.groups)

    RuntimeError: Given groups=1, weight of size [128, 3, 3, 3], expected input[1, 4, 512, 512] to have 3 channels, but got 4 channels instead













# ä»£ç 

## åˆå§‹åŒ– åŠ è½½

    class MoMA_generator:
        def __init__(self, device,args):
            self.args = args
            self.device = device
            
            noise_scheduler = DDIMScheduler(num_train_timesteps=1000,beta_start=0.00085,beta_end=0.012,beta_schedule="scaled_linear",clip_sample=False,set_alpha_to_one=False,steps_offset=1,)
            
            print('Loading VAE: stabilityai--sd-vae-ft-mse...')
            vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse")
            
            print('Loading StableDiffusion: Realistic_Vision...')
            self.pipe = StableDiffusionPipeline.from_pretrained(
                "SG161222/Realistic_Vision_V4.0_noVAE",
                torch_dtype=torch.bfloat16,
                scheduler=noise_scheduler,
                vae=vae,
                feature_extractor=None,
                safety_checker=None,
            ).to(self.device)

            self.unet = self.pipe.unet
            add_function(self.pipe)
            self.pipe.moMA_generator = self

            self.set_ip_adapter()
            self.image_proj_model = self.init_proj()


## ip_adapter

    def set_ip_adapter(self):
        unet = self.unet
        attn_procs = {}
        for name in unet.attn_processors.keys():
            cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
            if name.startswith("mid_block"):
                hidden_size = unet.config.block_out_channels[-1]
            elif name.startswith("up_blocks"):
                block_id = int(name[len("up_blocks.")])
                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
            elif name.startswith("down_blocks"):
                block_id = int(name[len("down_blocks.")])
                hidden_size = unet.config.block_out_channels[block_id]
            if cross_attention_dim is None:
                attn_procs[name] = IPAttnProcessor_Self(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim,scale=1.0,num_tokens=4).to(self.device, dtype=torch.float16)
            else:
                attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim,scale=1.0,num_tokens=4).to(self.device, dtype=torch.float16)
        unet.set_attn_processor(attn_procs)

è¿™æ˜¯ä»€ä¹ˆæ„æ€ ç”¨äº†ipadapter?

![alt text](assets/MOMA/image-10.png)


### IPAttnProcessor_Self

    class IPAttnProcessor_Self(nn.Module):
        r"""
        Attention processor for IP-Adapater. (But for self attention)













### IPAttnProcessor

    class IPAttnProcessor(nn.Module):
        r"""
        Attention processor for IP-Adapater.
        Args:
            hidden_size (`int`):
                The hidden size of the attention layer.
            cross_attention_dim (`int`):
                The number of channels in the `encoder_hidden_states`.
            scale (`float`, defaults to 1.0):
                the weight scale of image prompt.
            num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):
                The context length of the image features.
        """

        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)

call 

        query = attn.to_q(hidden_states)
        
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        else:
            # get encoder_hidden_states, ip_hidden_states
            end_pos = encoder_hidden_states.shape[1] - self.num_tokens
            encoder_hidden_states, ip_hidden_states = encoder_hidden_states[:, :end_pos, :], encoder_hidden_states[:, end_pos:, :]
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)

        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)

        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)


        # for ip-adapter
        if self.enabled:
            if self.mode == 'inject' or self.mode == 'masked_generation':
                ip_key = self.to_k_ip(ip_hidden_states.to(torch.float16))
                ip_value = self.to_v_ip(ip_hidden_states.to(torch.float16))
                ip_key = attn.head_to_batch_dim(ip_key)
                ip_value = attn.head_to_batch_dim(ip_value)
                ip_attention_probs = attn.get_attention_scores(query, ip_key.to(torch.float32), None)
                ip_hidden_states = torch.bmm(ip_attention_probs, ip_value.to(torch.float32))
                ip_hidden_states = attn.batch_to_head_dim(ip_hidden_states)
                if (self.mask_ig_prev is not None) and self.mode == 'masked_generation': 
                    mask_ig_prev = rearrange(F.interpolate(self.mask_ig_prev,size=int(math.sqrt(query.shape[1]))),"b c h w -> b (h w) c")
                    if not mask_ig_prev.shape[0]==ip_hidden_states.shape[0]: mask_ig_prev = mask_ig_prev.repeat(2,1,1)
                    ip_hidden_states = ip_hidden_states * mask_ig_prev
                hidden_states = hidden_states + self.scale * ip_hidden_states
                !!!!!!!!!!!!!!!!!!1

            if self.mode == 'extract' or self.mode == 'masked_generation':
                subject_idxs = self.subject_idxs*2 if not (hidden_states.shape[0] == len(self.subject_idxs)) else self.subject_idxs
                assert (hidden_states.shape[0] == len(subject_idxs))
                attentions = rearrange(attention_probs, '(b h) n d -> b h n d', h=8).mean(1)
                attn_extracted = [attentions[i, :, subject_idxs[i]].sum(-1) for i in range(hidden_states.shape[0])]  
                attn_extracted = [(atn-atn.min())/(atn.max()-atn.min()) for atn in attn_extracted]
                attn_extracted = torch.stack(attn_extracted, dim=0)
                attn_extracted = rearrange(attn_extracted, 'b (h w) -> b h w', h=int(math.sqrt(attention_probs.shape[1])))
                attn_extracted = torch.clamp(F.interpolate(attn_extracted.unsqueeze(1),size=512),min=0,max=1)
                self.mask_i = attn_extracted

        # linear proj
        hidden_states = attn.to_out[0](hidden_states)
        # dropout
        hidden_states = attn.to_out[1](hidden_states)

        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)

        return hidden_states






## æ¨ç†

    @torch.no_grad()
    def generate_with_MoMA(
        self,
        batch,
        llava_emb=None,
        seed=None,
        device='cuda',
    ):
        self.reset_all()
        img_ig,mask_id,subject,prompt = batch['image'].half().to(device),batch['mask'].half().to(device),batch['label'][0],batch['text'][0]

        prompt = [f"photo of a {subject}. "+ prompt]

        promptæ³¨å…¥æ–¹å¼     

        subject_idx = get_subject_idx(self.pipe,prompt,[subject],self.device)
        negative_prompt = None 
            
        # get context-cross-attention feature (from MLLM decoder)
        cond_llava_embeds, uncond_llava_embeds = self.get_image_crossAttn_feature(llava_emb,num_samples=1)
        # get subject-cross-attention feature (from Unet)
        self.get_image_selfAttn_feature(img_ig,subject) # features are stored in attn_processors

        è·å– ä¸Šä¸‹æ–‡ å’Œ ç›®æ ‡ç‰©ä½“ ç‰¹å¾

        with torch.inference_mode():
            prompt_embeds = self.pipe._encode_prompt(
                prompt, device=self.device, num_images_per_prompt=1, do_classifier_free_guidance=True, negative_prompt=negative_prompt)
            negative_prompt_embeds_, prompt_embeds_ = prompt_embeds.chunk(2)
            prompt_embeds = torch.cat([prompt_embeds_, cond_llava_embeds], dim=1)
            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_llava_embeds], dim=1)
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])

        generator = torch.Generator(self.device).manual_seed(seed) if seed is not None else None
        
        self.set_self_mask('eraseAll')
        self.toggle_enable_flag('all')
        self.toggle_extract_inject_flag('all','masked_generation')
        self.set_self_mask('self','id',mask_id) 
        self.set_cross_subject_idxs(subject_idx)
        
        images, mask = self.pipe.generate_with_adapters(
            self.pipe,
            prompt_embeds,
            50,
            generator,
        )
        images = torch.clip((images+1)/2.0,min=0.0,max=1.0)

        return images.cpu(), mask.cpu()


## get_subject_idx

æ³¨å…¥subjectä¿¡æ¯äºŒæ¬¡éªŒè¯åè·å¾—

    def get_subject_idx(model,prompt,src_subject,device):
        tokenized_prompt = model.tokenizer(prompt,padding="max_length",max_length=model.tokenizer.model_max_length,truncation=True,return_tensors="pt",).to(device)
        input_ids = tokenized_prompt['input_ids']
        src_subject_idxs = []
        for subject,input_id in zip(src_subject,input_ids):
            src_subject_token_id = [model.tokenizer.encode(i, add_special_tokens=False)[0] for i in subject.split(' ')]
            src_subject_idxs = [i for i, x in enumerate(input_id.tolist()) if x in src_subject_token_id]
        return [src_subject_idxs]


## åˆ†æ
å†…éƒ¨å¼€å¯ip_adapter




# å…¶ä»–
## DeepFloyd IF
æ–°çš„ç”Ÿå›¾æ¨¡å‹DeepFloyd IFæ¥äº†ï¼Œå¯ä»¥æ‹³æ‰“Stable Diffusionï¼Œè„šè¸¢Dall-Eï¼Ÿ

2023.05    
https://github.com/deep-floyd/IF    

Stability AIä¸å®ƒçš„å¤šæ¨¡å¼AIç ”ç©¶å®éªŒå®¤DeepFloydå…±åŒå®£å¸ƒç ”ç©¶ç‰ˆæœ¬DeepFloyd IFçš„å‘å¸ƒ,è¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„æ–‡text-to-imageçº§è”åƒç´ æ‰©æ•£æ¨¡å‹ï¼ˆcascaded pixel diffusion modelï¼‰ï¼Œå¤ç°äº†Googleçš„Imagenï¼ˆText-to-Image Diffusion Modelsï¼‰ã€‚

å¯¹æ¯”Stable Diffusionï¼ˆå¯ä»¥çœ‹æˆ‘ä»¥å‰çš„æ–‡ç« ï¼šåŒ—æ–¹çš„éƒï¼šæ·±å…¥æµ…å‡ºè®²è§£Stable DiffusionåŸç†ï¼Œæ–°æ‰‹ä¹Ÿèƒ½çœ‹æ˜ç™½ï¼‰ï¼ŒImagenä¹Ÿä¾èµ–äºä¸€ä¸ªå†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼šå…ˆå°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºåµŒå…¥ï¼Œç„¶åç”±æ‰©æ•£æ¨¡å‹è§£ç æˆå›¾åƒã€‚ä½†ä¸åŒçš„æ˜¯ï¼ŒImagenå¹¶æ²¡æœ‰ä½¿ç”¨å¤šæ¨¡æ€è®­ç»ƒçš„CLIPï¼Œè€Œæ˜¯ä½¿ç”¨äº†å¤§å‹T5-XXLè¯­è¨€æ¨¡å‹ã€‚è€ŒStabilityAIæ¨å‡ºçš„DeepFloyd IFå¤åˆ»çš„æ­£æ˜¯è¿™ä¸€æ¶æ„ã€‚åŒæ—¶DeepFloyd IFåœ¨åƒç´ ç©ºé—´å·¥ä½œï¼Œä¸Stable Diffusionä¸åŒï¼Œæ‰©æ•£æ˜¯åœ¨åƒç´ çº§å®ç°çš„ã€‚

è¿™äº›ç‰¹ç‚¹ä½¿å®ƒå¯ä»¥æ›´ç²¾ç¡®çš„ç”Ÿæˆå›¾åƒï¼Œä¾‹å¦‚ç”Ÿæˆå¸¦æœ‰ç‰¹å®šæ–‡æœ¬çš„å›¾ç‰‡ã€‚åœ¨æµ‹è¯•ä¸­ï¼ŒDeepFloyd IFç›´æ¥è¶…è¶Šäº†è°·æ­Œçš„Imagenï¼Œä»¥åŠä¸€ä¼—ç«å“ï¼ˆåŒ…æ‹¬å…„å¼Ÿäº§å“Stable Diffusionï¼‰ã€‚

DeepFloyd IFï¼Œå…·æœ‰é«˜åº¦çš„ç…§ç‰‡çº§çœŸå®æ„Ÿå’Œè¯­è¨€ç†è§£ã€‚DeepFloyd IF æ˜¯ä¸€ä¸ªç”±å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨å’Œä¸‰ä¸ªçº§è”åƒç´ æ‰©æ•£æ¨¡å—ç»„æˆçš„æ¨¡å—ï¼š

ä¸€ä¸ªåŸºäºæ–‡æœ¬æç¤ºï¼ˆText Promptï¼‰ç”Ÿæˆ 64x64 åƒç´ å›¾åƒçš„åŸºæœ¬æ¨¡å‹å’Œä¸¤ä¸ªè¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æ—¨åœ¨ç”Ÿæˆåˆ†è¾¨ç‡ä¸æ–­æé«˜çš„å›¾åƒï¼š256x256 åƒç´ å’Œ 1024x1024 åƒç´ ã€‚è¯¥æ¨¡å‹çš„æ‰€æœ‰é˜¶æ®µéƒ½åˆ©ç”¨åŸºäº T5 è½¬æ¢å™¨çš„å†»ç»“æ–‡æœ¬ç¼–ç å™¨æ¥æå–æ–‡æœ¬åµŒå…¥ï¼Œç„¶åå°†å…¶é¦ˆé€åˆ°é€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œæ³¨æ„åŠ›æ± å¢å¼ºçš„ UNet æ¶æ„ä¸­ã€‚

ç»“æœæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¨¡å‹ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨COCOæ•°æ®é›†ä¸Šå®ç°äº†6.66çš„ zero-shot FIDåˆ†æ•°ã€‚ç ”ç©¶è€…çš„å·¥ä½œä½“ç°äº†æ›´å¤§çš„UNetæ¶æ„åœ¨çº§è”æ‰©æ•£æ¨¡å‹ç¬¬ä¸€é˜¶æ®µçš„æ½œåŠ›ï¼Œå¹¶æç»˜äº†æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„å…‰æ˜æœªæ¥ã€‚
æè¿°å’Œç‰¹å¾

â€¢æ·±åº¦æ–‡æœ¬æç¤º(text prompt)ç†è§£:

åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹T5-XXL-1.1ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ã€‚å¤§é‡çš„æ–‡æœ¬-å›¾åƒäº¤å‰æ³¨æ„åŠ›å±‚(text-image cross-attention layers)ä¹Ÿæä¾›äº†æ›´å¥½çš„æç¤ºå’Œå›¾åƒè”ç›Ÿã€‚

â€¢å°†æ–‡æœ¬æè¿°åº”ç”¨äºå›¾åƒ:

ç»“åˆT5æ¨¡å‹çš„æ™ºèƒ½æ€§,DeepFloyd IFç”Ÿæˆè¿è´¯æ¸…æ™°çš„æ–‡æœ¬ä»¥åŠå‡ºç°åœ¨å„ç§ç©ºé—´å…³ç³»ä¸­çš„ä¸åŒå±æ€§çš„å¯¹è±¡ã€‚åˆ°ç›®å‰ä¸ºæ­¢,è¿™äº›ç”¨ä¾‹å¯¹å¤§å¤šæ•°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¥è¯´éƒ½æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚

â€¢é«˜åº¦å†™çœŸæ€§:

è¿™ä¸€ç‰¹ç‚¹åæ˜ åœ¨ä»¤äººå°è±¡æ·±åˆ»çš„ zero-shot FIDå¾—åˆ†6.66ä¸Š,è¯¥å¾—åˆ†æ˜¯åœ¨COCO datasetä¸Šè·å¾—çš„(FIDæ˜¯è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ€§èƒ½çš„ä¸»è¦æŒ‡æ ‡;åˆ†æ•°è¶Šä½,æ€§èƒ½è¶Šå¥½)ã€‚

â€¢å®½é«˜æ¯”è½¬æ¢:

ç”Ÿæˆéæ ‡å‡†å®½é«˜æ¯”çš„å›¾åƒçš„èƒ½åŠ›,å‚ç›´æˆ–æ°´å¹³çš„,ä»¥åŠæ ‡å‡†çš„æ–¹å½¢å®½é«˜æ¯”ã€‚

â€¢çº§è”:

DeepFloyd IFä»¥çº§è”æ–¹å¼å¯¹é«˜åˆ†è¾¨ç‡æ•°æ®è¿›è¡Œå»ºæ¨¡,ä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡ä¸‹å•ç‹¬è®­ç»ƒçš„ä¸€ç³»åˆ—æ¨¡å‹ã€‚è¯¥è¿‡ç¨‹ä»ç”Ÿæˆå”¯ä¸€ä½åˆ†è¾¨ç‡æ ·æœ¬çš„åŸºæœ¬æ¨¡å‹(â€œplayerâ€)å¼€å§‹,ç„¶åç”±è¿ç»­çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹(â€œamplifiersâ€)ä¸Šé‡‡æ ·ä»¥äº§ç”Ÿé«˜åˆ†è¾¨ç‡å›¾åƒã€‚

![alt text](assets/MOMA/861ff9f788944c40b9e862c9c8313c08.png)


è¿™å¹…ç”Ÿæˆæµç¨‹å›¾ä»£è¡¨ä¸‰ä¸ªé˜¶æ®µçš„å·¥ä½œï¼šæ–‡æœ¬æç¤ºé€šè¿‡å†»ç»“çš„T5-XXLè¯­è¨€æ¨¡å‹ä¼ é€’,å°†å…¶è½¬æ¢ä¸ºå®šæ€§æ–‡æœ¬è¡¨ç¤ºã€‚

ç¬¬ä¸€é˜¶æ®µ:åŸºæœ¬æ‰©æ•£æ¨¡å‹å°†å®šæ€§æ–‡æœ¬è½¬æ¢ä¸º64x64å›¾åƒã€‚DeepFloydå›¢é˜Ÿå·²è®­ç»ƒä¸‰ä¸ªç‰ˆæœ¬çš„åŸºæœ¬æ¨¡å‹,æ¯ä¸ªæ¨¡å‹çš„å‚æ•°éƒ½ä¸åŒ:IF-I 400Mã€IF-I 900Må’ŒIF-I 4.3Bã€‚

ç¬¬äºŒé˜¶æ®µ:ä¸ºäº†â€œæ”¾å¤§â€å›¾åƒ,åº”ç”¨ä¸¤ä¸ªæ–‡æœ¬æ¡ä»¶è¶…åˆ†è¾¨ç‡æ¨¡å‹(Efficient U-Net)å¯¹åŸºæœ¬æ¨¡å‹çš„è¾“å‡ºã€‚ç¬¬ä¸€ä¸ªæ¨¡å‹å°†64x64å›¾åƒæ”¾å¤§åˆ°256x256å›¾åƒã€‚åŒæ ·,è¯¥æ¨¡å‹ä¹Ÿæœ‰å‡ ä¸ªç‰ˆæœ¬å¯ç”¨:IF-II 400Må’ŒIF-II 1.2Bã€‚

ç¬¬ä¸‰é˜¶æ®µ:åº”ç”¨ç¬¬äºŒä¸ªè¶…åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹äº§ç”Ÿç”ŸåŠ¨çš„1024x1024å›¾åƒã€‚æœ€ç»ˆçš„ç¬¬ä¸‰é˜¶æ®µæ¨¡å‹IF-IIIæœ‰700Mä¸ªå‚æ•°ã€‚æ³¨æ„:ç ”ç©¶è€…è¿˜æ²¡æœ‰å‘å¸ƒè¿™ä¸ªç¬¬ä¸‰é˜¶æ®µæ¨¡å‹;ç„¶è€Œ,IFæ¨¡å‹çš„æ¨¡å—åŒ–ç‰¹æ€§å…è®¸ä»–ä»¬åœ¨ç¬¬ä¸‰é˜¶æ®µä½¿ç”¨å…¶ä»–æ”¾å¤§æ¨¡å‹ - å¦‚Stable Diffusion x4 Upscalerã€‚


æ•°æ®é›†è®­ç»ƒ

DeepFloyd IFåœ¨å®šåˆ¶çš„LAION-Aæ•°æ®é›†ä¸Šè®­ç»ƒ,è¯¥æ•°æ®é›†ç”±10äº¿å¯¹é«˜è´¨é‡å›¾åƒå’Œæ–‡æœ¬ç»„æˆã€‚LAION-Aæ˜¯LAION-5Bæ•°æ®é›†è‹±è¯­éƒ¨åˆ†çš„ä¼˜åŒ–åçš„å­é›†ï¼ŒåŒ…æ‹¬åŸºäºç›¸ä¼¼æ€§å“ˆå¸Œè¿›è¡Œå»é‡ã€é¢å¤–æ¸…ç†ä»¥åŠå¯¹åŸå§‹æ•°æ®é›†çš„å…¶ä»–ä¿®æ”¹ã€‚DeepFloydçš„å®šåˆ¶è¿‡æ»¤å™¨ç”¨äºåˆ é™¤å¸¦æ°´å°çš„ã€ä¸é€‚åˆå·¥ä½œç¯å¢ƒçš„å’Œå…¶ä»–ä¸æ°å½“çš„å†…å®¹ã€‚


![alt text](assets/MOMA/image-8.png)


è¿™ä¸ªå®éªŒåªæœ‰DeepFloyd IFæ­£ç¡®æ˜¾ç¤ºäº†æ–‡å­—ã€‚   
è¿™ä¸ªå®éªŒåªæœ‰DeepFloyd IFæ¯”è¾ƒæ­£ç¡®æ˜¾ç¤ºäº†æ–‡å­—ï¼ˆ4å¼ å›¾å°±1å¼ å›¾å¤šäº†ä¸€ä¸ªtï¼‰ã€‚   
Prompt: a neon sign says "It's Saturday"   
ä¸æ­£ç¡®     



è¿è¡Œ

ç›®å‰DeepFloyd IFæ¨¡å‹ä¹Ÿå·²ç»é›†æˆåˆ°äº†diffusersåº“

I. Dream

Dream is the text-to-image mode of the IF model

II. Zero-shot Image-to-Image Translation

III. Super Resolution

For super-resolution, users can run IF-II and IF-III or 'Stable x4' on an image that was not necessarely generated by IF (two cascades):=



IV. Zero-shot Inpainting


## Kohya Trainer
https://github.com/Linaqruf/kohya-trainer

pageå¤ªä¹±     



## sd script
è®­ç»ƒ




## llava
LLaVAï¼ˆLarge Language and Vision Assistantï¼‰

    LLaVA æ˜¯ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå°† CLIP çš„å¼€æ”¾å¼è§†è§‰ç¼–ç å™¨ä¸ LLaMA çš„è¯­è¨€è§£ç å™¨ç›¸è¿æ¥ï¼Œå¹¶åœ¨ç”Ÿæˆçš„è§†è§‰-è¯­è¨€æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒ
### è®ºæ–‡ä¿¡æ¯

[Submitted on 5 Oct 2023]     
Improved Baselines with Visual Instruction Tuning

[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.

Improved Baselines with Visual Instruction Tuning [Paper] [HF]
Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee

Visual Instruction Tuning (NeurIPS 2023, Oral) [Paper] [HF]
Haotian Liu*, Chunyuan Li*, Qingyang Wu, Yong Jae Lee (*Equal Contribution)


Acknowledgement     
Vicuna: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!

Related Projects     

    Instruction Tuning with GPT-4
    LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day
    Otter: In-Context Multi-Modal Instruction Tuning
For future project ideas, please check out:

    SEEM: Segment Everything Everywhere All at Once
    Grounded-Segment-Anything to detect, segment, and generate anything by marrying Grounding DINO and Segment-Anything.


### åŸç†

æ–‡ç« ä¸»è¦è´¡çŒ®ï¼š
1. å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿè¸ªæ•°æ®ï¼ˆå¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ•°æ®é‡å¡‘çš„è§†è§’å’Œæµç¨‹ï¼Œä½¿ç”¨GPT-4å°†å›¾åƒ-æ–‡æœ¬å¯¹è½¬æ¢ä¸ºé€‚å½“çš„æŒ‡ä»¤æ ¼å¼ï¼›

ä¸ºäº†å°†å›¾åƒç¼–ç ä¸ºå…¶è§†è§‰ç‰¹å¾ä»¥æç¤ºçº¯æ–‡æœ¬GPTï¼Œä½¿ç”¨ä¸¤ç§ç¬¦å·è¡¨ç¤ºï¼š ï¼ˆiï¼‰æ ‡é¢˜é€šå¸¸ä»å„ç§è§’åº¦æè¿°è§†è§‰åœºæ™¯ã€‚ ï¼ˆiiï¼‰è¾¹ç•Œæ¡†é€šå¸¸å®šä½åœºæ™¯ä¸­çš„å¯¹è±¡ï¼Œæ¯ä¸ªæ¡†ç¼–ç å¯¹è±¡æ¦‚å¿µåŠå…¶ç©ºé—´ä½ç½®

2. è§†è§‰æŒ‡ä»¤è®­ç»ƒ(Visual Instruction Tuning)æ¨¡å‹    
![alt text](assets/MOMA/v2-22a2ec7ca8597813205797e79da5632c_720w.webp)

![alt text](assets/MOMA/image-9.png)

Traning :

Step1-ç‰¹å¾å¯¹é½é¢„è®­ç»ƒ

1. æ•°æ®è½¬æ¢ï¼š ä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆä¸€ä¸ªç®€å•çš„é—®é¢˜,è¯·æ±‚åŠ©æ‰‹ç®€è¦æè¿°å›¾åƒå†…å®¹ã€‚å°†å›¾åƒ-æ–‡æœ¬å¯¹è½¬æ¢ä¸ºé—®é¢˜(æŒ‡ä»¤)-å›ç­”(æè¿°) æ ¼å¼

2. æ¨¡å‹æ„å»ºï¼š ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å’ŒLLaMAè¯­è¨€æ¨¡å‹,åŠ å…¥ä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚Wå°†å›¾åƒç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç‰¹å¾ç©ºé—´ï¼ˆå‚ç…§ä¸Šè¿°Vision Encoderæ¨¡å‹ï¼‰

3. è®­ç»ƒç›®æ ‡ï¼š æœ€å¤§åŒ–å›ç­”çš„ç”Ÿæˆä¼¼ç„¶æ¦‚ç‡,ä»…ä¼˜åŒ–æŠ•å½±å±‚Wçš„å‚æ•°,å†»ç»“è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹

4. è®­ç»ƒæ•ˆæœï¼š

å­¦ä¼šå°†å›¾åƒç‰¹å¾è½¬æ¢ä¸ºè¯­è¨€æ¨¡å‹å¯ç†è§£çš„è¡¨ç¤º,å®ç°ä¸¤è€…çš„å¯¹é½ã€‚

Step2-ç«¯åˆ°ç«¯å¾®è°ƒ

1. æ•°æ®è½¬æ¢ï¼š å°†3ç§ç±»å‹(å¯¹è¯ã€è¯¦ç»†æè¿°ã€å¤æ‚æ¨ç†)ç»„ç»‡æˆç»Ÿä¸€çš„æŒ‡ä»¤-å›ç­”åºåˆ—æ ¼å¼

2. æ¨¡å‹æ„å»ºï¼š å†»ç»“CLIPè§†è§‰ç¼–ç å™¨,è§£å†»LLaMAå‚æ•°åŠæŠ•å½±å±‚W

3. è®­ç»ƒæ•ˆæœï¼š æé«˜æ¨¡å‹éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›,å®ç°è§†è§‰é—®ç­”
tags: ä¸¤æ¬¡è®­ç»ƒçš„ä¸åŒä¹‹å¤„åœ¨äºé¢„è®­ç»ƒé˜¶æ®µä»…ä¼˜åŒ–æŠ•å½±å±‚,æ˜¯ä¸ºäº†å…ˆè·å¾—å›¾åƒç‰¹å¾ä¸è¯­è¨€ç‰¹å¾çš„å¯¹é½,è€Œä¸ç ´åè¯­è¨€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼›ä¸¤é˜¶æ®µè®­ç»ƒæ–¹å¼åˆ©ç”¨ä¸åŒç±»å‹çš„æ•°æ®ã€‚ 




## Vicuna-13B
An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.

https://github.com/lm-sys/FastChat


    [2024/03] ğŸ”¥ We released Chatbot Arena technical report.
    [2023/09] We released LMSYS-Chat-1M, a large-scale real-world LLM conversation dataset. Read the report.
    [2023/08] We released Vicuna v1.5 based on Llama 2 with 4K and 16K context lengths. Download weights.
    [2023/07] We released Chatbot Arena Conversations, a dataset containing 33k conversations with human preferences. Download it here.





## mermaid
å…³äº Mermaid

Mermaid æ˜¯ä¸€ä¸ªåŸºäº Javascript çš„å›¾è¡¨ç»˜åˆ¶å·¥å…·ï¼Œé€šè¿‡è§£æç±» Markdown çš„æ–‡æœ¬è¯­æ³•æ¥å®ç°å›¾è¡¨çš„åˆ›å»ºå’ŒåŠ¨æ€ä¿®æ”¹ã€‚Mermaid è¯ç”Ÿçš„ä¸»è¦ç›®çš„æ˜¯è®©æ–‡æ¡£çš„æ›´æ–°èƒ½å¤ŸåŠæ—¶è·Ÿä¸Šå¼€å‘è¿›åº¦ã€‚

![alt text](assets/MOMA/5d33325464ddc1d2c029fb190f4f5a06.png)


    Mermaid è‡´åŠ›äºè§£å†³ Doc-Rot è¿™ä¸ªä»¤äººå¤´ç–¼çš„é—®é¢˜ã€‚

ç»˜å›¾å’Œç¼–å†™æ–‡æ¡£èŠ±è´¹äº†å¼€å‘è€…å®è´µçš„å¼€å‘æ—¶é—´ï¼Œè€Œä¸”éšç€ä¸šåŠ¡çš„å˜æ›´ï¼Œå®ƒå¾ˆå¿«å°±ä¼šè¿‡æœŸã€‚ ä½†æ˜¯å¦‚æœç¼ºå°‘äº†å›¾è¡¨æˆ–æ–‡æ¡£ï¼Œå¯¹äºç”Ÿäº§åŠ›å’Œå›¢é˜Ÿæ–°äººçš„ä¸šåŠ¡å­¦ä¹ éƒ½ä¼šäº§ç”Ÿå·¨å¤§çš„é˜»ç¢ã€‚

Mermaid é€šè¿‡å‡å°‘åˆ›å»ºå¯ä¿®æ”¹çš„å›¾è¡¨æ‰€éœ€è¦çš„æ—¶é—´ã€ç²¾åŠ›å’Œå·¥å…·æ¥è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œä»è€Œæé«˜äº†å†…å®¹çš„æ™ºèƒ½åŒ–å’Œå¯é‡ç”¨æ€§ã€‚ ä½œä¸ºä¸€ä¸ªåŸºäºæ–‡æœ¬çš„ç»˜å›¾å·¥å…·ï¼Œ Mermaid å¤©ç”Ÿå°±æ˜“äºç»´æŠ¤å’Œæ›´æ–°ï¼Œå®ƒä¹Ÿå¯ä»¥ä½œä¸ºç”Ÿäº§è„šæœ¬ï¼ˆæˆ–å…¶ä»–ä»£ç ï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œä½¿å¾—æ–‡æ¡£ç¼–å†™å˜å¾—æ›´åŠ ç®€å•ã€‚ æœ‰äº†å®ƒä¹‹åï¼Œå¼€å‘è€…å¯ä»¥ä»ç»´æŠ¤æ–‡æ¡£è¿™ä¸ªä¸å¼€å‘å‰²ç¦»ä¸”éº»çƒ¦çš„ä»»åŠ¡ä¸­è§£æ”¾å‡ºæ¥ã€‚ 

```mermaid
flowchart BT
    %% Declare Nodes
    gws("Gradio (UI Server)")
    c("Controller (API Server):<br/>PORT: 10000")
    mw7b("Model Worker:<br/>llava-v1.5-7b<br/>PORT: 40000")
    mw13b("Model Worker:<br/>llava-v1.5-13b<br/>PORT: 40001")
    sglw13b("SGLang Backend:<br/>llava-v1.6-34b<br/>http://localhost:30000")
    lsglw13b("SGLang Worker:<br/>llava-v1.6-34b<br/>PORT: 40002")

    %% Declare Styles
    classDef data fill:#3af,stroke:#48a,stroke-width:2px,color:#444
    classDef success fill:#8f8,stroke:#0a0,stroke-width:2px,color:#444
    classDef failure fill:#f88,stroke:#f00,stroke-width:2px,color:#444

    %% Assign Styles
    class id,od data;
    class cimg,cs_s,scsim_s success;
    class ncimg,cs_f,scsim_f failure;

    subgraph Demo Connections
        direction BT
        c<-->gws
        
        mw7b<-->c
        mw13b<-->c
        lsglw13b<-->c
        sglw13b<-->lsglw13b
    end
```











# ç»“å°¾
# CustomNet
: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models.   
CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models    





éœ€è¦é¢å¤–å®‰è£…  
basicsr   
ä½†ä¾èµ–tb-nightly    
æ¸…åæºä¸å­˜åœ¨tb-nightly    
pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple    


CustomNet is novel unified customization method that can generate harmonious customized images without test-time optimization. CustomNet supports explicit viewpoint, location, text controls while ensuring object identity preservation.    
![alt text](assets/pixart-sigma&customnet/image.png)   
ä¸çŸ¥é“background-imageåœ¨å“ªé‡Œè®¾ç½®ï¼Œç¤ºä¾‹ä»£ç å¥½åƒæ²¡æœ‰   


![alt text](assets/pixart-sigma&customnet/image-1.png)
å¯ä»¥é€šè¿‡æ–‡å­—æè¿°æˆ–ç”¨æˆ·å®šä¹‰çš„èƒŒæ™¯æ¥å®ç°ä½ç½®æ§åˆ¶å’Œçµæ´»çš„èƒŒæ™¯æ§åˆ¶ã€‚   
`Some` methods `finetune T2I models for each object individually at test-time`, which tend to be overfitted and time-consuming     
`Others train an extra encoder` to extract object visual information for customization efficiently but struggle to preserve the objectâ€™s identity.       
we incorporates `3D novel view synthesis` capabilities into the customization process    
we propose a `dataset construction pipeline` to better handle real-world objects and complex backgrounds.    
Additionally, we introduce delicate designs that enable `location control and flexible background control` through textual descriptions or user-defined backgrounds. Our method allows for object customization without the need of test-time optimization     


å°†å®šåˆ¶å¯¹è±¡åˆå¹¶åˆ°å›¾åƒç”Ÿæˆä¸­æ˜¯æ–‡æœ¬åˆ°å›¾åƒ (T2I) ç”Ÿæˆçš„ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„åŠŸèƒ½ã€‚ä¸€äº›æ–¹æ³•åœ¨æµ‹è¯•æ—¶å•ç‹¬å¾®è°ƒæ¯ä¸ªå¯¹è±¡çš„ T2I æ¨¡å‹ï¼Œè¿™å¾€å¾€ä¼šè¿‡åº¦æ‹Ÿåˆä¸”è€—æ—¶ã€‚å…¶ä»–äººè®­ç»ƒé¢å¤–çš„ç¼–ç å™¨æ¥æå–å¯¹è±¡è§†è§‰ä¿¡æ¯ä»¥è¿›è¡Œæœ‰æ•ˆçš„å®šåˆ¶ï¼Œä½†å¾ˆéš¾ä¿ç•™å¯¹è±¡çš„èº«ä»½ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† CustomNetï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç¼–ç å™¨çš„ç»Ÿä¸€å¯¹è±¡å®šåˆ¶æ¡†æ¶ï¼Œå®ƒæ˜ç¡®åœ°å°† 3D æ–°é¢–è§†å›¾åˆæˆåŠŸèƒ½åˆå¹¶åˆ°å®šåˆ¶è¿‡ç¨‹ä¸­ã€‚è¿™ç§é›†æˆæœ‰åŠ©äºç©ºé—´ä½ç½®å’Œè§†ç‚¹çš„è°ƒæ•´ï¼Œäº§ç”Ÿä¸åŒçš„è¾“å‡ºï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä¿ç•™å¯¹è±¡çš„èº«ä»½ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†æ„å»ºç®¡é“ï¼Œä»¥æ›´å¥½åœ°å¤„ç†ç°å®ä¸–ç•Œçš„å¯¹è±¡å’Œå¤æ‚çš„èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ç²¾è‡´çš„è®¾è®¡ï¼Œå¯ä»¥é€šè¿‡æ–‡å­—æè¿°æˆ–ç”¨æˆ·å®šä¹‰çš„èƒŒæ™¯æ¥å®ç°ä½ç½®æ§åˆ¶å’Œçµæ´»çš„èƒŒæ™¯æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å¯¹è±¡å®šåˆ¶ï¼Œæ— éœ€æµ‹è¯•æ—¶ä¼˜åŒ–ï¼Œæä¾›å¯¹è§†ç‚¹ã€ä½ç½®å’Œæ–‡æœ¬çš„åŒæ­¥æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½ä¿å­˜ã€å¤šæ ·æ€§å’Œå’Œè°æ€§æ–¹é¢ä¼˜äºå…¶ä»–å®šåˆ¶æ–¹æ³•ã€‚

ä½¿ç”¨æ–¹æ³•ï¼šè¾“å…¥ä¸€å¼ ç™½èƒŒæ™¯çš„ç‰©å“å›¾ç‰‡ä½œä¸ºå‚è€ƒå›¾å›¾ï¼Œè¾“å…¥promptç¼–è¾‘èƒŒæ™¯ï¼Œå¯ä»¥é€šè¿‡å‚æ•°æ”¹å˜ç‰©å“åœ¨å›¾ç‰‡ä¸­çš„åæ ‡ä½ç½®å’Œ3Dè§†è§’ã€‚   
æµ‹è¯•æ¨¡å‹ï¼šCustomNet   
æµ‹è¯•å‚æ•°ï¼š   
DDIMï¼Œé‡‡æ ·50æ­¥ï¼ŒGUIæ— æ³•ä¿®æ”¹    
æµ‹è¯•ç»“è®ºï¼šäººç‰©æ¢å¤æ•ˆæœå·®ï¼Œæ–‡æœ¬æ§åˆ¶ä¸å¤ªå‡†ç¡®ï¼Œä¸Šä¸‹è§†è§’ä¸å¤ªå‡†ç¡®ï¼Œå°å›¾æ—¶ï¼Œ ç»˜åˆ¶æ•ˆæœä¸å¥½ã€‚å›¾ç‰‡ä¼šè¢«é¢„å¤„ç†æˆ256*256ã€‚    










# PixArt-Î£ 
Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation   

(ğŸ”¥ New) Apr. 6, 2024. ğŸ’¥ PixArt-Î£ checkpoint 256px & 512px are released!   
(ğŸ”¥ New) Mar. 29, 2024. ğŸ’¥ PixArt-Î£ training & inference code & toy data are released!!!   

åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤ã€å¤§è¿ç†å·¥å¤§å­¦ã€é¦™æ¸¯å¤§å­¦ã€é¦™æ¸¯ç§‘æŠ€å¤§å­¦    
https://pixart-alpha.github.io/PixArt-sigma-project/    
https://arxiv.org/abs/2403.04692    
[Submitted on 7 Mar 2024 (v1), last revised 17 Mar 2024 (this version, v2)]





## è¯¥ç»„ç»‡å‰æœŸç ”ç©¶
https://arxiv.org/abs/2310.00426   
[Submitted on 30 Sep 2023 (v1), last revised 29 Dec 2023 (this version, v3)]    
PixArt-Î±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis    
è¢«yosoç”¨æ¥å¾®è°ƒæ¨¡å‹    
few_step_gen folderæœ‰ç®€ç•¥ä»‹ç»   



https://arxiv.org/abs/2401.05252    
[Submitted on 10 Jan 2024]   
PIXART-Î´: Fast and Controllable Image Generation with Latent Consistency Models    

## æ¨ç†
ä½¿ç”¨gradioæ¨ç†   
å°šä¸æ”¯æŒdiffusers   
å¯ä»¥è®­ç»ƒå’Œæ¨ç†   
æœ‰256 512 1024æ¨¡å‹    
åç»­è¿˜ä¼šå‡ºdmdæ¨¡å‹     



nvcc11.8,torch 2.0.0æ²¡è¯´æ˜cuç‰ˆæœ¬   
å¥½åƒé»˜è®¤11.7   

    File "/root/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
        return F.conv2d(input, weight, bias, self.stride,
    RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR

pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118      
å¸è½½é‡è£…     
è¿˜æ˜¯cudnné”™è¯¯   

ä½†æ˜¯è¿™ä¸ª3090æ˜¯å¯ä»¥è¿è¡Œwebuiæ¨ç†    

æœ‰äººè¯´ å…¶å®å°±æ˜¯gpuæ˜¾å­˜ä¸å¤Ÿï¼Œå‡å°ç‚¹å·¥ä½œé‡å°±å¯ä»¥äº†    

 2ã€æ‰‹åŠ¨ä½¿ç”¨cudnn

    import torch
    torch.backends.cudnn.enabled = False



## åŸç†
ä¸»è¦æ¨¡å‹ç»“æ„ä¸PixArt-Î±ç›¸åŒ   




# cosxl
Cos Stable Diffusion XL 1.0 and Cos Stable Diffusion XL 1.0 Edit   

å¯ä»¥ä¸€è‡´çš„ç”Ÿæˆ2k 4k 8k   
æ˜¾å­˜æ’ä¸º25gå·¦å³    
æ¸…æ™°åº¦æ²¡æœ‰ä¸Šå‡ï¼Œå°±æ˜¯å›¾ç‰‡å¤§å°å˜å¤§äº†    
æ‰€ä»¥æ˜¯ä¸ºä»€ä¹ˆèƒ½å¤Ÿä¸€è‡´æ€§çš„ç”Ÿæˆï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ    
è€Œä¸”é€Ÿåº¦è¿˜æŒºå¿«   

å°è¯•ä½¿ç”¨realistic_vision_v51è¿›è¡Œ4096*4096ç”Ÿå›¾   
Ran out of memory when regular VAE encoding, retrying with tiled VAE encoding.     
æ˜¾å­˜2gä¸Šå‡åˆ°5g   
42s/it     
42steps 
![alt text](assets/pixart-sigma&customnet&cosxl/WeChat93b104784546b5c1c61f7d31dd987388.jpg)
åŠä¸ªå°æ—¶    


![alt text](assets/pixart-sigma&customnet&cosxl/image.png)   
æ­£å¸¸å†™æ³•ç”Ÿå›¾æ•ˆæœä¸å¥½    
![alt text](assets/pixart-sigma&customnet&cosxl/image-2.png)   
ç¡®åˆ‡åœ°è¯´éœ€è¦ä½¿ç”¨eular42æ­¥    
![alt text](assets/pixart-sigma&customnet&cosxl/image-3.png)   
ä½¿ç”¨dpm ++ 2mä¸å¤ªæ­£å¸¸   


è¦åŠ ä¸Šip2på†™æ³•    
![alt text](assets/pixart-sigma&customnet&cosxl/image-1.png)   



![alt text](assets/pixart-sigma&customnet&cosxl/image-4.png)


ç±»ä¼¼ip2på·¥ä½œ  
[Submitted on 28 Jan 2023 (v1), last revised 2 Nov 2023 (this version, v2)]     
SEGA: Instructing Text-to-Image Models using Semantic Guidance         


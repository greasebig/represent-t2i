# CustomNet

æ›¿ä»£æ–¹æ¡ˆ lora    

: Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models.   
CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models    





éœ€è¦é¢å¤–å®‰è£…  
basicsr   
ä½†ä¾èµ–tb-nightly    
æ¸…åæºä¸å­˜åœ¨tb-nightly    
pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple    


CustomNet is novel unified customization method that can generate harmonious customized images without test-time optimization. CustomNet supports explicit viewpoint, location, text controls while ensuring object identity preservation.    
![alt text](assets/pixart-sigma&customnet/image.png)   
ä¸çŸ¥é“background-imageåœ¨å“ªé‡Œè®¾ç½®ï¼Œç¤ºä¾‹ä»£ç å¥½åƒæ²¡æœ‰   


![alt text](assets/pixart-sigma&customnet/image-1.png)
å¯ä»¥é€šè¿‡æ–‡å­—æè¿°æˆ–ç”¨æˆ·å®šä¹‰çš„èƒŒæ™¯æ¥å®ç°ä½ç½®æ§åˆ¶å’Œçµæ´»çš„èƒŒæ™¯æ§åˆ¶ã€‚   
`Some` methods `finetune T2I models for each object individually at test-time`, which tend to be overfitted and time-consuming     
`Others train an extra encoder` to extract object visual information for customization efficiently but struggle to preserve the objectâ€™s identity.       
we incorporates `3D novel view synthesis` capabilities into the customization process    
we propose a `dataset construction pipeline` to better handle real-world objects and complex backgrounds.    
Additionally, we introduce delicate designs that enable `location control and flexible background control` through textual descriptions or user-defined backgrounds. Our method allows for object customization without the need of test-time optimization     


å°†å®šåˆ¶å¯¹è±¡åˆå¹¶åˆ°å›¾åƒç”Ÿæˆä¸­æ˜¯æ–‡æœ¬åˆ°å›¾åƒ (T2I) ç”Ÿæˆçš„ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„åŠŸèƒ½ã€‚ä¸€äº›æ–¹æ³•åœ¨æµ‹è¯•æ—¶å•ç‹¬å¾®è°ƒæ¯ä¸ªå¯¹è±¡çš„ T2I æ¨¡å‹ï¼Œè¿™å¾€å¾€ä¼šè¿‡åº¦æ‹Ÿåˆä¸”è€—æ—¶ã€‚å…¶ä»–äººè®­ç»ƒé¢å¤–çš„ç¼–ç å™¨æ¥æå–å¯¹è±¡è§†è§‰ä¿¡æ¯ä»¥è¿›è¡Œæœ‰æ•ˆçš„å®šåˆ¶ï¼Œä½†å¾ˆéš¾ä¿ç•™å¯¹è±¡çš„èº«ä»½ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† CustomNetï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç¼–ç å™¨çš„ç»Ÿä¸€å¯¹è±¡å®šåˆ¶æ¡†æ¶ï¼Œå®ƒæ˜ç¡®åœ°å°† 3D æ–°é¢–è§†å›¾åˆæˆåŠŸèƒ½åˆå¹¶åˆ°å®šåˆ¶è¿‡ç¨‹ä¸­ã€‚è¿™ç§é›†æˆæœ‰åŠ©äºç©ºé—´ä½ç½®å’Œè§†ç‚¹çš„è°ƒæ•´ï¼Œäº§ç”Ÿä¸åŒçš„è¾“å‡ºï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä¿ç•™å¯¹è±¡çš„èº«ä»½ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†æ„å»ºç®¡é“ï¼Œä»¥æ›´å¥½åœ°å¤„ç†ç°å®ä¸–ç•Œçš„å¯¹è±¡å’Œå¤æ‚çš„èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ç²¾è‡´çš„è®¾è®¡ï¼Œå¯ä»¥é€šè¿‡æ–‡å­—æè¿°æˆ–ç”¨æˆ·å®šä¹‰çš„èƒŒæ™¯æ¥å®ç°ä½ç½®æ§åˆ¶å’Œçµæ´»çš„èƒŒæ™¯æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å¯¹è±¡å®šåˆ¶ï¼Œæ— éœ€æµ‹è¯•æ—¶ä¼˜åŒ–ï¼Œæä¾›å¯¹è§†ç‚¹ã€ä½ç½®å’Œæ–‡æœ¬çš„åŒæ­¥æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½ä¿å­˜ã€å¤šæ ·æ€§å’Œå’Œè°æ€§æ–¹é¢ä¼˜äºå…¶ä»–å®šåˆ¶æ–¹æ³•ã€‚

ä½¿ç”¨æ–¹æ³•ï¼šè¾“å…¥ä¸€å¼ ç™½èƒŒæ™¯çš„ç‰©å“å›¾ç‰‡ä½œä¸ºå‚è€ƒå›¾å›¾ï¼Œè¾“å…¥promptç¼–è¾‘èƒŒæ™¯ï¼Œå¯ä»¥é€šè¿‡å‚æ•°æ”¹å˜ç‰©å“åœ¨å›¾ç‰‡ä¸­çš„åæ ‡ä½ç½®å’Œ3Dè§†è§’ã€‚   
æµ‹è¯•æ¨¡å‹ï¼šCustomNet   
æµ‹è¯•å‚æ•°ï¼š   
DDIMï¼Œé‡‡æ ·50æ­¥ï¼ŒGUIæ— æ³•ä¿®æ”¹    
æµ‹è¯•ç»“è®ºï¼šäººç‰©æ¢å¤æ•ˆæœå·®ï¼Œæ–‡æœ¬æ§åˆ¶ä¸å¤ªå‡†ç¡®ï¼Œä¸Šä¸‹è§†è§’ä¸å¤ªå‡†ç¡®ï¼Œå°å›¾æ—¶ï¼Œ ç»˜åˆ¶æ•ˆæœä¸å¥½ã€‚å›¾ç‰‡ä¼šè¢«é¢„å¤„ç†æˆ256*256ã€‚    










# PixArt-Î£    
è¯¥ç»„ç»‡ä¸å«Œç´¯

Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation   

(ğŸ”¥ New) Apr. 6, 2024. ğŸ’¥ PixArt-Î£ checkpoint 256px & 512px are released!   
(ğŸ”¥ New) Mar. 29, 2024. ğŸ’¥ PixArt-Î£ training & inference code & toy data are released!!!   

åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤ã€å¤§è¿ç†å·¥å¤§å­¦ã€é¦™æ¸¯å¤§å­¦ã€é¦™æ¸¯ç§‘æŠ€å¤§å­¦    
https://pixart-alpha.github.io/PixArt-sigma-project/    
https://arxiv.org/abs/2403.04692    
[Submitted on 7 Mar 2024 (v1), last revised 17 Mar 2024 (this version, v2)]





## è¯¥ç»„ç»‡å‰æœŸç ”ç©¶
1Huawei Noah's Ark Lab, 2Dalian University of Technology, 3The University of Hong Kong,     
OpenXLab    




  
### PixArt-Î±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis    
è¢«yosoç”¨æ¥å¾®è°ƒæ¨¡å‹    
few_step_gen folderæœ‰ç®€ç•¥ä»‹ç»    
https://arxiv.org/abs/2310.00426   
[Submitted on 30 Sep 2023 (v1), last revised 29 Dec 2023 (this version, v3)]  



### PIXART-Î´: Fast and Controllable Image Generation with Latent Consistency Models    
https://arxiv.org/abs/2401.05252    
[Submitted on 10 Jan 2024]   

Alphaçš„å¤§å°å†™å½¢å¼åˆ†åˆ«æ˜¯ Î‘ å’Œ Î± ã€‚å®ƒæ˜¯å¸Œè…Šå­—æ¯è¡¨ä¸­çš„ç¬¬1ä¸ªå­—æ¯ã€‚
Delta(å¤§å†™ Î”,å°å†™ Î´),æ˜¯ç¬¬å››ä¸ªå¸Œè…Šå­—æ¯      
è‹±è¯­åç§°ï¼š sigma ï¼Œæ±‰è¯­åç§°ï¼šè¥¿æ ¼ç›ï¼ˆå¤§å†™Î£ï¼Œå°å†™Ïƒï¼‰Sigmaæ˜¯å¸Œè…Šå­—æ¯çš„ç¬¬åå…«ä¸ªå­—æ¯        
  
PIXART-Î´: Fast and Controllable Image Generation with Latent Consistency Models    
PixArt-Î£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation    

Thanks to PixArt-Î±, DiT and OpenDMD for their wonderful work and codebase!


## æ¨ç†
å¯ä»¥ä½¿ç”¨gradioæ¨ç†   
æ”¯æŒdiffusers   
å¯ä»¥è®­ç»ƒå’Œæ¨ç†   
æœ‰256 512 1024æ¨¡å‹    
512dmdæ¨¡å‹     



nvcc11.8,torch 2.0.0æ²¡è¯´æ˜cuç‰ˆæœ¬   
å¥½åƒé»˜è®¤11.7   

    File "/root/miniconda3/envs/pixart/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
        return F.conv2d(input, weight, bias, self.stride,
    RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR

pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118      
å¸è½½é‡è£…     
è¿˜æ˜¯cudnné”™è¯¯   

ä½†æ˜¯è¿™ä¸ª3090æ˜¯å¯ä»¥è¿è¡Œwebuiæ¨ç†    

æœ‰äººè¯´ å…¶å®å°±æ˜¯gpuæ˜¾å­˜ä¸å¤Ÿï¼Œå‡å°ç‚¹å·¥ä½œé‡å°±å¯ä»¥äº†    

 2ã€æ‰‹åŠ¨ä½¿ç”¨cudnn

    import torch
    torch.backends.cudnn.enabled = False


dmdæ¨¡å‹æŒ‡å®štimesteps 400    

    timesteps (`List[int]`, *optional*):
            Custom timesteps to use for the denoising process. If not defined, equal spaced `num_inference_steps`
            timesteps are used. Must be in descending order.       





## åŸç†
ä¸»è¦æ¨¡å‹ç»“æ„ä¸PixArt-Î±ç›¸åŒ   

    use_resolution_binning (`bool` defaults to `True`):
        If set to `True`, the requested height and width are first mapped to the closest resolutions using
        `ASPECT_RATIO_1024_BIN`. After the produced latents are decoded into images, they are resized back to
        the requested resolution. Useful for generating non-square images.



## k-diffusion

æœ€åï¼Œæ‚¨å¯èƒ½å¬è¯´è¿‡ k-diffusion è¿™ä¸ªæœ¯è¯­ï¼Œå¹¶æƒ³çŸ¥é“å®ƒæ˜¯ä»€ä¹ˆæ„æ€ã€‚å®ƒæŒ‡çš„æ˜¯ Katherine Crowson çš„ k-diffusion GitHub å­˜å‚¨åº“ä»¥åŠä¸ä¹‹ç›¸å…³çš„é‡‡æ ·å™¨ã€‚    
https://github.com/crowsonkb/k-diffusion    
An implementation of Elucidating the Design Space of Diffusion-Based Generative Models (Karras et al., 2022) for PyTorch, with enhancements and additional features, such as improved sampling algorithms and transformer-based diffusion models.     




è¯¥èµ„æºåº“å®ç°äº† Karras 2022 æ–‡ç« ä¸­ç ”ç©¶çš„é‡‡æ ·å™¨ã€‚    
 [Submitted on 1 Jun 2022 (v1), last revised 11 Oct 2022 (this version, v2)]   
Elucidating the Design Space of Diffusion-Based Generative Models     

åŸºæœ¬ä¸Šï¼Œé™¤äº† DDIMã€PLMS å’Œ UniPC ä¹‹å¤–ï¼ŒAUTOMATIC1111 ä¸­çš„æ‰€æœ‰é‡‡æ ·å™¨éƒ½æ˜¯ä» k-diffusion ä¸­å€Ÿç”¨çš„ã€‚

Enhancements/additional features

    k-diffusion supports a highly efficient hierarchical transformer model type.

    k-diffusion supports a soft version of Min-SNR loss weighting for improved training at high resolutions with less hyperparameters than the loss weighting used in Karras et al. (2022).

    k-diffusion has wrappers for v-diffusion-pytorch, OpenAI diffusion, and CompVis diffusion models allowing them to be used with its samplers and ODE/SDE.

    k-diffusion implements DPM-Solver, which produces higher quality samples at the same number of function evalutions as Karras Algorithm 2, as well as supporting adaptive step size control. DPM-Solver++(2S) and (2M) are implemented now too for improved quality with low numbers of steps.

    k-diffusion supports CLIP guided sampling from unconditional diffusion models (see sample_clip_guided.py).

    k-diffusion supports log likelihood calculation (not a variational lower bound) for native models and all wrapped models.

    k-diffusion can calculate, during training, the FID and KID vs the training set.

    k-diffusion can calculate, during training, the gradient noise scale (1 / SNR), from An Empirical Model of Large-Batch Training, https://arxiv.org/abs/1812.06162).

To do

    Latent diffusion



## ç±»ä¼¼æ¨¡å‹
### stabilityai/sdxl-turbo   
SDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.    

SDXL-Turbo is a distilled version of SDXL 1.0, trained for real-time synthesis. SDXL-Turbo is based on a novel training method called Adversarial Diffusion Distillation (ADD) (see the technical report), which allows sampling large-scale foundational image diffusion models in `1 to 4 steps` at high image quality. This approach uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.

Finetuned from model: SDXL 1.0 Base

### latent-consistency/lcm-lora-sdxl
Latent Consistency Model (LCM) LoRA: SDXL    

Latent Consistency Model (LCM) LoRA was proposed in LCM-LoRA: A universal Stable-Diffusion Acceleration Module by Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.

It is a distilled consistency adapter for stable-diffusion-xl-base-1.0 that allows to reduce the number of inference steps to only between `2 - 8 steps.`

The adapter can be loaded with it's base model stabilityai/stable-diffusion-xl-base-1.0. Next, the scheduler needs to be changed to LCMScheduler and we can reduce the number of inference steps to just 2 to 8 steps. Please make sure to either disable guidance_scale or use values between 1.0 and 2.0.

Combine with styled LoRAs    
LCM-LoRA can be combined with other LoRAs to generate styled-images in very few steps (4-8). In the following example, we'll use the LCM-LoRA with the papercut LoRA. To learn more about how to combine LoRAs, refer to this guide.


### PixArt-Î´-1024-LCM
PixArt-alpha/PixArt-LCM-XL-2-1024-MS

Pixart-Î± consists of pure transformer blocks for latent diffusion: It can directly generate 1024px images from text prompts `within a single sampling process.`

LCMs is a diffusion distillation method which predict PF-ODE's solution directly in latent space, achieving super fast inference with few steps.

[Submitted on 6 Oct 2023]      
Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference     
Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. 

https://latent-consistency-models.github.io/










# cosxl     
Cosine-Continuous Stable Diffusion XL   
Cosine-Continuous EDM VPred schedule     


Cos Stable Diffusion XL 1.0 and Cos Stable Diffusion XL 1.0 Edit   

å¯ä»¥ä¸€è‡´çš„ç”Ÿæˆ2k 4k 8k   
æ˜¾å­˜æ’ä¸º25gå·¦å³    
æ¸…æ™°åº¦æ²¡æœ‰ä¸Šå‡ï¼Œå°±æ˜¯å›¾ç‰‡å¤§å°å˜å¤§äº†    
æ‰€ä»¥æ˜¯ä¸ºä»€ä¹ˆèƒ½å¤Ÿä¸€è‡´æ€§çš„ç”Ÿæˆï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ    
è€Œä¸”é€Ÿåº¦è¿˜æŒºå¿«   

å°è¯•ä½¿ç”¨realistic_vision_v51è¿›è¡Œ4096*4096ç”Ÿå›¾   
Ran out of memory when regular VAE encoding, retrying with tiled VAE encoding.     
æ˜¾å­˜2gä¸Šå‡åˆ°5g   
42s/it     
42steps 
![alt text](assets/pixart-sigma&customnet&cosxl/WeChat93b104784546b5c1c61f7d31dd987388.jpg)
åŠä¸ªå°æ—¶    


![alt text](assets/pixart-sigma&customnet&cosxl/image.png)   
æ­£å¸¸å†™æ³•ç”Ÿå›¾æ•ˆæœä¸å¥½    
![alt text](assets/pixart-sigma&customnet&cosxl/image-2.png)   
ç¡®åˆ‡åœ°è¯´éœ€è¦ä½¿ç”¨eular42æ­¥    
![alt text](assets/pixart-sigma&customnet&cosxl/image-3.png)   
ä½¿ç”¨dpm ++ 2mä¸å¤ªæ­£å¸¸   


è¦åŠ ä¸Šip2på†™æ³•    
![alt text](assets/pixart-sigma&customnet&cosxl/image-1.png)   



![alt text](assets/pixart-sigma&customnet&cosxl/image-4.png)


ç±»ä¼¼ip2på·¥ä½œ  
[Submitted on 28 Jan 2023 (v1), last revised 2 Nov 2023 (this version, v2)]     
SEGA: Instructing Text-to-Image Models using Semantic Guidance         



## å¯¹æ¯”åº¦
å¯¹æ¯”åº¦æŒ‡çš„æ˜¯ä¸€å¹…å›¾åƒä¸­æ˜æš—åŒºåŸŸæœ€äº®çš„ç™½å’Œæœ€æš—çš„é»‘ä¹‹é—´ä¸åŒäº®åº¦å±‚çº§çš„æµ‹é‡ï¼Œå·®å¼‚èŒƒå›´è¶Šå¤§ä»£è¡¨å¯¹æ¯”è¶Šå¤§ï¼Œå·®å¼‚èŒƒå›´è¶Šå°ä»£è¡¨å¯¹æ¯”è¶Šå°ï¼Œå¥½çš„å¯¹æ¯”ç‡120:1å°±å¯å®¹æ˜“åœ°æ˜¾ç¤ºç”ŸåŠ¨ã€ä¸°å¯Œçš„è‰²å½©ï¼Œå½“å¯¹æ¯”ç‡é«˜è¾¾300:1æ—¶ï¼Œä¾¿å¯æ”¯æŒå„é˜¶çš„é¢œè‰²ã€‚ä½†å¯¹æ¯”ç‡é­å—å’Œäº®åº¦ç›¸åŒçš„å›°å¢ƒï¼Œç°ä»Šå°šæ— ä¸€å¥—æœ‰æ•ˆåˆå…¬æ­£çš„æ ‡å‡†æ¥è¡¡é‡å¯¹æ¯”ç‡ï¼Œæ‰€ä»¥æœ€å¥½çš„è¾¨è¯†æ–¹å¼è¿˜æ˜¯ä¾é ä½¿ç”¨è€…çœ¼ç›ã€‚    
åœ¨æš—å®¤ä¸­ï¼Œç™½è‰²ç”»é¢(æœ€äº®æ—¶)ä¸‹çš„äº®åº¦é™¤ä»¥é»‘è‰²ç”»é¢(æœ€æš—æ—¶)ä¸‹çš„äº®åº¦ã€‚  










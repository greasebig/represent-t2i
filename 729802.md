# è‡ªå›å½’
## mar
æ— éœ€çŸ¢é‡é‡åŒ–çš„è‡ªå›å½’å›¾åƒç”Ÿæˆ

[Submitted on 17 Jun 2024 (v1), last revised 28 Jul 2024 (this version, v2)]
Autoregressive Image Generation without Vector Quantization

ç«Ÿç„¶æ¯”å­—èŠ‚æ™š

Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, Kaiming He
Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: this https URL




## llamagen
Autoregressive Model Beats Diffusion: ğŸ¦™ Llama for Scalable Image Generation

Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation
Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, Zehuan Yuan
HKU, ByteDance

[Submitted on 10 Jun 2024]




[2024.06.28] Image tokenizers and AR models for text-conditional image generation are released ! Try it !
[2024.06.15] All models ranging from 100M to 3B parameters are supported by vLLM !
[2024.06.11] Image tokenizers and AR models for class-conditional image generation are released !
[2024.06.11] Code and Demo are released !

We introduce LlamaGen, a new family of image generation models that apply original next-token prediction paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality.

In this repo, we release:

Two image tokenizers of downsample ratio 16 and 8.
Seven class-conditional generation models ranging from 100M to 3B parameters.
Two text-conditional generation models of 700M parameters.
Online demos in Hugging Face Spaces for running pre-trained models.
Supported vLLM serving framework to enable 300% - 400% speedup.


We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.



## VAR
VAR: a new visual generation method elevates GPT-style models beyond diffusionğŸš€ & Scaling laws observedğŸ“ˆ


About
[GPT beats diffusionğŸ”¥] [scaling laws in visual generationğŸ“ˆ] Official impl. of "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction". An *ultra-simple, user-friendly yet state-of-the-art* codebase for autoregressive image generation!


![alt text](assets/729802/image.png)


å¥½åƒéƒ½æ¯”he kaimingæ—© åè€…ä¸è¿‡æ˜¯åˆ›æ–°mar



# mamba

mamba è‡ªå›å½’ beat 


# open clip

https://github.com/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md

![alt text](assets/729802/image-2.png)

![alt text](assets/729802/image-1.png)

ç«Ÿç„¶æœ‰è¿›åŒ–äº† è¶…è¶ŠconNext


# AI-ISP

AIé™å™ªæ¨¡å‹: é’ˆå¯¹RAW domainçš„2D AI-NR

æœ¬äººæ˜¯åˆšå…¥è¡ŒISPçš„å°ç™½ï¼Œä¸ŠåŠå¹´æ¥è§¦AI-ISPç›¸å…³çš„çŸ¥è¯†å¹¶ç€é‡å­¦ä¹ äº†é™å™ªç›¸å…³çš„æ¦‚å¿µå’Œå¼€å‘æµç¨‹ï¼Œ

è¿™æ˜¯ä¸€ä¸ªå…³äºAI-ISPæ¨¡å—ï¼šNoise Reduction çš„å·¥ç¨‹å®ç°æ–‡æ¡£ï¼Œé’ˆå¯¹ç›®æ ‡cameraå¦‚ï¼ˆsensorï¼šIMX766ï¼‰æ¢³ç†AIé™å™ªçš„å®ç°æµç¨‹ï¼Œè¯¥é¡¹ç›®åŒ…å«ï¼šæ•°æ®å‡†å¤‡ã€æ¨¡å‹è®¾è®¡ã€æ¨¡å‹è®­ç»ƒã€æ¨¡å‹å‹ç¼©ã€æ¨¡å‹æ¨ç†ç­‰ã€‚è¯·å…ˆç¡®ä¿å®‰è£…è¯¥é¡¹ç›®çš„ä¾èµ–é¡¹ï¼Œé€šè¿‡git cloneä¸‹è½½è¯¥é¡¹ç›®ï¼Œç„¶ååœ¨è¯¥é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–é¡¹ã€‚


AI-ISPçš„ç”¨é€”æ˜¯é€æ­¥å–ä»£ä¼ ç»ŸISPé“¾æ¡ä¸Šä¸€äº›éš¾ä»¥ä¼˜åŒ–çš„æ¨¡å—å¦‚NRã€HDRï¼Œä»¥å®ç°äººçœ¼è§‚æ„Ÿæå‡æˆ–æœºå™¨è§†è§‰æŒ‡æ ‡çš„ç‰¹å®šä¼˜åŒ–ã€‚
å½“å‰ä¸»æµçš„æ–¹æ¡ˆæ˜¯ç”¨AI-ISPå’Œä¼ ç»Ÿç®—æ³•å…±åŒä½œç”¨äºä¸€ä¸ªæ¨¡å—æ¥ä¿è¯å…¶ç¨³å®šæ€§ï¼Œä¹Ÿæœ‰ä¸€äº›paperå¸Œæœ›ç”¨ä¸€ä¸ªNetworkæ¥å®ç°æ•´ä¸ªISP Pipeçš„æ›¿ä»£ï¼Œä½†ç›®å‰è¿˜å­˜åœ¨æ— æ³•åˆç†tuningåŠä¸ç¨³å®šç­‰ç¼ºé™·ã€‚
AI-ISP modelçš„åº”ç”¨é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šåµŒå…¥å¼ç¡¬ä»¶æ¥å°†PCç«¯ä¾§çš„æ¨ç†æ¡†æ¶ï¼ˆå¦‚torchã€tensorflowï¼‰è½¬ä¸ºå¹³å°è‡ªç ”çš„æ¨ç†æ¡†æ¶æ¥å®ç°OPçš„ä¸€ä¸€æ˜ å°„ï¼Œä¸­é—´å¯ä»¥ä¼šå­˜åœ¨æŸäº›OPçš„ä¼˜åŒ–å’Œæ”¹å†™ä»¥å®ç°è‰¯å¥½çš„éƒ¨ç½²æ•ˆæœï¼Œæ‰€ä»¥ä¹Ÿèƒ½æ¥è§¦ä¸€äº›ç¡¬ä»¶æ¶æ„å­¦ä¹ å’Œéƒ¨ç½²ç›¸å…³çš„æ¦‚å¿µï¼Œä¸ªäººè®¤ä¸ºæœ‰è‰¯å¥½çš„å­¦ä¹ å‰æ™¯ï¼Œå…±å‹‰ï¼


# coreml
å®ƒæ”¯æŒç”¨äºåˆ†æå›¾åƒçš„ Visionã€ç”¨äºå¤„ç†æ–‡æœ¬çš„ Natural Languageã€ç”¨äºå°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬çš„ Speech ä»¥åŠç”¨äºè¯†åˆ«éŸ³é¢‘ä¸­å£°éŸ³çš„å£°éŸ³åˆ†æã€‚Core ML æœ¬èº«å»ºç«‹åœ¨ Accelerate å’Œ BNNS ç­‰ä½çº§åŸºå…ƒä»¥åŠ Metal Performance Shaders ä¹‹ä¸Šã€‚

Core ML æ˜¯ Apple çš„æ¡†æ¶ï¼Œç”¨äºå°†æœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆåˆ°æ‰€æœ‰å¹³å°ä¸Šçš„ app ä¸­ã€‚å®ƒä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿç›´æ¥åœ¨è®¾å¤‡ä¸Šå®ç°ä¸€ç³»åˆ—æœºå™¨å­¦ä¹ åŠŸèƒ½ï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚


æ­£å¦‚ Apple æ‰€è¯´ï¼Œâ€œCore ML é’ˆå¯¹è®¾å¤‡æ€§èƒ½è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†å†…å­˜å ç”¨å’ŒåŠŸè€—ã€‚ä¸¥æ ¼åœ¨è®¾å¤‡ä¸Šè¿è¡Œå¯ç¡®ä¿ç”¨æˆ·æ•°æ®çš„éšç§ï¼Œå¹¶ä¿è¯æ‚¨çš„åº”ç”¨ç¨‹åºåœ¨ç½‘ç»œè¿æ¥ä¸å¯ç”¨æ—¶ä¿æŒåŠŸèƒ½å’Œå“åº”èƒ½åŠ›ã€‚


# å³æ¢¦
![alt text](assets_picture/729802/image.png)

å‚»é€¼ç©æ„

äºŒæ¬¡çŸ­ä¿¡éªŒè¯éƒ½ä¸ç»™

# topazai
![alt text](assets_picture/729802/image-1.png)















# ç»“å°¾